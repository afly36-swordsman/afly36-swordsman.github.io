<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>文献整理·整合篇</title>
    <link href="/2023/12/20/papers4/"/>
    <url>/2023/12/20/papers4/</url>
    
    <content type="html"><![CDATA[<p>正在码字ing...</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理·综述篇（二）</title>
    <link href="/2023/12/20/papers3/"/>
    <url>/2023/12/20/papers3/</url>
    
    <content type="html"><![CDATA[<p>敬请期待~</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理·综述篇（一）</title>
    <link href="/2023/12/20/papers2/"/>
    <url>/2023/12/20/papers2/</url>
    
    <content type="html"><![CDATA[<p>声明：本文是根据VIPLab的一位博学多才的博士师兄的知乎专栏内容进行归纳总结的，地址如下： <ahref="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">部分基于深度学习的红外与可见光图像融合模型总结</a></p><hr /><h2 id="引言">引言</h2><p>传统方法用于图像融合的一些问题：1.没有完全考虑不同模态图像的特性，使用同种方法对不同模态图像进行特征提取可能不会提取到最有效的信息；2.融合规则是人工设计的简单规则，例如最大值法、平均值法，对于融合结果有限制。</p><p>深度学习方法做融合的优势：1.不同的网络分支可以对不同模态的图像进行特征提取，甚至对同种模态使用不同分支提取多种信息；2.融合策略可以通过学习得到，可不经过人工设计的融合规则获取融合结果。</p><p>近两年做红外-可见光图像融合的深度学习模型也是以下面的模态特点为基础的：&gt;红外图像独有的信息为像素值幅度信息，能够体现温度显著性，温度越高的目标像素值越大；可见光图像独有的信息为纹理细节信息，多数红外图像中模糊的区域在可见光图像中则具有良好的细节，所以一些自监督模型在设计损失函数时针对红外图像的损失项使用的是MSE/L2-norm，保持温度显著性信息，而针对可见光图像使用的是SSIM或者梯度算子，保持细节纹理信息。自监督方法需要设计出合理的指标作为损失函数。</p><p>根据时间来分类，2019年之前基本都是有监督模型或者直接套用预训练模型，2020年开始有自监督和GAN模型出现，整体还是修改网络结构和损失函数，个别模型思考了不同模态信息的保留方式，亦有加入了self-attention机制的融合模型。到了2021年，涌现了很多的深度学习模型，也相应诞生了很多新的处理思路。很多学者开始思考到底在红外与可见光图像融合中该如何定义互补信息、如何计算每种模态下图像具有的互补信息，以及如何有效地融合这些信息？展现在模型上，就有了基于显著性的模型、基于transformer的模型等，不再是简单地修改损失函数和网络结构，不再简单地将红外图像的特有信息定义为像素幅度、可见光图像的特有信息定义为纹理细节。同时，还出现了多任务融合模型，例如将超分和融合一起做的端对端模型CF-Net。任务驱动的融合方法也是一种趋势，如SeAFusion。</p><p>总体来说，红外与可见光图像融合包含有监督模型、无监督/自监督模型、GAN、显著性模型、transformer/self-attention、多任务驱动、不同分辨率下的融合模型、引导滤波辅助的模型等。下面根据年份对其中一部分模型做简单的介绍。</p><h2 id="densefuse">1.DenseFuse</h2><p>A Fusion Approach to Infrared and Visible Images (Transactions onImage Processing, 2018)</p><p>DenseFuse一般认为是第一个使用深度学习模型对红外与可见光图像进行融合的方法，也是一种有监督学习模型。该模型将红外与可见光图像分解为basepart和detail part两部分，其中base part直接用平均法进行融合，detailpart通过一个预训练的VGG网络先进行特征提取，然后使用multi-layers fusionstrategy基于已有特征图得到融合权重图，将权重图和特征图相乘，再和另一模态的结果相加视为融合特征图的结果。</p><p><imgsrc="https://pic1.zhimg.com/v2-1c849a57e96fa6aa833257b9857a9ae4_r.jpg" /></p><p>实现：经过L1-norm和average获得activity levelmap，然后继续基于该activity level map，使用softmax计算final activitylevel map，该结果和两个模态的detail part分别相乘后再相加就是detailpart的融合结果，这一思想与Infrared and Visible Image Fusion using a DeepLearning Framework(International Conference on Pattern Recognition,2018)中的融合策略一致。Base part和detailpart都完成融合后，将这两个结果相加得到最终的融合结果。</p><p>图像分解与重建使用的是预训练过的自编码器，而此前的论文中对图像分解还多用的是传统的变换域分解方法。可以说DenseFuse不再使用传统图像融合的图像分解方法，取而代之的是利用了卷积层最擅长的特征提取能力，一个优势是提取到的特征对不同场景图像的适应性要好。</p><p>模型的训练是以可见光图像作为输入和输出训练得到一个自编码器，编码器中加入了denseconnection，有利于层之间信息的传播。使用的损失函数为MSE和SSIM。训练完成后直接在编码器和解码器之间加入以上提及的特征图融合方法即可。</p><p><imgsrc="https://pic3.zhimg.com/v2-37f40ceff40885e3435b3fd69f6d2cb2_r.jpg" /></p><p>DenseFuse整体思路和2020年很多模型的思路差不多，其实都是用卷积神经网络作为特征提取与重建模块，不过由于DenseFuse在训练自编码器时使用的只有可见光图像，对红外图像的特征提取能力有一定限制，因此针对融合所需要的互补信息的提取能力可能也存在一定的限制。不过相对于传统方法提升较好，尤其是不同场景下融合结果中的伪影较少。</p><h2 id="fusiongan">2.FusionGAN</h2><p>A generative adversarial network for infrared and visible imagefusion (Information Fusion, 2019)</p><p>这是首个使用GAN来融合红外与可见光图像的模型，通过生成器和判别器之间的对抗学习避免人工设计activitylevel和融合规则。其中生成器同时将红外图像与可见光图像作为输入，输出融合图像；判别器将融合图像与可见光图像作为输入，得到一个分类结果，用于区分融合图像与可见光图像。在生成器和判别器的对抗学习过程中，融合图像中保留的可见光信息将逐渐增多。训练完成后，只保留生成器进行图像融合即可。由于可见光图像的纹理细节不能全部都用梯度表示，所以需要用判别器单独调整融合图像中的可见光信息。</p><p>实际训练中的生成器和判别器的平衡不好把握，FusionGAN的融合结果对比度不是很好，红外目标的显著性保留的不是很好。</p><h2 id="nestfuse">3.NestFuse</h2><p>An Infrared and Visible Image Fusion Architecture Based on NestConnection and Spatial/Channel Attention Models (IEEE Transactions onInstrumentation and Measurement, 2020)</p><p>NestFuse可以说是DenseFuse的升级版本，仍然是有监督模型，在第一阶段训练自编码器时用多张可见光图像作为输入输出来训练网络。编码器的结构与DenseFuse没有太大差异，不同之处就是多个卷积层输出的特征图都会进行融合，因此NestFuse是一种多尺度融合模型，对下采样带来的细节损失有一定保护效果。融合模块使用spitalattention和channel attention生成显著图来进行融合，解码器加入了nestconnection，保护编码器提取到的多尺度特征。</p><p><imgsrc="https://pic4.zhimg.com/v2-f85aa4bd3c89c975845e44469f91fe03_r.jpg" /></p><p>训练完成后的融合模块则不需要训练，仍然是传统方法，采用了两个通道的attention：Spitalattention和channelattention分别在同一图像内对不同像素生成显著性和不同特征图之间的通道维度生成显著性。经过spitalattention和channelattention后可以得到两个特征图，直接进行平均法得到融合特征图。</p><p>NestFuse虽然在解码器阶段使用了nestconnection，不过和之前的有监督模型一样，针对模态之间的互补信息提取的较少，并且融合规则仍然是人工设计的方法，不能针对红外与可见光特有的信息进行融合。</p><h2 id="didfuse">4.DIDFuse</h2><p>Deep Image Decomposition for Infrared and Visible Image Fusion(International Joint Conferences on Artificial Intelligence, 2020)</p><p>最早使用深度学习方法的模型，基本都是先用传统方法将输入图像分解为basepart和detailpart，相当于图像中的低频部分和高频部分。DIDFuse则是将深度图像分解方法引入红外与可见光图像融合模型中，在训练阶段通过编码器将图像分解为背景部分和细节部分，测试阶段将红外与可见光图像的背景部分和细节部分分别进行融合后，再送入解码器进行图像重建，得到融合结果。</p><p><imgsrc="https://pic1.zhimg.com/v2-012dfb0a77a6c984a1e948e132475ba0_r.jpg" /></p><p>为了避免细节信息损失，在网络中加入shortconnection，分别将编码器的前两个卷积层输出特征图与解码器后两个卷积层输出的特征图进行直接拼接。编码器输出的背景特征图和细节特征图需要通过训练得到，因此设计了相应的损失函数。损失函数各项包括编码器损失L1（Figure 1(a) <spanclass="math inline">\(L_{total}\)</span>中的前两项）和解码器损失L2（<spanclass="math inline">\(L_{total}\)</span>的后三项）。L1用于图像分解，希望两种模态背景部分相差较小、细节部分相差较大，因此第一项和第二项分别为正、负。经过tanh函数，可以将二者的差距限定在-1到1之间。L2用于图像重建，第三、四项目的是保持源图像和重建图像的像素幅度信息和细节纹理信息，很多2020年的自监督模型都是通过这种损失函数来训练网络的，用的是非常普遍的L2-norm和结构相似度SSIM，分别计算红外源图像与重建图像之间的损失、可见光源图像与重建图像之间的损失。具体形式如下：</p><p><img src="/img/stage2/2_3.png" /></p><p>损失函数最后一项使用梯度算子来保留可见光图像的细节信息。自监督模型在计算纹理细节损失中，使用SSIM和梯度算子很普遍。</p><h2 id="ddcgan">5.DDcGAN</h2><p>A Dual-Discriminator Conditional Generative Adversarial Network forMulti-Resolution Image Fusion (Transactions on Image Processing,2020)</p><p>是2020年的GAN模型之一，融合效果在细节和对比度方面要优于FusionGAN，并且是针对红外图像分辨率低于可见光图像分辨率的情况。模型首先通过训练过的卷积层对分辨率低的红外图像进行上采样到可见光图像的分辨率（也可以将融合与超分一起做，具体的模型看CF-Net），然后生成器对这两张图像进行融合，两个判别器分别针对红外与可见光图像进行判断。损失函数方面和先前模型类似，都是基于保留红外图像像素幅度、可见光图像细节信息的基础上设计的。</p><p><imgsrc="https://pic3.zhimg.com/v2-d38428c0ec444dd44f06c5dbefd48b96_r.jpg" /></p><p>生成器的结构如下图，首先通过反卷积层将红外图像上采样到和可见光图像一样的尺寸（红外图像训练集手动下采样到原图的1/4大小），两张图像在通道维度拼接后送入5个加入denseconnection的编码层，再经过5个常规卷积层就能得到融合图像了。</p><p><imgsrc="https://pic3.zhimg.com/v2-00228a8f61da0bd5fbb9b21a4e33b726_r.jpg" /></p><p>判别器的结构如下图，红外图像和可见光图像各一个判别器。红外图像对应的判别器是从融合图像或红外原输入图像中随机选出一张作为输入，可见光图像对应的判别器是从融合图像或可见光原输入图像中随机选出一张作为输入，它们的输出都是一个代表概率的标量，表示输入图像是真实图像（原图）的概率。</p><p><imgsrc="https://pic2.zhimg.com/v2-e3e41342d77fe025435deb850101f55d_r.jpg" /></p><p>编码器的损失函数是由adversarial loss和content loss组成，其中Contentloss中红外图像的像素幅度计算通过Frobeniusnorm得到，而可见光图像的细节信息通过TV-norm得到。判别器的损失函数用于区分源（原）图像和融合图像，判别器的adversarialloss可以用来计算不同分布之间的Jensen-Shannon差异，因此可以用来判断像素强度和纹理细节分布的真实性并促使融合图像的分布更贴近真实分布。</p><h2 id="fast-uif">6.Fast-UIF</h2><p>Rethinking the Image Fusion: A Fast Unified Image Fusion Networkbased on Proportional Maintenance of Gradient and Intensity (AAAI,2020)</p><p>这是用于通用融合任务的模型，进行不同融合任务时需要调整损失函数中各项的权重。网络整体分为gradientpath和intensitypath两个分支，输入则是不同模态的混合输入，而非先前模型采用的gradient输入可见光图像（细节信息多的模态），intensity输入红外图像（像素幅度信息多的模态），在进行最终的融合前还有预融合模块。</p><p><imgsrc="https://pic1.zhimg.com/v2-bf617fd5154be3c8d0104ed4bf0ef9dc_r.jpg" /></p><p>Gradient path的输入由两张可见光图像和一张红外图像组成，intensitypath的输入由两张红外图像和一张可见光图像组成，也可以根据场景调整两个模态图像的比例。两个分支都是由卷积层组成，不包含下采样步骤，并且各自包含两次通道维度拼接过程和两个预融合模块（pathwisetransferblock），最后每个卷积块的输出都会全部进行一次通道维度的拼接，经过单层卷积后是最终的融合图像。损失函数的设计以像素幅度项与梯度项为主，不同的融合任务调整四个项的权重即可，比如红外与可见光的融合中红外图像包含的幅度信息多，所以计算像素幅度的项：红外对应的那一项权重应该大于可见光的；同理，计算梯度的项：可见光对应的权重应该大于红外的。</p><h2 id="fusiondn-u2fusion">7.FusionDN &amp; U2Fusion</h2><p>FusionDN和U2Fusion的融合思想是一样的，FusionDN作者先发表在AAAI(2020)上，后来作者将改进后的U2Fusion发表在了TPAMI上。</p><p>两篇文章都是对融合过程中每种模态图像信息量的测量与保留方法进行了研究，FusionDN通过已有的图像质量评价模型NR-IQA和熵Entropy的计算来测量每个模态图像的信息量得到权重，而U2Fusion通过FeatureExtraction、Information Measurement、Information PreservationDegree三个模块根据源图像直接得到其相应的信息量权重，作用就相当于FusionDN的NR-IQA和Entropy，其中FeatureExtraction使用的是预训练的VGG-16，在大量数据上预训练过的深度网络可以提取出图像中各类特征。信息权重作用于损失函数。另外，U2Fusion在损失函数中加了EWC(ElasticWeight Consolidation, 可塑权重巩固, <ahref="https://zhuanlan.zhihu.com/p/86365066">点击查看详细解读</a>)这一项，避免网络串行训练数据时遗忘先前的任务内容。融合网络比较简单，将两个输入在通道维度拼接后送入卷积层就OK了。</p><p>详情可以点击以下链接查看：</p><p><a href="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">红外与可见光图像融合模型总结2.6</a></p><p><a href="https://zhuanlan.zhihu.com/p/397425256"title="这也是博士师兄写的文章哟~">无监督图像融合模型U2Fusion/FusionDN</a></p><h2 id="fusiongan-1">8.FusionGAN++</h2><p>Infrared and visible image fusion via detail preserving adversariallearning (Information Fusion, 2020)</p><p>本文的模型用于改善先前GAN模型带来的细节损失问题，并且加入了针对边缘的保护机制。模型的生成器产生融合图像，然后将融合结果与可见光源图像一起送入判别器，判断融合结果是否来自于可见光图像。当判别器不能区分融合结果与可见光图像时，此时认为融合结果包含了充足的细节信息，通过这种方式就能自动进行细节信息的表示和选择，不再通过人工设计融合规则。边缘保护体现在损失函数中。</p><h2 id="rfn-nest">9.RFN-Nest</h2><p>An end-to-end residual fusion network for infrared and visible images(Information Fusion, 2021)</p><p>RFN-Nest在2020年NestFuse的基础上，将融合模块从人工设计的融合规则变成了用网络进行融合，同时不再是单独用自然图像进行有监督训练，也和很多其他模型类似，加入了自监督学习方法，其中第一阶段训练和NestFuse一样，用大量自然图像训练一个自编码器，而第二阶段训练是RFN-Nest新增加的自监督训练方式，会生成四个用于多尺度融合的RFN(Residualfusionnetwork)模块。相较于其他自监督模型的不同之处是多尺度（很多融合模型为了避免细节损失不会使用任何下采样步骤）和大规模红外-可见光数据集参与了训练。</p><figure><imgsrc="https://pic4.zhimg.com/v2-5ff110ca353056e0f43c189c026d94b3_r.jpg"alt="Framework of proposed RFN-Nest" /><figcaption aria-hidden="true">Framework of proposedRFN-Nest</figcaption></figure><p>首先来说第一阶段训练，编码器和解码器与先前的NestFuse相同，并且第一阶段训练也仍然是用自然图像训练自编码器，采用的是COCO数据集的八万张图像。</p><p>第二阶段是训练四个RFN模块，其实也就是学习融合策略，需要先将编码器和解码器的网络参数都固定（相当于RFN的输入输出固定），然后用自监督方法专门训练该模块的网络。RFN结构如下：</p><figure><imgsrc="https://pic1.zhimg.com/v2-73b9e582ce5b2f0414a8e678879d9dbc_r.jpg"alt="the structure of RFN" /><figcaption aria-hidden="true">the structure of RFN</figcaption></figure><p>两个模态的特征图会分成两个分支，一个分支各自经过单个卷积层后在通道维度拼接，再用后续三个卷积层处理，另一个分支则经过单个卷积层后作为残差连接和另个分支的组合，得到的就是融合特征图。四个RFN模块均为这种结构，在四个尺度上对特征图进行融合。这阶段训练使用的数据集为KAIST数据集，包含八万个红外-可见光图像对。</p><h2 id="cf-net">10.CF-Net</h2><p>Deep Coupled Feedback Network for Exposure Fusion and ImageSuper-Resolution (Transactions on Image Processing, 2021)</p><p>本文提出的CF-Net是将超分和多曝光融合用一个网络做的端对端模型，这种多任务图像融合模型也是综述Imagefusion meets deep learning: A survey andperspective中提出的图像融合趋势。超分辨有利于目标检测的准确度，因此如果把多曝光任务和超分辨率任务一起做，应该也可以提高high-level的任务效果，因此CF-Net也可以认为是任务驱动的融合模型，即未来的发展趋势之一。</p><p>查看详情：<a href="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">红外与可见光图像融合模型总结3.2</a></p><h2 id="stdfusionnet">11.STDFusionNet</h2><p>An Infrared and Visible Image Fusion Network Based on Salient TargetDetection (Transactions on Instrumentation and Measurement, 2021)</p><p>STDFusionNet是基于显著性目标检测的融合方法，可以保护红外显著性目标的纹理信息和温度信息。首先用mask对红外显著性目标进行标注，然后结合mask设计损失函数来进行提取特征和重建。mask只在训练阶段使用，也就是说本文模型可以隐式完成显著目标检测和关键信息融合。应该说STDFusionNet也考虑了红外与可见光融合的互补信息，只不过人为地将其定义为红外图像的显著目标和可见光图像中的背景纹理的组合。</p><p>网络结构如下图，可以看到网络的输入是完整的图像，只不过在计算损失函数时会用mask对红外图像中的显著性目标单独提出来，将可见光图像的背景区域单独提出来计算相应的损失。两个模态的特征图也是在通道维度拼接后送入图像重建网络中的。</p><p><imgsrc="https://pic2.zhimg.com/v2-f24e7babafa72572dbf86554da7681e5_r.jpg" /></p><p>损失函数：</p><p><img src="/img/stage2/3_4.png" /></p><h2 id="csfsaliency-based">12.CSF(Saliency-based)</h2><p>Classification Saliency-Based Rule for Visible and Infrared ImageFusion (Transactions on Computational Imaging, 2021)</p><p>本文也提出了一种基于显著性分类的融合方法classification saliency-basedfusion method(CSF)，不过和上一个STDFusionNet在图像内选择显著性区域不同，本文的模型是根据多个卷积层输出特征图的可解释性重要性来评估的，这种面向重要性的融合规则有助于保留有价值的特征图，得到的显著性信息就直接作为融合的加权系数。首先使用分类器对两种类型的源图像进行分类，可以测量源图像之的差异信息和独有信息。然后根据每个像素对分类结果的贡献程度计算它的重要性，该重要性将以分类显著图的形式呈现，根据这个分类显著图来融合各个特征图，这种通过预训练过的分类器来自动保留重要特征也是一个新颖的角度。</p><p>整体结构如下图，和其他深度学习模型一样，先提取特征图进行融合，然后对融合特征图进行重建。特征提取和重建都是普通的卷积层，重点就在于融合特征图这部分的设计，也是本文的核心。</p><p><imgsrc="https://pic1.zhimg.com/v2-c96de3a4ab9709d5d838a0bbfff9d628_r.jpg" /></p><p>分类显著性估计（Classification SaliencyEvaluation）：红外与可见光图像融合的一大策略就是将每个模态下图像的重要信息和互补信息提取出来进行融合，因此该融合的关键点就在于特征图每个区域的重要性评估，重要的区域需要被保留，冗余区域需要被压缩。</p><p>一种直观的评价方式是将部分可见光特征图替换为红外特征图，然后观察替换后结果的变化。如果这部分是多余的，在替换之后，重建的图像仍然会看起来像原始的可见光图像。如果不是多余的（这部分包含红外图像中的唯一/关键信息），替换后重建的图像将与红外图像相似。反之亦然。为了量化图像风格，使用一个二分类器测量图像属于任何一种风格的概率。此外，分类器能找到每种风格最明显的特性和不同风格之间最明显的差异，可以比较不同类型的信息并帮助识别重要且值得保留的信息，这种功能对融合很有用处，所以使用分类器来帮助定量设计融合规则。</p><p>上述思想的具体实现方法：以图像对为例进行通道替换。当24张输入的特征图全部都是红外特征图时，该特征图组被判断风格为红外的概率接近1，而将这组中的其中一张特征图替换为可见光图像特征图，再看风格被判断为红外的概率是多少。用这种方法，按照顺序依次将24张特征图的某一张替换为可见光特征图，概率值的变化就可以统计出来了。</p><p><imgsrc="https://pic4.zhimg.com/v2-209f505af521d3b3abeb754e1199295b_r.jpg" /></p><p>上图中有一些位置特征图被替换后，风格被判断为红外的概率大大降低，有的则基本没有下降，可以说那些被替换后概率没有下降太多的特征图，对于分类的贡献就不是很大，也就是说这个位置的特征图对分类没什么作用，即红外和可见光在这个通道位置上对应的特征比较一致，没有较大的区分度。相反地，一些位置的特征图被替换后，风格被判断为红外的概率大大降低，这些位置可以说对分类的贡献较大，红外和可见光在这些位置上的特征差异较大。根据特征图对分类结果的影响，可以评估重要性以反映这些特征图是否包含重要信息，将其称为分类显着性（classificationsaliency），以此作为融合时信息量保留的基础。</p><p>上面所说的是通道维度上的显著性，即channel-wise。在每个特征图上的每个像素也存在显著性差异，即pixel-wise，因此需要综合看待每个特征图上的单个像素对分类器判断为红外或可见光图像中概率较大的那个值的变化，从而计算出每个像素位置上的显著性值，即分类显著性图。</p><p>有了这两个权重图，下一步和先前NestFuse模型一样，每个模态的特征图和权重图相乘再相加即可。</p><p>另外，文中有一段阐述了现有人工设计融合方式的缺陷，写的很好： Thereason why existing fusion rules are rough for fusing features is asfollows. Because of the unexplainability and incomprehensibility ofCNNs, the specific characteristics represented in feature maps areunknowable. For instance, some convolution kernels extract brightregions, some extract dark regions while some may extract lines. If thisis the case, the max rule can well preserve the bright regions while theinformation with low brightness will suffer from distortion. Because ofthe unknown and variability, it is difficult to measure the importanceof different regions of feature maps. Thus, it is groundless to design afusion rule by assigning pixel-wise weight maps, which take thepixel-wise importance of feature maps into account. In this case, thelimited choices of fusion rules and their roughness restrict theimprovement of fusion results. Even a well-designed feature extractionway may fail to achieve its optimal performance because of therestriction of the fusion rule.</p><h2 id="ganmcc">13.GANMcC</h2><p>A Generative Adversarial Network With Multiclassification Constraintsfor Infrared and Visible Image Fusion (Transactions on Instrumentationand Measurement, 2021)</p><p>以往提起红外与可见光图像，一般都认为红外图像细节差，但是目标显著性好、对比度高，要用像素幅度约束；可见光图像细节好，要用梯度或SSIM约束。GANMcC提出的思想是，红外图的细节不一定就比可见光差，可见光也有可能对比度优于红外图像，而在某些场景下确实如此。</p><p>此外要确保融合图像既有显著的对比度又有丰富的纹理细节，关键是保证源图像的对比度和梯度信息是平衡的，本质上是同时估计两个不同域的分布，GAN可以在无监督情况下更好地估计目标的概率分布，而多分类GAN可以进一步同时拟合多个分布特征，解决这种不平衡的信息融合。</p><p>具体做法是：用一个多分类器作为判别器，可以确定输入是红外图像和可见图像的概率。对于融合图像，在多分类约束下，生成器期望这两个概率都很高，即判别器认为它既是红外图像又是可见光图像；而判别器期望这两个概率同时很小，即判别器判断融合图像既不是红外图像也不是可见光图像。在此过程中，同时约束这两个概率，以确保融合图像在两个类别中的true/false程度相近/相同。经过不断的对抗学习，生成器可以同时拟合红外图像和可见光图像的概率分布，从而产生对比度显著和纹理细节丰富的结果。通过这两种设计的配合，可以生成具有良好视觉效果的融合图像。模型结构如下图所示。</p><p><imgsrc="https://pic2.zhimg.com/v2-33585597ce11890b9fafe430f2d1dba5_r.jpg" /></p><p>输入与上述第6个网络Fast Unified Image FusionNetwork一样，也是两个模态的混合输入，由生成器得到融合图像，判别器输出该融合图像属于红外/可见光的两个概率值，多轮对抗训练后，当两个概率值都比较大的时候，认为互补融合信息达到了平衡。</p><p>生成器的结构如下，其中在拼接特征图时是将两个分支特征图交叉拼接的，目的是为了更好融合信息。</p><p><imgsrc="https://pic3.zhimg.com/v2-d57089ccf6d96ab529bc4849200285d2_r.jpg" /></p><p>因为使用了GAN网络，故损失函数包含content loss和adversarialloss两项，而本文模型的思想是红外图像也有细节信息，可见光图像也有较好的对比度，因此contentloss的设计考虑了辅助信息，也就是红外图像的纹理细节与可见光图像的对比度信息。通过调整每一项损失的权重大小来决定每种信息的保留程度，首先主要信息的权重应该大于辅助信息的权重，并且梯度信息的权重一般要小于像素强度信息的权重。按照这个规则进行权值的设置。</p><p>判别器的结构如下，判别器也是一个多分类器，对输入图像（红外/可见光/融合图像任选一个输入）进行分类得到输入图像的类别，输出是一个包含两个概率值的向量。</p><p><imgsrc="https://pic2.zhimg.com/v2-238205700b307b4261bf2c6f8e5d4489_r.jpg" /></p><p>损失函数必须促使判别器不断提高其判别能力，才能有效地识别出什么是红外图像或可见光图像。损失函数由三项组成，分别为可见光图像、红外图像、融合图像的decisionloss。具体的损失函数设计就不摆在这里了，可以查看作者论文原文或者<ahref="https://zhuanlan.zhihu.com/p/467039982">专栏第3.7中的内容</a>。</p><h2 id="rxdnfuse">14.RXDNFuse</h2><p>A aggregated residual dense network for infrared and visible imagefusion (Information Fusion, 2021)</p><p>RXDNFuse是在融合网络中加入残差连接的模型，对损失函数也有一定的改进，分为像素级损失和特征级损失，像素级损失还是以像素幅度和结构相似度为基础计算的，特征级损失则是以VGG-19提取的深度特征图作为基础。</p><p><imgsrc="https://pic1.zhimg.com/v2-b47d8e450daf3e4754366633b1e034b0_r.jpg" /></p><p>这里知识简单说明文章的中心思想，网络子结构主要是添加了残差连接和密集连接，idea在损失函数方面要更多一点。</p><p>像素级损失的结构损失和像素幅度损失计算如下图，融合图像与红外、可见光源图都会进行像素幅度和SSIM的计算。</p><p><imgsrc="https://pic3.zhimg.com/v2-c401c565fa3a73ce26e5d29951f28faa_r.jpg" /></p><p>特征级损失其实就是perceptualloss，同样也需要分别计算结构损失和像素幅度损失。而使用深度网络特征图来计算损失的原因作者也引用了其他文章中的结论：通过深度神经网络对图像特征进行逐层处理，输入图像会转换为对图像实际内容越来越敏感的抽象表示，如果直接用较浅层的特征图进行重建，其实只是简单地将原始图像的每个像素进行了重现，与之相反的是深层特征可以捕获抽象语义特征。因此这里将每个输入图像复制为三个通道后送入VGG-19模型来提取高级（深层次的）语义特征，如下图所示。</p><p><imgsrc="https://pic3.zhimg.com/v2-2300d01da41d38aebef93d40b4b99dca_r.jpg" /></p><hr /><p>这篇文章到这里就结束啦！</p><p>本文总结了《部分基于深度学习的红外与可见光图像融合模型总结》3.8及以前的内容，3.9之后的内容和别的文章将在本系列（二）中进行更新，内容涉及：部分传统方法、更新的Paper等，敬请期待~</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像融合</title>
    <link href="/2023/12/16/Review/"/>
    <url>/2023/12/16/Review/</url>
    
    <content type="html"><![CDATA[<h3 id="融合方法分析">融合方法分析</h3><p><a href="https://zhuanlan.zhihu.com/p/475470158?utm_id=0"title="图像融合的方法与分析">图像融合的方法与分析</a></p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>basic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>草稿</title>
    <link href="/2023/11/29/Write/"/>
    <url>/2023/11/29/Write/</url>
    
    <content type="html"><![CDATA[<p>写点什么~</p>]]></content>
    
    
    <categories>
      
      <category>Default</category>
      
    </categories>
    
    
    <tags>
      
      <tag>draft</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理·开山篇</title>
    <link href="/2023/11/25/Papers/"/>
    <url>/2023/11/25/Papers/</url>
    
    <content type="html"><![CDATA[<h2 id="tardal">TarDAL</h2><h3 id="title">Title</h3><p>Target-aware Dual Adversarial Learning and a Multi-scenarioMulti-modality Benchmark to Fuse Infrared and Visible for ObjectDetection [CVPR]</p><h3 id="motivation">Motivation</h3><p>Previous approaches:</p><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>.Aiming <span class="hljs-built_in">at</span> generating an image of high visual quality<span class="hljs-number">2</span>.<span class="hljs-keyword">Discover </span>commons underlying the two modalities <span class="hljs-keyword">and </span>fuseupon the common space <span class="hljs-keyword">either </span><span class="hljs-keyword">by </span>iterative optimization <span class="hljs-keyword">or </span>deep networks<span class="hljs-number">3</span>.Neglect that modality <span class="hljs-keyword">differences </span>implying the complementary infomationwhich <span class="hljs-keyword">extremely </span>important for <span class="hljs-keyword">both </span>fusion <span class="hljs-keyword">and </span><span class="hljs-keyword">subsequent </span>detection task</code></pre></div><h3 id="info">Info</h3><div class="code-wrapper"><pre><code class="hljs maxima">Visible <span class="hljs-built_in">image</span>, provides rich details with high spatial <span class="hljs-built_in">resolution</span>(under welldefined lighting conditions)</code></pre></div><div class="code-wrapper"><pre><code class="hljs sqf">Infrared <span class="hljs-built_in">image</span>, capturing ambient temperature variations emitted <span class="hljs-keyword">from</span> objects, highlight structures of thermal <span class="hljs-built_in">targets</span> (insensive <span class="hljs-keyword">to</span> lighting changes)</code></pre></div><p>Their evident appearance discrepancy, it's challenging to fusevisually appealing images and/or to support higher-level vision tasks(by making full use of the complementary info from the infrared andvisible images)</p><p>The fusion emphasizes more on "seeking commons" but neglect thedifferences of these two modalities on presenting structure info oftargets and textural details of ambient background.</p><h3 id="method">Method</h3><p>Fusion network:</p><div class="code-wrapper"><pre><code class="hljs mipsasm">Composed of one generator <span class="hljs-keyword">and </span>two target-aware <span class="hljs-keyword">discriminators, </span><span class="hljs-keyword">and </span>a commonly used detection network  [Detection-<span class="hljs-keyword">oriented </span>fusion]<span class="hljs-symbol"></span><span class="hljs-symbol">Discriminator:</span><span class="hljs-keyword">Distinguishes </span>foreground(structure infomation of targets), i.e., thermal targets(infrared image), <span class="hljs-keyword">and </span><span class="hljs-keyword">differentiates</span><span class="hljs-keyword"></span>the <span class="hljs-keyword">background, </span>i.e., textural details(visible image)Derive a cooperative training <span class="hljs-keyword">scheme</span></code></pre></div><h3 id="structure-diagram">Structure diagram</h3><figure><img src="/img/stage1/11.png"alt="Methodology framework: (a) bilevel optimization formulation for fusion and detection, (b) target-aware adversarial dual learning network for fusion, and (c) cooperative training scheme." /><figcaption aria-hidden="true">Methodology framework: (a) bileveloptimization formulation for fusion and detection, (b) target-awareadversarial dual learning network for fusion, and (c) cooperativetraining scheme.</figcaption></figure><figure><img src="/img/stage1/12.png" alt="The details of network" /><figcaption aria-hidden="true">The details of network</figcaption></figure><figure><img src="/img/stage1/13.png"alt="The architectures of generator and discriminator" /><figcaption aria-hidden="true">The architectures of generator anddiscriminator</figcaption></figure><h3 id="loss-function">Loss function</h3><p>Detection network backbone: YOLOv5</p><p>Generator: 1.Structural similarity index (SSIM)</p><blockquote><p>Contributes to generate a fused image that preserves overallstructures and maintains a similar intensity distribution as sourceimages.</p></blockquote><figure><img src="/img/stage1/14.png" alt="Generator loss SSIM" /><figcaption aria-hidden="true">Generator loss SSIM</figcaption></figure><p>2.Pixel loss (based on the saliency degree weight (SDW))</p><blockquote><p>To balance the pixel intensity distribution of source images</p></blockquote><p><img src="/img/stage1/15.png" alt="Pixel Loss" /> w1, w2 arecalculated by saliency value of x and y.</p><p>Target and detail discriminators: Wasserstein divergence (with targetmask m)</p><blockquote><p>The target discriminator <span class="math inline">\(D_T\)</span> isused to distinguish the foreground thermal targets of fused result tothe infrared while the detail discriminator <spanclass="math inline">\(D_D\)</span> contributes to distinguish thebackground details of fused result to the visible.</p></blockquote><figure><img src="/img/stage1/16.png" alt="Discriminator loss" /><figcaption aria-hidden="true">Discriminator loss</figcaption></figure><h2 id="seafusion">SeAFusion</h2><h3 id="title-1">Title</h3><p>Image fusion in the loop of high-level vision tasks: A semantic-awarereal-time infrared and visible image fusion network [Image Fusion]</p><h3 id="intro">Intro</h3><p>The infrared sensor captures thermal radiation emitted from objects,which could highlight salient targets, but the infrared image neglectstexture and is vulnerable to noise.</p><p>The visible sensor captures reflective light infomation, the visibleimage usually contains abundant texture and structure info, but issensitive to the environment, such as illumination and occlusion.</p><p>Complementary info: to fuse Ir and Vis image, to generate a desiredimage.</p><p>Fused image has been broadly used as a preprocessing module forhigh-level vision tasks, e.g., object detection, tracking, semanticsegmentation.</p><h3 id="motivation-1">Motivation</h3><p>Pressing challenges:</p><div class="code-wrapper"><pre><code class="hljs livecodeserver"><span class="hljs-number">1.</span>The existing fusion algorithms are inclined <span class="hljs-built_in">to</span> pursue better visual quality<span class="hljs-keyword">and</span> higher evaluation metrics but seldom systematically considerwhether fused image can facilitate high-level vision tasksSome studies: cannot effectively enhance/boost <span class="hljs-keyword">the</span> semantic info <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> fused image<span class="hljs-number">2.</span>Neither visual comparison nor quantitative evaluation (evaluation manners)reflects <span class="hljs-keyword">the</span> facilitation <span class="hljs-keyword">of</span> fused images <span class="hljs-keyword">for</span> high-level vision tasks<span class="hljs-number">3.</span>Not <span class="hljs-keyword">effective</span> <span class="hljs-keyword">in</span> extracting fine-grained detail features<span class="hljs-number">4.</span>Ignore <span class="hljs-keyword">the</span> demand <span class="hljs-keyword">for</span> real-<span class="hljs-built_in">time</span> image fusion</code></pre></div><p>Ours network:</p><p>SeAFusion, can be used for achieving real-time Ir &amp; Vis imagefusion</p><h3 id="method-1">Method</h3><div class="code-wrapper"><pre><code class="hljs pgsql"><span class="hljs-number">1.</span>Simultaneously obtaining superior performance <span class="hljs-keyword">in</span><span class="hljs-keyword">both</span> image fusion <span class="hljs-keyword">and</span> high-<span class="hljs-keyword">level</span> vision tasks.<span class="hljs-number">2.</span>A segmentation network <span class="hljs-keyword">to</span> predict the segmentation results<span class="hljs-keyword">on</span> fused images, which <span class="hljs-keyword">is</span> utilized <span class="hljs-keyword">to</span> construct semantic loss(<span class="hljs-keyword">is</span> leveraged <span class="hljs-keyword">to</span> guide the training <span class="hljs-keyword">of</span> the fusion network via back-propagation, so the loss can flow back <span class="hljs-keyword">to</span> the image fusion module<span class="hljs-keyword">to</span> forcing fused images <span class="hljs-keyword">to</span> contain more semantic infomation).<span class="hljs-number">3.</span><span class="hljs-type">Real</span>-<span class="hljs-type">time</span>: light-weight network based <span class="hljs-keyword">on</span> GRDB (gradient residual dense block)<span class="hljs-keyword">to</span> boost the description ability <span class="hljs-keyword">for</span> fine-grained details <span class="hljs-keyword">and</span> achieve feature reuse.<span class="hljs-number">4.</span>The authors proposed a joint low-<span class="hljs-keyword">level</span> <span class="hljs-keyword">and</span> high-<span class="hljs-keyword">level</span> adaptive training strategy<span class="hljs-keyword">to</span> achieve simultaneously impressive performance <span class="hljs-keyword">in</span> <span class="hljs-keyword">both</span>image fusion <span class="hljs-keyword">and</span> various high-<span class="hljs-keyword">level</span> vision tasks.</code></pre></div><h3 id="structure-diagram-1">Structure diagram</h3><figure><img src="/img/stage1/21.png"alt="The overall framework of the proposed semantic-aware infrared and visible image fusion algorithm." /><figcaption aria-hidden="true">The overall framework of the proposedsemantic-aware infrared and visible image fusion algorithm.</figcaption></figure><figure><img src="/img/stage1/22.png"alt="The architecture of the real-time infrared and visible image fusion network based on gradient residual dense block." /><figcaption aria-hidden="true">The architecture of the real-timeinfrared and visible image fusion network based on gradient residualdense block.</figcaption></figure><figure><img src="/img/stage1/23.png"alt="The specific devise of the gradient residual dense block. The Sobel operator is selected as the Gradient Operator to extract fine-grained detail information of feature maps." /><figcaption aria-hidden="true">The specific devise of the gradientresidual dense block. The Sobel operator is selected as the GradientOperator to extract fine-grained detail information of featuremaps.</figcaption></figure><h3 id="分析">分析</h3><figure><img src="/img/stage1/24.png" alt="Analysis1: fusion network" /><figcaption aria-hidden="true">Analysis1: fusion network</figcaption></figure><figure><img src="/img/stage1/25.png" alt="Analysis2: segmentation module" /><figcaption aria-hidden="true">Analysis2: segmentationmodule</figcaption></figure><figure><img src="/img/stage1/26.png" alt="Algorithm" /><figcaption aria-hidden="true">Algorithm</figcaption></figure><p>注：分割网络的backbone采用YOLOv5网络</p><h2 id="segmif">SegMiF</h2><h3 id="title-2">Title</h3><p>Multi-interactive Feature Learning and a Full-time Multi-modalityBenchmark for Image Fusion and Segmentation [ICCV]</p><h3 id="abstract">Abstract</h3><p>Early efforts boost performance for only one task.</p><p>This paper, SegMiF can dual-task correlation to promote theperformance of both tasks (fusion and segmentation).</p><h3 id="intro-1">Intro</h3><p>Contributions:</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>SegMiF <span class="hljs-keyword">contains</span> two modules, i.e., <span class="hljs-keyword">fusion</span> network<span class="hljs-keyword">and</span> common segmentation network.<span class="hljs-number">2.</span>Hierarchical interactive attention (HIA) block. Fine<span class="hljs-operator">-</span>grained mapping <span class="hljs-keyword">of</span> <span class="hljs-keyword">all</span> the vital infomation <span class="hljs-keyword">between</span> <span class="hljs-keyword">fusion</span> <span class="hljs-keyword">and</span> segmentation. Modality<span class="hljs-operator">-</span><span class="hljs-operator">/</span>Semantic<span class="hljs-operator">-</span> oriented features can be fully mutual<span class="hljs-operator">-</span>interactive(bridge the feature gap <span class="hljs-keyword">between</span> <span class="hljs-keyword">fusion</span> <span class="hljs-keyword">and</span> segmentation).<span class="hljs-number">3.</span><span class="hljs-keyword">Dynamic</span> weight factor, automatically adjust the <span class="hljs-keyword">corresponding</span> weights<span class="hljs-keyword">of</span> <span class="hljs-keyword">each</span> task (optimal parameters).<span class="hljs-number">4.</span>Interactive feature training scheme.<span class="hljs-number">5.</span>Construct an imaging <span class="hljs-keyword">system</span> (benchmark).</code></pre></div><h3 id="structure-diagram-2">Structure diagram</h3><figure><img src="/img/stage1/31.png"alt="Workflow of the proposed SegMiF. The left part depicts the latent interactive relationship between image fusion and segmentation. The middle part plots the concrete architecture of the SegMiF. The right part details the components of proposed hierarchical interactive attention." /><figcaption aria-hidden="true">Workflow of the proposed SegMiF. The leftpart depicts the latent interactive relationship between image fusionand segmentation. The middle part plots the concrete architecture of theSegMiF. The right part details the components of proposed hierarchicalinteractive attention.</figcaption></figure><figure><img src="/img/stage1/32.png" alt="Partial zoom" /><figcaption aria-hidden="true">Partial zoom</figcaption></figure><h3 id="分析-1">分析</h3><figure><img src="/img/stage1/33.png" alt="Fusion network and DRDB module" /><figcaption aria-hidden="true">Fusion network and DRDBmodule</figcaption></figure><p><img src="/img/stage1/34.png"alt="Detailed architectures of SoAM and MoAM" />注：分割网络是论文[SegFormer: Simple and Efficient Design for SemanticSegmentation with Transformers]的内容</p><p>分割网络： <img src="/img/stage1/36.png"alt="Segmentation network" /></p><h4 id="hia">HIA</h4><p>1.SoAM (Semantic-oriented attention module)</p><blockquote><p>SoAM utilizes the token <spanclass="math inline">\(F_{seg}^{s}\)</span> to generate the query <spanclass="math inline">\(Q_s\)</span>, which represents the inhere semanticinformation that needs to be enhanced.</p></blockquote><blockquote><p>The global context representation of each can be calculated by as<span class="math inline">\(K_{ir}^{T}\cdot V_{ir} (G_{ir})\)</span> and<span class="math inline">\(K_{vis}^{T}\cdot V_{vis} (G_{vis})\)</span>, 其中K, V ∈ {<span class="math inline">\(F_{ir}^{s}\)</span>, <spanclass="math inline">\(F_{vis}^{s}\)</span>}</p></blockquote><p><span class="math inline">\(S_{ir} = Q_s\cdot G_{ir}\)</span>, <spanclass="math inline">\(S_{vis} = Q_s\cdot G_{vis}\)</span></p><blockquote><p>SoAM to provide more semantic attention for the modality feature.</p></blockquote><p>2.MoAM (Modality-oriented attention module)</p><blockquote><p>MoAM introduce two modality queries <spanclass="math inline">\(Q_{ir}\)</span> and <spanclass="math inline">\(Q_{vis}\)</span> ∈ {<spanclass="math inline">\(F_{ir}^{m}\)</span>, <spanclass="math inline">\(F_{vis}^{m}\)</span>}</p></blockquote><blockquote><p>The global context of segmentation <spanclass="math inline">\(G_s\)</span> by <spanclass="math inline">\(K_{s}^{T}\cdot V_s\)</span></p></blockquote><p><span class="math inline">\(M_{ir} = Q_{ir}\cdot G_s\)</span>, <spanclass="math inline">\(M_{vis} = Q_{vis}\cdot G_s\)</span></p><blockquote><p>MoAM to investigate the significant feature from semanticcontexts.</p></blockquote><h4 id="目标函数">目标函数</h4><figure><img src="/img/stage1/35.png" alt="Joint formulation" /><figcaption aria-hidden="true">Joint formulation</figcaption></figure><p>损失函数： <img src="/img/stage1/37.png" alt="Loss" /></p><div class="code-wrapper"><pre><code class="hljs armasm">损失函数中涉及了两个常用的概念：结构相似度、显著性图<span class="hljs-number">1</span>.结构相似度SSIM主要用来衡量两幅图亮度、对比度和结构的相似性  详见<span class="hljs-string">&quot;专业笔记&quot;</span>中所述<span class="hljs-number">2</span>.显著性图主要用来计算MSE损失中的显著性参数m  源自论文: Infrared <span class="hljs-keyword">and</span> visible image fusion based on visual saliency <span class="hljs-meta">map</span>            <span class="hljs-keyword">and</span> weighted least square optimization</code></pre></div><h3 id="相关内容">相关内容</h3><div class="code-wrapper"><pre><code class="hljs mathematica">当<span class="hljs-variable">CNN</span>层数变深时，输出到输入的路径就会变得很长。梯度反向传播，到达输入层可能就会消失。<span class="hljs-variable">DenseNet</span>是一种深度卷积神经网络，引入密集连接（<span class="hljs-variable">Dense</span> <span class="hljs-variable">Connection</span>）将前面所有层与后面的层建立密集连接。与<span class="hljs-variable">ResNet</span>的关键区别是，<span class="hljs-variable">ResNet</span>是简单相加，<span class="hljs-variable">DenseNet</span>是进行连接。<span class="hljs-variable">DenseNet</span>通过基本构建单元<span class="hljs-variable">Dense</span> <span class="hljs-built_in">Block</span>实现稠密连接对特征进行重用，实现信息共享，并能增强梯度流动，避免梯度消失。过渡层（<span class="hljs-variable">Transition</span> <span class="hljs-variable">Layer</span>）控制通道数（稠密块会带来通道数的增加），防止模型过于复杂。<span class="hljs-variable">DenseNet</span>在模型精度和泛化能力上通常表现优异，训练更稳定，但结构仍比较复杂，需要消耗较多的计算资源和时间，内存占用大。<span class="hljs-variable">ResNet</span>（<span class="hljs-variable">Residual</span> <span class="hljs-variable">Neural</span> <span class="hljs-variable">Network</span>）是基于残差学习框架的神经网络，其在前向网络中增加了一些快捷连接<span class="hljs-variable">Shortcut</span><span class="hljs-punctuation">(</span><span class="hljs-built_in">Short</span><span class="hljs-punctuation">)</span> <span class="hljs-variable">Connection</span><span class="hljs-operator">/</span><span class="hljs-built_in">Skip</span> <span class="hljs-variable">Connection</span>，这些连接会跳过某些层，将数据直接传到之后的层。<span class="hljs-variable">ResNet</span>对网络深度增加带来的梯度消失或爆炸、网络退化（由于训练和测试误差的积累导致正确率趋于饱和甚至下降，与过拟合（训练误差小，测试误差大，泛化能力差）不同）等问题具有一定的作用，增加了非线性，一定程度上抑制了语义间隙的影响，但模型表现可能略逊一筹。残差块（<span class="hljs-variable">Residual</span> <span class="hljs-built_in">Block</span>）是<span class="hljs-variable">ResNet</span>的基本组成成分。</code></pre></div><div class="code-wrapper"><pre><code class="hljs arcade">空洞卷积（Dilated/Atrous Convolution）可以增加卷积核的尺寸，如原本<span class="hljs-number">3</span>*<span class="hljs-number">3</span>的卷积核可以扩充至<span class="hljs-number">5</span>*<span class="hljs-number">5</span>，但有效的参数个数不变，仍为<span class="hljs-number">9</span>个，剩余的位置不予考虑（空洞/零），利用超参数“扩张率”来定义卷积核处理数据时各值的间距（可以理解为空洞数），在扩大了感受野的同时，避免了Pooling操作，从而保持分辨率不变。PS：正常扩大感受野会带来计算量的增加，后面进行池化的降采样处理可以降低计算量，但是空间分辨率也随之降低了。感受野（Receptive Field）越大，捕获的图像区域越大，对图像全局的特征提取能力也就越强。而且对于目标检测任务而言，最后一层特征图（<span class="hljs-built_in">Feature</span> <span class="hljs-built_in">Map</span>）的感受野大小要大于等于输入图像大小，否则分类性能会不理想。一般而言，感受野越大、网络越深，对复杂问题求解的模型性能越好。</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>专业笔记</title>
    <link href="/2023/11/25/Professional/"/>
    <url>/2023/11/25/Professional/</url>
    
    <content type="html"><![CDATA[<h3 id="terminology-index">Terminology Index</h3><div class="code-wrapper"><pre><code class="hljs mipsasm">IQA：图像质量评价，image quality assessment上游任务，预训练模型，对应 low-level task；下游任务，具体的 task，对应 high-level taskAblation study：消融实验，移除 Model 的部分<span class="hljs-string">&quot;feature&quot;</span>（控制变量法），对比研究模型的性能FPS：每秒传输帧数，frames per secondFine-tune：微调光谱（Spectrum）：复色光经过分光成为单色光，经成像系统得到按波长或者频率依次排列的光学图像多光谱（<span class="hljs-keyword">Multispectrum）：同时获取多个光学频谱波段（大于3个），在可见光基础上向红外、紫外两个方向扩展</span><span class="hljs-keyword"></span>高光谱（Hyperspectrum）：包含成百上千的波段，可以捕获和分析一片空间区域内“逐点”的光谱Embedding：在深度学习中利用线性或非线性转换，对复杂的数据进行自动特征抽取，并将其features表示为向量形式，以便于输入到网络中进行处理。这个过程被称之为<span class="hljs-string">&quot;Embedding&quot;</span><span class="hljs-keyword">BN(Batch </span><span class="hljs-keyword">Normalization)：像深度学习模型输入数据时，要对data进行归一化操作，如均值为0、方差为1的规范化处理，</span><span class="hljs-keyword"></span>否则过大的特征会“淹没”小特征。这种情况不仅会在输入模型时出现，也会在层与层中存在，因为随着数据的逐级传播，经过损失函数（如Softmax）后会使得特征之间的差异变大，逐渐累积会对最终结果造成较大的影响。所以需要对层间数据也进行规范化，<span class="hljs-keyword">BN就是在两个隐藏层之间对数据在Batch方向进行Norm处理，</span><span class="hljs-keyword"></span>并且最后加入了<span class="hljs-keyword">Scale </span><span class="hljs-keyword">and </span><span class="hljs-keyword">Shift操作，以提高网络对Norm的学习能力。</span><span class="hljs-keyword"></span>LN(Layer <span class="hljs-keyword">Normalization)：与BN不同之处在于，LN是在数据的Channel方向进行Norm归一化操作。</span><span class="hljs-keyword"></span><span class="hljs-keyword">BN是对一个batch的数据的对应位置上所有的feature进行归一化，而LN是对每个sample进行归一化。</span><span class="hljs-keyword"></span>在三维视觉任务的情形下，可以理解为：<span class="hljs-keyword">BN对每一张图片对应位置的像素做Norm处理，LN则是对单幅图像的整体做Norm。</span><span class="hljs-keyword"></span>但通常在数据传入Model前会进行数据的归一化处理，因此通常不会再使用LN。</code></pre></div><p><img src="/img/stage1/bn.png" alt="Batch Norm" /> <imgsrc="/img/stage1/ln.png" alt="Layer Norm" /></p><h3 id="距离度量">距离度量</h3><div class="code-wrapper"><pre><code class="hljs arcade">欧氏距离（Euclidean <span class="hljs-built_in">Distance</span>）：两点连线长度城区距离（曼哈顿距离，Manhattan <span class="hljs-built_in">Distance</span>）：直角移动距离，不涉及对角线棋盘距离（切比雪夫距离，Chebyshev <span class="hljs-built_in">Distance</span>）：沿某个轴的距离最大值汉明距离（Hamming <span class="hljs-built_in">Distance</span>）：比较两等长二进制串不同值的个数余弦相似度（Cosine Similarity）：两向量夹角余弦表示距离，与内积有关</code></pre></div><h3 id="图片格式">图片格式</h3><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-keyword">JPEG，静态图像压缩标准，压缩比越高，质量越差</span><span class="hljs-keyword"></span><span class="hljs-keyword">JPG，与JPEG类似，相当于简化版，去除了相机的拍照参数等</span><span class="hljs-keyword"></span>PNG，无损压缩，位图片格式GIF，静态、动态图像，如动图、表情包TIFF，无失真压缩，占用空间较大TGA，兼顾 <span class="hljs-keyword">BMP </span>图像的质量与 <span class="hljs-keyword">JPEG </span>的体积<span class="hljs-keyword">BMP，位图 </span><span class="hljs-keyword">Bitmap，不采用压缩，占用空间大</span><span class="hljs-keyword"></span>SVG，二维矢量图形格式RAW，经 CMOS 或 CCD 图像感应器将捕捉到的光信号转换为数字信号的原始数据HDR，高动态范围图像，High-Dynamic Range，记录了照明信息</code></pre></div><h3 id="拓扑结构">拓扑结构</h3><p>什么是拓扑结构？</p><p>所谓“拓扑”，就是把实体抽象成与其大小、形状无关的“点”，把连接实体的线路抽象成“线”，进而以图的形式来表达这些点与线的关系。这个图被称为拓扑结构图，表示的关系是拓扑结构。</p><div class="code-wrapper"><pre><code class="hljs">几何结构：强调点与线构成的形状及大小，考察点、线（甚至面）的位置关系。拓扑结构：注重点、线之间的连接关系。</code></pre></div><p>例子：梯形、矩形、平行四边形、圆具有不同的形状，因此属于不同的几何结构，但是组成它们的点、线连接关系是一样的，因此具有相同的拓扑结构（环形结构）。</p><p>在计算机网络中，我们把计算机、终端、通信处理机等设备抽象成点，把连接这些设备的通信线路抽象成线，其构成的拓扑叫网络拓扑结构，反应网络的结构关系。</p><div class="code-wrapper"><pre><code class="hljs">几种常见的网络拓扑结构：总线型、星型、环型、树型、网状型等。</code></pre></div><p><a href="https://zhuanlan.zhihu.com/p/451069548?utm_id=0"title="什么是拓扑结构？">详情点击此处查看</a></p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>basic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列5：数码</title>
    <link href="/2023/11/25/NumericalCode/"/>
    <url>/2023/11/25/NumericalCode/</url>
    
    <content type="html"><![CDATA[<p>正在施工中……</p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>functional</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列4：ICT</title>
    <link href="/2023/11/25/ICT/"/>
    <url>/2023/11/25/ICT/</url>
    
    <content type="html"><![CDATA[<h3 id="显示器接口">显示器接口</h3><ol type="1"><li>VGA接口</li></ol><blockquote><p>采用模拟协议，常用于老式显示器。目前不常用了，像一个四圆角梯形。</p></blockquote><figure><imgsrc="https://pica.zhimg.com/80/v2-9260a9e7bb2d44713e0976a8ac2ee667_1440w.webp?source=2c26e567"title="VGA接口" alt="VGA接口" /><figcaption aria-hidden="true">VGA接口</figcaption></figure><ol start="2" type="1"><li>DVI接口</li></ol><blockquote><p>有很多种类，算是VGA到HDMI的过渡产物。</p></blockquote><figure><imgsrc="https://pica.zhimg.com/80/v2-5dd3c4951a6a5c178e773deb1ba6b422_1440w.webp?source=2c26e567"title="DVI-D接口" alt="DVI-D接口" /><figcaption aria-hidden="true">DVI-D接口</figcaption></figure><ol start="3" type="1"><li>HDMI接口</li></ol><blockquote><p>音视频同时传输，支持高动态范围HDR成像，很常用，像带两个弯角的矩形。</p></blockquote><figure><imgsrc="https://picx.zhimg.com/80/v2-50888e26e96dafd6b0b5df25451923b9_1440w.webp?source=2c26e567"title="HDMI接口" alt="HDMI接口" /><figcaption aria-hidden="true">HDMI接口</figcaption></figure><ol start="4" type="1"><li>DP接口</li></ol><blockquote><p>DP(Digital Port)，也很常用，是HDMI的竞争对手，有很大的发展前景。</p></blockquote><figure><imgsrc="https://picx.zhimg.com/80/v2-3c1c4109ffc3f017bfdea221a49b77ca_1440w.webp?source=2c26e567"title="DP接口" alt="DP接口" /><figcaption aria-hidden="true">DP接口</figcaption></figure><h3 id="dns">DNS</h3><p>DNS(Domain NameSystem，域名系统)，用于域名解析，是因特网上作为域名和IP地址互相映射的一个分布式数据库，能让用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。</p><p><a href="https://zhuanlan.zhihu.com/p/186028919"title="什么是DNS？">点击查看详情</a></p><h3 id="路由器相关">路由器相关</h3><h4 id="直连">直连</h4><p>用一根网线连接两台主机的网口，可以相互发送和接收数据。<div class="code-wrapper"><pre><code class="hljs">多台主机通信怎么办？</code></pre></div></p><h4 id="集线器">集线器</h4><p>集线器(HUB)，将网线集结起来，实现初级的网络互通，工作在物理层，半双工通信。同时可以将信号放大后再传输以扩大传输距离。<div class="code-wrapper"><pre><code class="hljs 1c">集线器以广播形式以<span class="hljs-string">&quot;泛红转发&quot;</span>来发送数据，无法分辨具体的主机。</code></pre></div></p><h4 id="交换机">交换机</h4><p>交换机(Switch)，在集线器原有功能上，增加了自动寻址能力和交换作用，通过学习MAC地址（MAC地址表），查找对应的端口号，建立临时交换路径进行收发数据，工作在数据链路层，全双工通信。<div class="code-wrapper"><pre><code class="hljs">要求交换机端口上所有主机在同一个子网中。不同网段的主机怎么通信？</code></pre></div></p><h4 id="路由器">路由器</h4><p>路由器(Router)，连接不同类型的网络并能选择数据传输的IP路径，充当网关的角色，能在多网络互联环境中建立灵活的连接，工作在网络层。<div class="code-wrapper"><pre><code class="hljs arduino">家里拉了一条宽带（一条网线，现在多为光纤），只能一个人享受上网。如果要多设备连接网络，或者享受无线上网(<span class="hljs-built_in">WiFi</span>)，那么就需要安装路由器实现IP地址分配、网络共享和无线网使用（无线路由器），甚至进行网络加速（增强）。</code></pre></div></p><h4 id="猫">猫</h4><p>猫(Modem)，即光猫，调制解调器，进行光电转换。为了取代之前慢速的电话线上网，目前运营商多采用光纤传输，利用高速的光信号（会被限速，按你交的钱让你使用相应的带宽）进行数据传输。光信号无法直接被设备利用，经过光猫，将其转换为数字信号，即可被设备使用。</p><h4 id="ipmac地址">IP/MAC地址</h4><p>IP(InternetProtocol)，即互联网协议地址，为互联网上每一个网络和每一台主机配置唯一的逻辑地址，与物理地址区分。</p><p>IP地址分为IPv4和IPv6，IPv4使用32位（4字节）地址，IPv6地址长度为128位。最初设计互联网络时，为了便于寻址和层次化构造网络，每个IP地址包括两个标识码（ID），也就是网络ID和主机ID。同一个物理网络上的所有主机都使用同一个网络ID，网络上的一个主机（包括网络上工作站，服务器和路由器等）有一个主机ID与其对应。以IPv4地址为例，IP地址分为：1、公有地址(Publicaddress)，我们通过公有IP地址是可以实现直接访问因特网的。2、私有地址(Privateaddress)，分为五类：A类、B类、C类、D类、E类。其中，A、B、C类私有地址是由InternetNIC公司在全球范围内统一分配的，D、E类为特殊地址。<div class="code-wrapper"><pre><code class="hljs dns"><span class="hljs-keyword">A</span>类IP地址(适用于大型网络)的网络的标识(网络ID)长度为<span class="hljs-number">8</span>位，主机标识(主机ID)长度为<span class="hljs-number">24</span>位，它的范围：<span class="hljs-number">1.0.0.1</span>到<span class="hljs-number">127.255.255.254</span>；B类IP地址(适用于中型网络)的网络ID为<span class="hljs-number">16</span>位，主机ID长度为<span class="hljs-number">16</span>位，它的范围：<span class="hljs-number">128.0.0.1</span>-<span class="hljs-number">191.255.255.254</span>；C类IP地址(适用于小型网络)网络ID为<span class="hljs-number">24</span>位，主机ID长度为<span class="hljs-number">8</span>位，它的范围：<span class="hljs-number">192.0.0.1</span>-<span class="hljs-number">223.255.255.254</span>；D类地址被叫做多播地址(multicast address)，即组播地址，它的范围：<span class="hljs-number">224.0.0.0</span>到<span class="hljs-number">239.255.255.255</span>；E类地址主要用于Internet试验和开发，它的范围：<span class="hljs-number">240.0.0.0</span>~<span class="hljs-number">255.255.255.255</span></code></pre></div></p><p>MAC地址（Media Access ControlAddress）的全称叫做媒体访问控制地址，也称作局域网地址、以太网地址或者物理地址。MAC地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的MAC地址。MAC地址共48位（6个字节），前24位由IEEE（电气和电子工程师协会）决定如何分配，后24位由实际生产该网络设备的厂商自行制定。</p><p>OSI模型（Open System Interconnection ReferenceModel），是一种概念模型，是一个标准，一个试图使各种计算机在世界范围内互连为网络的标准框架。<div class="code-wrapper"><pre><code class="hljs armasm">第一层：物理层（Physical Layer），它是提供物理链路、传递电信号或光信号用的在局域网上传输比特流，它负责管理计算机通信设备和网络媒体之间的互通包括针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等第二层：数据链路层（<span class="hljs-meta">Data</span> Link Layer），负责网络寻址、错误侦测和改错当表头和表尾被加至数据包时，会形成帧（数据帧：<span class="hljs-meta">data</span> frame）而且此层还负责MAC地址第三层：网络层（Network Layer），决定数据的路径选择（数据选路）和转寄将网络表头（NH）加至数据包，以形成分组网络表头包含了网络数据，例如：<span class="hljs-built_in">IP</span>地址第四层：传输层（Transport Layer），它会建立一个安全通道，以防数据丢失端到端之间的连接建立，把传输表头（TH）加至数据以形成数据包传输表头包含了所使用的协议等发送信息，例如：传输控制协议（TCP）第五层：会话层（Session Layer），负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接第六层：表达层（Presentation Layer），将信息数据进行加密，及数据的转化信息数据经过加密、转换、压缩，转换为能与接收者的系统格式兼容并适合传输的格式第七层：应用层（Application Layer），起调用的作用提供为应用软件而设的接口，设置与另一应用软件之间的通信例如: HTTP，HTTPS，FTP，TELNET，SSH，SMTP，POP3.HTML.等</code></pre></div></p><p>IP地址与MAC地址的区别：1、IP地址应用于OSI模型的网络层，而MAC地址应用在OSI模型的数据链路层。2、地址长度、设计理念不同、分配依据不同。3、IP地址未必都不一样，与地理区域等有关。MAC地址只和硬件设备有关。</p><p>IP地址的提出，主要是为了减少广播的数量，可以智能学习目标地址在哪个网卡。</p><p><a href="https://www.zhihu.com/question/21546408"title="有了 IP 地址，为什么还要用 MAC 地址？">点击查看详情</a></p><h3 id="存储">存储</h3><p><a href="https://zhuanlan.zhihu.com/p/166633984"title="存储技术入门详解">点击跳转：存储技术入门详解</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列3：生活常识</title>
    <link href="/2023/11/25/Informal%20Essay%2003/"/>
    <url>/2023/11/25/Informal%20Essay%2003/</url>
    
    <content type="html"><![CDATA[<h3 id="常见亲属称谓">常见亲属称谓</h3><p>父亲的哥哥 &gt; 伯父、伯伯、大爷（~~~伯母、大娘）</p><p>父亲的弟弟 &gt; 叔叔（~~~婶婶）</p><p>父亲的姐妹 &gt; 姑姑（~~~姑父/夫）</p><p>母亲的兄弟 &gt; 舅舅（~~~舅妈）</p><p>母亲的姐妹 &gt; 姨（~~~姨夫/父）</p><p>父亲之父 &gt; 爷爷（祖父）</p><p>父亲之母 &gt; 奶奶（祖母）</p><p>母亲之父 &gt; 姥爷、外公、外爷（外祖父）</p><p>母亲之母 &gt; 姥姥、外婆（外祖母）</p><p>爷爷的姐妹 &gt; 姑奶（~~~姑爷）</p><p>爷爷的兄弟 &gt; 爷爷</p><p>奶奶的姐妹 &gt; 姨奶（~~~姨爷）</p><p>奶奶的兄弟 &gt; 舅爷（~~~舅奶）</p><h3 id="名酒记">名酒记</h3><p>中国新老八大名酒： <div class="code-wrapper"><pre><code class="hljs">茅台  汾酒  泸州老窖（特曲）古井贡酒  五粮液  董酒剑南春  洋河大曲  西凤酒</code></pre></div> 安徽酒： <div class="code-wrapper"><pre><code class="hljs">古井贡酒  口子窖  高炉家酒  文王贡酒迎驾贡酒  金种子  宣酒  皖酒  金坛子酒店小二酒  沙河王  明光酒  ···</code></pre></div></p><h3 id="处方药和非处方药">处方药和非处方药</h3><p>无论处方药还是非处方药，都必须经过国家食品药品监督管理局（CFDA）的批准才能上市，包装盒上必须注明国药准字。</p><div class="code-wrapper"><pre><code class="hljs gcode">国际上通用 Rx <span class="hljs-comment">(拉丁文Recipe，请求)</span>表示处方药，用 OTC <span class="hljs-comment">(Over The Counter，可以在柜台上买到的)</span>表示非处方药。包装盒上只有国药准字，没有Rx或OTC标识的，为处方药。</code></pre></div><p>非处方药又分为甲、乙两类，在包装盒正面分别用红色OTC、绿色OTC标识。</p><p>乙类非处方药一般不需要医生或药师的指导，消费者可以自行购买，看说明书使用即可，风险较低。</p><p>使用区别：</p><blockquote><p>处方药（无标识或Rx），医生处方，需要仔细阅读说明书并在医师指导下使用（服用）。</p></blockquote><blockquote><p>非处方药（红色或绿色OTC），医生处方、药师指导或自我诊断用，仔细阅读说明书并按照说明书或药师指导下使用。</p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/27355994?utm_id=0"title="你会识别处方药和非处方药吗？">点击此处查看详情</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列2：省会</title>
    <link href="/2023/11/25/Informal%20Essay%2002/"/>
    <url>/2023/11/25/Informal%20Essay%2002/</url>
    
    <content type="html"><![CDATA[<h3 id="省份直辖市-简称-首都省会">省份/直辖市-简称-首都（省会）</h3><div class="code-wrapper"><pre><code class="hljs">北京市-京-北京  天津市-津-天津  上海市-沪-上海  重庆市-渝-重庆黑龙江省-黑-哈尔滨  吉林省-吉-长春  辽宁省-辽-沈阳  内蒙古自治区-蒙-呼和浩特新疆维吾尔族自治区-新-乌鲁木齐  西藏自治区-藏-拉萨  甘肃省-甘-兰州青海省-青-西宁  陕西省-陕-西安  宁夏回族自治区-宁-银川  河南省-豫-郑州河北省-冀-石家庄  安徽省-皖-合肥  山西省-晋-太原  湖南省-湘-长沙湖北省-鄂-武汉  江苏省-苏-南京  四川省-蜀-成都  贵州省-黔-贵阳云南省-云-昆明  广西壮族自治区-桂-南宁  广东省-粤-广州  山东省-鲁-济南浙江省-浙-杭州  江西省-赣-南昌  福建省-闽-福州  海南省-琼-海口香港特别行政区-港-香港  澳门特别行政区-澳-澳门  台湾省-台-台北</code></pre></div><h3 id="行政区划分">行政区划分</h3><p>省级行政区：23个省，5个自治区，4个直辖市，2个特别行政区，共计34个省级行政区。</p><p>地级行政区：地级市（含省会）、地区、自治州、盟。</p><p>县级行政区：县（包括县级市、自治县、旗、自治旗）、市辖区、特区、林区。</p><p>乡级行政区：街道办事处、镇、乡（包括民族乡、苏木）、县辖区。</p><p>村级：居民委员会、村民委员会。</p><p>PS：根据中华人民共和国宪法，人民代表大会是中国最高权利机关，各级人民代表大会选举各级人民政府作为行政机关。所以严格来说，只有拥有政府和人大的级别才算行政区划，没有的就不算。因此，地区、盟、街道办事处、村民委员会、居民委员会均不是行政区划，其中村民委员会、居民委员会是基层群众自治组织，地区、盟、街道办事处是上级政府的派出机构。相应的，地区和盟下辖的县级行政区，实际上直接属于上级行政区（省、自治区），街道办事处下辖的村委会和居委会也同样直接隶属于县、县级市、市辖区。</p><h3 id="中国地理区域">中国地理区域</h3><p>行政大区是在中国建国初期设置的，是当年位于省级之上的行政区划。当时为了便于管理，政府将全国划分为六大行政区：华北（北京）、东北（沈阳）、华东（上海）、中南（武汉）、西南（重庆）、西北（西安）。</p><p>中国七大地理区域：华东地区、华南地区、华北地区、华中地区、东北地区、西南地区、西北地区。</p><p><a href="https://zhuanlan.zhihu.com/p/107971624?utm_source=weibo"title="点击此处查看你不知道的中国各种地理区域划分！">⪧中国地理划分⪦</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列1：朝代</title>
    <link href="/2023/11/25/Informal%20Essay%2001/"/>
    <url>/2023/11/25/Informal%20Essay%2001/</url>
    
    <content type="html"><![CDATA[<h4 id="夏朝">夏朝</h4><p>大禹 ---------- 桀</p><h4 id="商朝">商朝</h4><p>成汤 ---------- 商纣王</p><h4 id="周朝">周朝</h4><p>分封诸侯，是中国历史上最长的朝代，从西周【周武王】到东周【周平王】，终于【郝王】。<div class="code-wrapper"><pre><code class="hljs">东周时期分为春秋、战国春秋五霸：齐 晋 楚 秦 宋战国七雄：秦 韩 赵 魏 楚 燕 齐诸子百家也在这一时期</code></pre></div></p><h4 id="秦国">秦国</h4><p>始皇帝：嬴政 ---------- 胡亥 <div class="code-wrapper"><pre><code class="hljs">奴隶社会的终结，封建社会的开始，提出并实行郡县制</code></pre></div></p><h5 id="楚汉之争">楚汉之争</h5><p>项羽 VS 刘邦</p><h4 id="西汉">西汉</h4><p>始于汉高祖：刘邦</p><h5 id="新朝">新朝</h5><p>王莽篡汉，成立新朝</p><h4 id="东汉">东汉</h4><p>光武帝刘秀 ---------- 汉献帝刘协</p><h4 id="三国">三国</h4><p>曹魏（曹操）、蜀汉（刘备）、孙吴（孙权）</p><h4 id="西晋">西晋</h4><p>司马炎（司马昭之子）建立</p><h5 id="东晋-五胡十六国-南北朝">东晋 五胡十六国 南北朝</h5><p>乱世时期</p><h4 id="隋朝">隋朝</h4><p>隋文帝杨坚 ---------- 隋炀帝杨广</p><h4 id="唐朝">唐朝</h4><p>唐高祖李渊 ---------- 唐景宗李柷 &gt; 开元盛世：玄宗李隆基 &gt;贞观之治：太宗李世民</p><h5 id="五代十国">五代十国</h5><p>乱世时期</p><h4 id="北宋">北宋</h4><p>宋太祖赵匡胤 ---------- 宋钦宗赵恒（宋徽宗赵佶之子）</p><h5 id="辽">辽</h5><p>实为北宋前由耶律阿保机建立</p><h5 id="金">金</h5><p>完颜阿骨打建立，灭辽、北宋，亡于南宋和蒙古</p><h4 id="南宋">南宋</h4><p>宋高宗赵构（亦为徽宗之子）建立</p><h4 id="元朝">元朝</h4><p>忽必烈建立 <div class="code-wrapper"><pre><code class="hljs">成吉思汗建立蒙古政权，算元朝的前身元朝是首次由少数民族建立的朝代</code></pre></div></p><h4 id="明朝">明朝</h4><p>明太祖朱元璋 ---------- 末代皇帝崇祯（朱由检）</p><h4 id="清朝">清朝</h4><p>由清太宗皇太极改国号为清 ———— 顺治皇帝爱新觉罗•福临一统 ————末代皇帝宣统帝（溥仪） <div class="code-wrapper"><pre><code class="hljs">努尔哈赤建立的是后金（称汗），不算清帝</code></pre></div></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markdown基本语法</title>
    <link href="/2023/11/24/Markdown/"/>
    <url>/2023/11/24/Markdown/</url>
    
    <content type="html"><![CDATA[<h3 id="一标题">一、标题</h3><p>在文字前加井号 # 表示标题，共支持六级标题。</p><p><strong>注：几乎所有语法符号后要跟个空格再接内容</strong></p><p>示例： <div class="code-wrapper"><pre><code class="hljs clean"># 一级标题## 二级标题### 三级标题···以此类推···</code></pre></div></p><h3 id="二文字与排版">二、文字与排版</h3><ul><li><p><strong>加粗</strong> 在两个星号 * ··· *之间写的内容会被加粗</p></li><li><p><em>斜体</em> 将要倾斜的文字前后各用两个 * 包起来</p></li><li><p><strong><em>斜体加粗</em></strong> 前后各三个 *</p></li><li><p><del>删除线</del> 前后各两个 ~</p></li></ul><p>注：以上不加空格</p><ul><li>换行 &lt;br/&gt; (&lt;br&gt;···&lt;/br&gt;)</li></ul><p>可以内嵌Html语法，例如：段落&lt;p&gt;&lt;/p&gt;(可能不加结束标签也可，但不要依赖这种做法！)、链接&lt;ahref="URL名"&gt;显示内容&lt;/a&gt;、图片&lt;img src="URL名" width="宽度"height="高度" /&gt;等</p><ul><li>引用 在引用的文字前加一个或多个 &gt; 即可</li></ul><p>效果：</p><blockquote><p>引用1</p></blockquote><blockquote><blockquote><p>引用2</p></blockquote></blockquote><ul><li>分割线 用三个及以上的 * 或 - 单独成行</li></ul><hr /><ul><li><p>列表</p><ul><li><p>无序列表 用 * 或 - 或 + 都行，后接空格</p></li><li><p>有序列表 数字加点 <div class="code-wrapper"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> list1<span class="hljs-bullet">2.</span> list2</code></pre></div></p></li><li><p>嵌套列表 上下两级之间敲两个空格即可</p></li></ul></li><li><p>代码块 (``````)用括号中的反引号将需要标注的块区域包起来，三个就行，但是自动补全机制写六个更省事（不加括号，自成一行）</p><p>单行代码：前后各用一个 ` 就行</p><p>代码块： <div class="code-wrapper"><pre><code class="hljs">这里是代码块</code></pre></div> 单行代码： <code>这里是单行代码</code></p><p>支持语法高亮，在首行 ` 后紧跟着写相应语言即可，如python</p></li><li><p>脚注用方括号[]和^可以构成一个脚注，可在后方写注释，注释显示在文末</p><p>脚注写法：[^1] <br/> 注释写法：[^1]: Content</p><p>效果：脚注<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="这是一个脚注^_^">[1]</span></a></sup></p></li></ul><h3 id="三插入超链接和图片">三、插入超链接和图片</h3><ol type="1"><li><p>[超链接名] (超链接地址 "超链接title") 示例：[百度](http://baidu.com)</p></li><li><p>![图片名] (图片地址 "图片title") 图片名为下标</p></li><li><p><URL名> 可点击的链接，也可填入Email地址</p></li></ol><p>注：名和地址之间中间不加空格，title可不加<br>title与地址间空一格，内容在鼠标悬停于超链接或图片上时显示</br></p><h3 id="四表格">四、表格</h3><p>用管道符 | 来分隔各列，用 ------ (多个) 来创建（隔开）表头。</p><p>示例： <div class="code-wrapper"><pre><code class="hljs gherkin">|<span class="hljs-string"> theHead1 </span>|<span class="hljs-string"> theHead2 </span>|<span class="hljs-string"> theHead3 </span>|<span class="hljs-string"> theHead4 </span>||<span class="hljs-string"> -------- </span>|<span class="hljs-string"> :------- </span>|<span class="hljs-string"> -------: </span>|<span class="hljs-string"> :------: </span>||<span class="hljs-string"> 内容1 </span>|<span class="hljs-string"> 内容2 </span>|<span class="hljs-string"> 内容3 </span>|<span class="hljs-string"> 内容4 </span>||<span class="hljs-string"> 内容1 </span>|<span class="hljs-string"> 内容2 </span>|<span class="hljs-string"> 内容3 </span>|<span class="hljs-string"> 内容4 </span>|</code></pre></div></p><table><thead><tr class="header"><th>theLongHead1</th><th style="text-align: left;">theLongHead2</th><th style="text-align: right;">theLongHead3</th><th style="text-align: center;">theLongHead4</th></tr></thead><tbody><tr class="odd"><td>内容1</td><td style="text-align: left;">内容2</td><td style="text-align: right;">内容3</td><td style="text-align: center;">内容4</td></tr><tr class="even"><td>内容1</td><td style="text-align: left;">内容2</td><td style="text-align: right;">内容3</td><td style="text-align: center;">内容4</td></tr></tbody></table><table><thead><tr class="header"><th>1</th><th style="text-align: left;">2</th><th style="text-align: right;">3</th><th style="text-align: center;">4</th></tr></thead><tbody><tr class="odd"><td>超长的内容1</td><td style="text-align: left;">超长的内容2</td><td style="text-align: right;">超长的内容3</td><td style="text-align: center;">超长的内容4</td></tr><tr class="even"><td>超长的内容1</td><td style="text-align: left;">超长的内容2</td><td style="text-align: right;">超长的内容3</td><td style="text-align: center;">超长的内容4</td></tr></tbody></table><p>在分隔行的连字符 - 左侧、右侧或两侧可以加冒号 :使得该列内容左对齐、右对齐或居中显示，默认（不加冒号时）左对齐。</p><h3 id="扩展语法">扩展语法</h3><h4 id="任务列表语法">任务列表语法</h4><p>短横杠 - 加方括号 [ ] （都有空格） <br/> 选择（复选框）则用 x替换方括号中的空格：[x]</p><p>效果：</p><ul class="task-list"><li><label><input type="checkbox"checked="" />这是一个复选框</label></li><li><label><input type="checkbox" />这是一个普通任务项</label></li></ul><h4 id="注意事项">注意事项</h4><p>使用hexo-renderer-pandoc渲染器，卸载了hexo自带的marked渲染器后，有些Markdown语法格式与LaTex语法格式冲突，因此需要注意以下几点：</p><ol type="1"><li>不要在代码块(`划定的区域)中插入公式，会无法解析/渲染</li><li>引用&gt;时，或者用其他符号如任务列表语法-[]、标题#时，最好单独成行，前、后各空一行，LaTex对“吃”空格/行</li><li>使用公式要在Front-matter中指定："math: true"</li><li>有脚注时，要在文末空个两行，防止脚注紧贴正文，正文格式会影响脚注的显示格式</li><li>空一行及以上，渲染结果就是内容之间间隔一行；换行渲染后仅相当于空格；可用&lt;br/&gt;来换行</li></ol><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>这是一个脚注^_^<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>auxiliary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/11/24/HelloWorld/"/>
    <url>/2023/11/24/HelloWorld/</url>
    
    <content type="html"><![CDATA[<p>This is my first article.</p><p>In order to create my own blog, I studied for several days, andfinally achieved initial results.</p><hr /><p>I've been studying for days to create a blog of my own, and I'mfinally getting there.</p><p>This shows that it is very important to learn English well.</p>]]></content>
    
    
    <categories>
      
      <category>Default</category>
      
    </categories>
    
    
    <tags>
      
      <tag>foreword</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
