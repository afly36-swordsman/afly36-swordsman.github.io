<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>随笔系列6：计算机基础</title>
    <link href="/2023/12/21/Computer/"/>
    <url>/2023/12/21/Computer/</url>
    
    <content type="html"><![CDATA[<h3 id="磁盘">磁盘</h3><h4 id="分区知识1簇">分区知识1：簇</h4><p>在磁盘（如U盘）格式化的时候，有一项是“分配单元大小”，这就是分区中的“簇”。“簇”是分区的最小存储单元。一个数据在存入硬盘前，会事先按照簇的大小将其分成N等份，然后分别存入分区中不同的簇内。例如名叫“毕业论文.pdf”的文件，大小是1600KB，将其存入磁盘时有以下情况：</p><p>（1）如果分配单元大小设置为512KB，那么意味着该磁盘的“簇”大小为512KB。在数据存入前，系统会先将其分为512KB、512KB、512KB、64KB四份，然后分别存入硬盘对应的四个簇中，这样一来存在一个问题，最后的64KB不够填满一个512KB的簇，那么剩下的448KB空间也就被浪费了。其实，即使最后一个簇中仅使用了1个字节（Byte），这个簇剩下的全部空间也会被弃之不用。</p><p>（2）如果将分配单元大小设置为64KB，那么数据存入前，系统会按64KB将这个数据分为25份，然后存入对应的25个簇中。</p><p>将簇设置为512KB时，最后会有一部分空间被浪费，但是它存取数据时只需分成4份就完成了，因此理论上讲效率更高。而将其分成64KB时，虽然对空间的利用率更高、不容易造成浪费，但存取效率也会相对降低。故簇越小，效率越低，但浪费的空间也小；簇越大，效率越高，浪费的空间也多。在格式化时，如果不调整这个参数，系统默认将“分配单元大小”设置为4096字节，即4KB，由于在x86时代，每个内存页的大小被定义为4KB，因此认为将硬盘的块大小也分成4KB可以与内存页吻合而提升硬盘的运行效率。</p><p>另外，目前市面上主流的硬盘格式有三种：APFS、NTFS、exFAT，特点如下。</p><div class="code-wrapper"><pre><code class="hljs stata">APFS：优点：可靠，安全。对配合苹果电脑使用友好。缺点：不兼容Windows系统。NTFS：优点：采用日志式，稳定安全，Windows系统使用友好，是主流格式。缺点：<span class="hljs-keyword">Mac</span>只支持读，写入要第三方驱动。exFAT：优点：对Windows和<span class="hljs-keyword">MAC</span>格式都兼容，对闪存SSD硬盘优化更好。缺点：机械硬盘用这格式数据容易丢。</code></pre></div><h4 id="易混淆知识1磁盘与硬盘">易混淆知识1：磁盘与硬盘</h4><p>磁盘是计算机主要的存储介质，可以存储大量的二进制数据。早期的计算机使用的磁盘是软磁盘（FloppyDisk，简称软盘），如今常用的是硬磁盘（Harddisk，即硬盘）。磁盘是一个广泛概念，包括多种存储介质，如磁盘、光盘（用激光打上去的数据）等，其中磁盘以磁性原理存储数据，包括盘片、磁头、盘片主轴、控制电机、磁头控制器、数据转换器等，存储容量由扇区数、磁道数、磁头数决定。</p><p>硬盘，即硬盘驱动器，英文名称是Hard DiskDrive，简称是HDD。硬盘可分为固态硬盘（Solid StateDrive，SSD）、机械硬盘（Hard Disk Drive，HDD）、混合硬盘（Hybrid HardDrive，SSHD orHHD），而硬盘就是磁盘的一种。硬盘是一种常见的外部存储设备，它将数据保存在一个磁盘上，使用一个磁头扫描磁盘上的磁道，读取或者写入数据，主要采用磁盘作为存储媒介（机械硬盘）。</p><h4 id="易混淆知识2内存外存与硬盘">易混淆知识2：内存外存与硬盘</h4><p>存储器分类：存储芯片种类很多，依据不同的特性有多种分类方法。</p><p>1.按存取方式分类</p><p>(a)随机存取存储器(Random Access Memory, RAM)这种存储器每个单元读写时间一样，且与各单元所在位置无关。我们平常说的手机内存，8G或者12G的，指的就是这种。（注：这里的读写时间一样主要强调地址译码时间相同。现在的DRAM芯片采用行缓冲，因而可能因为位置不同而使访问时间有所差别）</p><p>(b)顺序存取存储器(Sequential Access Memory, SAM)数据按顺序从存储载体的始端读出或写入，因而存取时间的长短与信息所在位置有关。例如：磁带。</p><p>(c)直接存取存储器(Direct Access Memory, DAM)直接定位到读写数据块，在读写数据块时按顺序进行。例如：磁盘。</p><p>(d)相联存储器(Associate Memory, AM)按内容检索到存储位置再进行读写。例如：快表，一种高速缓冲存储器(Cache)。</p><p>2.按存储介质分类</p><p>(a)半导体存储器：双极型、静态SMOS型、动态DMOS型。</p><p>(b)磁表面存储器：磁盘（Disk）、磁带（Tape）。</p><p>(c)光存储器：CD、CD-ROM、DVD。</p><p>3.按信息的可更改性分类</p><p>(a)读写存储器(Read/Write Memory)：可读可写。</p><p>(b)只读存储器(Read Only Memory, ROM)：只能读不能写。</p><p>4.按断电后信息的可保存性分类</p><p>(a)非易失性存储器(NonvolatileMemory)：信息可一直保留，不需电源维持。如：ROM、磁表面存储器、光存储器等。</p><p>(b)易失性存储器(VolatileMemory)：电源关闭时信息自动丢失，如：RAM、Cache等。</p><p>5.按功能/容量/速度/所在位置分类</p><p>(a)寄存器(Register)这种存储器封装在CPU内，用于存放当前正在执行的指令和使用的数据。用触发器实现，速度极快，容量很小（几个字节到几十字节）。</p><p>(b)高速缓存(Cache)位于CPU内部或附近，用来存放当前要执行的局部程序段和数据。用SRAM实现，速度可与CPU匹配，容量小（几十KB到几MB）。</p><p>(c)内存储器MM(又称主存储器Main(Primary) Memory)位于CPU之外，用来存放已被启动的程序及所用的数据。用DRAM实现，速度较快，容量较大（目前来说通常是2GB到16GB）。</p><p>(d)外存储器AM(又称辅助存储器Auxiliary/Secondary Storage)位于主机之外，用来存放暂不运行的程序、数据或存档文件。用磁表面或光存储器实现，容量大而速度慢。</p><p>目前我们讨论较多的还是按照第4种分类方式，因为这样分只有2类，描述方便，当然第3种方式也是分为两类，不过目前存储器都支持读写，只读的很少，所以一般说第4类种分法较多。这里再补充个闪存（FlashMemory）：内存RAM使用寄生电容充放电来表示0和1，充了电的电容需要不断的被充电维持状态，否则会很快漏电，所以需要持续供电，但是速度块。Flash闪存使用MOS管中间的一个绝缘体包括层来储备电子，充电之后可以长期保存，外部使用senseamp比较放大器来感受每个MOS管内的状态，从而输出0和1状态，断电后数据依然保存，比RAM慢一个数量级，但是比磁盘快多个数量级。闪存相当于一种电子式可擦除程序化只读存储器（EEPROM）的变种，与EEPROM不同的是，EEPROM能在字节水平上进行删除和重写而不是整个芯片擦写，而闪存的大部分芯片需要块擦除。</p><p>前面说到，内存是保存进程的相关数据。一旦关机（断电），内存里面的数据就随之清空。对手机来说，像照片、文档什么的，并不是保存在内存中，而是存在闪存里面，也就是手机中的128G、256G等。内存越大，可以同时运行的软件就越多，闪存越大，可以存储的文件就越多。目前的手机闪存大多用的是NandFlash（eMMC和UFS是两种通信协议/标准），也有少数用Nor Flash的。</p><p>对电脑来说，电脑除了内存，也要有存文件的地方，那么是不是和手机一样也是闪存呢？有人马上会说：不是闪存，是固态硬盘或者机械硬盘，这种说法不完全对。接下来先从协议和接口开始简述，然后再叙述硬盘的种类。</p><p>目前电脑大多使用固态硬盘SSD，SSD数据通信普遍是PCle标准。注意，M.2不是通信协议，也不是数据通道，而是接口标准。举个简单的例子：USB接口，常见的有Type-A和Type-C，拿type-c来说，它可以支持多种通信协议，比如雷达协议、dp协议等。M.2接口类型既可以走PCle通道，也可以走SATA通道，所以有双M.2的说法。值得注意是的是，PCle通道有专门的PCle接口类型。当然，SATA通道也有专门的接口类型：SATA接口。</p><p>也就是说，硬盘与主板连接的接口，主要有SATA、PCIe、M.2三种形式，但从主板插槽到主板芯片之间，传输数据的总线（就是上面提到的“数据通道”）有SATA和PCIe两种。基于此，M.2接口有两种规范，一种是socket2，一种是socket3，前者支持SATA3和PCIex2的总线接口，后者支持PCIe x4的总线接口。</p><p>最后还有个NVMe协议。电脑bios里面硬盘的一个设置项，可选择为AHCI或者ATAPI。一块SATA接口的硬盘，可以选择使用AHCI传输协议或者ATAPI传输协议。不过ATAPI是一个并行ATA传输协议，主要用在旧式的IDE接口上，现在大多数情况下已经不再被使用。对M.2接口来说，如果走SATA通道，只能使用AHCI协议，而不能用NVMe协议；如果走PCle通道，既可采用AHCI协议，也可以采用NVMe协议。NVMe是一个专门为固态硬盘设计的协议，它比AHCI更高效，更适合高速的固态硬盘。因此，对于高性能的M.2接口固态硬盘，NVMe协议通常是首选。</p><p>固态硬盘SSD是用固态电子存储芯片阵列而制成的硬盘，由闪存（FLASH）作为基础存储介质的存储设备。手机上的闪存和电脑的固态，存储介质都是NandFlash，也就是说它们的存储芯片是一样的，只是控制芯片不同。FLASH的速度比机械硬盘速度快很多，所以被广泛应用到存储设备。不仅是手机的闪存和电脑的固态硬盘，目前的U盘、SD卡等都是FLASH。还有一种SSD存储介质用的是DRAM。优点：读写速度快，防震抗摔能力强，功耗和噪音低，工作温度范围大。缺点：容量小，寿命短，价格高。</p><figure><imgsrc="https://picx.zhimg.com/v2-d2f31ec719f785b0e30e6e413ffcdb54_r.jpg?source=1def8aca"alt="固态硬盘" /><figcaption aria-hidden="true">固态硬盘</figcaption></figure><p>机械硬盘HDD所有盘片装在一个旋转轴上，每张盘片都是平行的，所有磁头连在一个磁头控制器上，由控制器负责各个磁头的运动。</p><p><imgsrc="https://picx.zhimg.com/v2-113779599ab48ab31795c3f75856e6ae_r.jpg?source=1def8aca" /><imgsrc="https://picx.zhimg.com/v2-4fd207fea287e89ab87d590697eb731d_r.jpg?source=1def8aca"alt="机械硬盘" /></p><h4 id="分区知识2磁盘接口与分区">分区知识2：磁盘接口与分区</h4><p>我们都知道一块磁盘是可以被分成多个分区（partition）的，以Windows系统来看，你会有一块磁盘并且将它分区成了C、D、E盘，这三个盘就是3个分区。</p><p>个人计算机常见的磁盘接口有两种，分别是IDE接口和SATA接口，目前主流的已经是SATA接口了。我们称可连接到IDE接口的设备为IDE设备，不管是磁盘还是光盘。</p><p>在Linux系统中，以IDE接口来说，主机会提供两个IDE接口，每个接口连接两个设备，所以最多可以连接到4个设备。每个接口的IDE设备又有Master和Slave之分。</p><p>再以SATA接口来说，由于SATA/USB/SCSIA等磁盘接口都是使用SCSI模块来驱动的，因此这些接口的磁盘设备文件名都是使用SCSI模块来驱动的，因此这些接口的磁盘设备文件名都是/dev/sd[a-p]的格式。但是与IDE接口不同的是，SATA/USB接口的磁盘设备文件名根本就没有一定的顺序，顺序是由Linux内核检测到磁盘的顺序决定的。</p><p>想了解硬盘分区，首先需要清楚硬盘的存储机制。硬盘的两个单位是扇区（Sector）和柱面（Cylinder），其中每个扇区大小为512Bytes。每个扇区的重要程度是不一样的，整块磁盘的第一个扇区特别重要，因为它记录了整块磁盘的重要信息。磁盘的第一个扇区主要记录了两个重要的信息，分别是：主引导分区（MasterBoot Record,MBR）：可以安装引导加载程序的地方，有446bytes；分区表（PartitionTable）：记录整块硬盘分区的状态，有64bytes。</p><p>硬盘默认的分区表仅能写入四组分区信息，这四组分区信息我们称为主（Primary）分区或扩展（Extended）分区。当系统要写入磁盘时，一定会参考磁盘分区表，才能针对某个分区进行数据的处理。虽然分区表只有记录四组数据的空间，但不代表一块硬盘最多只能分出四个分区。扩展分区的目的是使用额外的扇区来记录分区信息，扩展分区本身并不能被拿来格式化。我们可以通过扩展分区所指向的那个区块继续作分区的记录。由扩展分区分配继续切出来的分区，就被称为逻辑分区（Logicalpartition）。</p><p>逻辑分区是在扩展分区上面创建的，相当于一块存储介质，和操作系统还有别的逻辑分区、主分区没有什么关系，是独立的。逻辑分区可以看作是“裸”的分区，也就是一块“裸”的硬盘，没有操作系统在里面。逻辑分区相当于一个独立的个体，有自己的文件系统，有自己的操作系统。扩展分区则不是真正意义上的分区，它仅仅是一个指向下一个分区的指针，这种指针结构将形成一个单向链表。这样在主引导扇区中除了主分区外，仅需要存储一个被称为扩展分区的分区数据，通过这个扩展分区的数据可以找到下一个分区（逻辑磁盘）的起始位置，以此位置类推可以找到所有的分区。无论系统中建立多少个逻辑磁盘，在主引导扇区中通过一个扩展分区的参数就可以逐个找到每一个逻辑磁盘。</p><p>综上，主分区与扩展分区最多可以有四个（硬盘的限制），扩展分区最多只能有一个（操作系统的限制）且无法格式化。逻辑分区是由扩展分区继续切割出来的分区，能够被格式化后作为数据存取的分区为主分区与逻辑分区。逻辑分区的数量随着操作系统不同而不同。</p><p><ahref="https://zhuanlan.zhihu.com/p/27926239">硬盘基础知识科普</a></p><p><a href="https://www.cnblogs.com/hust-ghtao/"title="这里作者记载了很多互联网知识！">互联网相关知识学习-华科小涛的博客</a></p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>basic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理 · 海纳篇</title>
    <link href="/2023/12/20/papers4/"/>
    <url>/2023/12/20/papers4/</url>
    
    <content type="html"><![CDATA[<h2 id="l-puif">L-PUIF</h2><p>A Lightweight Pixel-Level Unified Image Fusion Network,2023(IEEE)</p><h3 id="motivation">Motivation</h3><p>传统的图像融合虽然computationalcost很低，但是随着GPU的发展，高计算能力平台使其这一优势变得越来越平常。而且传统的像素级融合在提取特征和计算融合策略时高度依赖expertknowledge，且需要人工设计融合规则，这会限制模型的泛化能力，同时这些方法普遍需要根据融合的类型调整参数，这一过程比较复杂。因此深度学习技术成为unifiedimage fusion的主流。</p><blockquote><p>The feature extraction methods of these traditional methods are toosingle and difficult to adapt to different types of fusion tasks, whichgreatly limits the fusion performance.</p></blockquote><p>但是一些deep learning networks的网络性能仍然需要priorknowledge来支撑，比如需要groundtruth去指导融合的迭代过程，或者是使用fixed-parameter feature extractionnetwork分析源图像的信息。此外，还有一些深度学习网络使用大量的参数来达到更高的融合质量。现有的一些方法也没有过多关注不同类型源图像之间的dataspecificity，这将限制融合网络的性能。</p><div class="code-wrapper"><pre><code class="hljs livecodeserver">Hence, how <span class="hljs-built_in">to</span> <span class="hljs-built_in">combine</span> <span class="hljs-keyword">the</span> specificity <span class="hljs-keyword">of</span> source images is <span class="hljs-keyword">the</span> key <span class="hljs-built_in">to</span> image fusion.</code></pre></div><p>The existing methods are only aimed at the fusion effect or therobustness of the model, ignoring the requirement of real-time needs. Infact, the FPS of the existing network is far lower than the standard of30 when facing high-resolution datasets.</p><p>为了克服上述问题或缺陷，作者设计了一个轻量化的像素级统一（联合）图像融合网络：L-PUIF，适用于三种融合场景：多模态、多聚焦和多曝光。</p><h3 id="introduction">Introduction</h3><p>作者发现广泛使用高维多尺度特征提取模式不适合构建轻型网络，因为高维、多尺度特征缺少局部细节信息，会增加计算成本和图像重建的复杂性。</p><p>基于此，作者提出一个basenetwork，由4个不同的卷积层组成，用于特征提取与重建；一个high-dimensionalrefinement(精炼、细化) process，增强特征提取的信息挖掘（informationmining）能力。</p><p>提取的高维特征通过应用于损失函数的information preservationmeasurement生成权重，引导网络对structure, intensity, and other info fromthe source images进行合理利用。</p><p>这样，网络就可以实现无监督图像融合过程，减少模型参数，实现统一、准确、高效率的图像融合。</p><p>作者的contributions总结如下：</p><blockquote><p>A lightweight network named L-PUIF is proposed, which avoids theredundant structural and can be adapted to different pixel-level imagefusions with one training model.</p></blockquote><blockquote><p>A new measurement method of information preservation degree for imagefusion has been proposed, which can adaptively enhance the gradient andintensity info extraction ability of the network.</p></blockquote><h3 id="method">Method</h3><p>作者提出的L-PUIF方法可以消除先验信息和人工操作的限制，轻量化网络模型，具有良好的实时性、较高的融合效率和较好的可视化融合结果。</p><p>特征提取在图像融合中是相当重要的环节。作者将特征提取分为两类主要的模式：一种是高维度特征提取，如FusionGAN，另一种是多尺度特征提取，如RFN-Nest。实验表明，small-scalefeatures和high-dimensionalfeatures包含的信息几乎都是全局信息和语义信息，而局部细节和强度信息丢失。这两种模式的网络结构很大，不利于图像重建的快速实现。然而，texture,intensity, and otherinfo可以从尺度不变特性和低维特征中获得，对图像融合和重建起重要作用。即：这两种patterns在构建轻量级融合网络中不是必需的。</p><h4 id="base-network">1.Base Network</h4><p>该网络主要由特征提取和特征重构模块两部分组成。首先，我们使用具有相同权重的特征提取模块来处理两个不同的输入。这样，网络的特征提取参数可以通过两个不同的源进行优化，可以减少网络模型的大小。特征提取模块有两层组合，每一层由卷积层和激活函数组成。</p><p>我们没有使用手动融合策略，例如加法或L1范数，而是选择拼接不同源图像生成的特征，并将它们放入特征重建模块中。特征重构模块的结构与特征提取模块相同。具体来说，它完全由3*3卷积构建，特征重建模块中最后一层的激活函数是tanh函数，而其余激活函数是LeakyReLU。最后，根据融合图像和源图像计算损失函数。</p><p><img src="/img/stage2/stage3/01.png" /><!-- ![](https://tvax4.sinaimg.cn/large/008iio8ply1hly1awjg92j30jw0btn1k.jpg) --></p><h4 id="high-dimensional-information-refinement">2.High-DimensionalInformation Refinement</h4><p>基础网络仅将来自不同来源的图像融合到一张图像中，但由于网络体积的限制，网络的特征提取能力并不突出。因此，需要通过其他方法提高基础网络的特征提取能力。我们针对基础网络提出了一种高维信息处理方法，增强了对所需信息的利用，优化了整个网络融合过程。</p><p>具体思想是：高维特征由基础网络的特征提取模块生成，该模块包含大量源信息。我们通过high-dimensionalinformationrefinement过程重新提取高维特征，并在随后的步骤中将提取的信息注入到损失函数中。这样，特征提取模块可以通过特征重构过程和高维特征提取过程进行优化。这种优化方法还增强了网络中前后特征的交互性。</p><p>我们使用1*1的卷积和tanh激活函数来进行高维特征提取，输入是基础网络的特征提取结果。将生成的张量提供给informationpreservation measurement过程进行进一步处理，网络架构如下图所示。</p><!-- ![](https://tvax2.sinaimg.cn/large/008iio8ply1hlxxld7jozj30jq09cwh9.jpg) --><p><img src="/img/stage2/stage3/02.png" /></p><h4 id="information-preservation-measurement">3.Information PreservationMeasurement</h4><p>在基础网络中，来自不同源图像的信息对融合结果的影响是相等的，无法处理数据特异性和其他属性。因此，有必要为损失函数获得合理的权重来定义不同源图像的影响。U2Fusion为这个问题提供了初步解决方案。U2Fusion使用具有确定参数的VGG-16网络提取源图像不同维度和尺度的特征，然后计算这些特征的梯度，并为损失函数生成两个不同的权重。这样就考虑了不同的源图像对融合图像的影响。</p><p>然而，这些信息测量方法不是逐个像素地实现的，而且需要一些先验信息来提高测量精度。因此，我们提出了一种更好的方法来评估融合信息。首先，我们认为这个权重可以从像素级进行确定，其可以灵活地反映图像中不同区域的差异。其次，不需要预定义权重，而是通过从源图像训练网络来获得（学习）权重。基于上述考虑，我们提出了一种信息保存测量方法，该方法使用两个high-dimensionalinformationrefinement过程生成的张量作为输入，评估梯度和强度信息。梯度和强度是融合中最关键的两种信息类型，它们可以同时反映图像的局部和全局信息。这两种信息为损失函数生成两组不同的权重W1,W2; W3,W4。这种像素级融合权重可以根据源图像的特征自适应调整，使融合结果更适合不同类型的融合。同时，这些权重有利于信息的存储和计算。</p><p>梯度信息权重<span class="math inline">\(W_1\)</span>和<spanclass="math inline">\(W_2\)</span>计算方式如下：</p><p><span class="math display">\[W_1 = \varepsilon (\vert F_{grad}(\Phi_1)\vert - \vert F_{grad}(\Phi _2)\vert)\]</span> <spanclass="math display">\[W_2 = 1 - W_1\]</span></p><p><span class="math inline">\(\varepsilon()\)</span>表示阶跃函数，<span class="math inline">\(\Phi_1\)</span>和<span class="math inline">\(\Phi_2\)</span>表示不同中间信息层生成的特征。这样，我们就得到了融合图像中每个元素与源图像之间独特的对应关系。</p><p>为了进一步处理不同源图像强度的不均匀，我们减去同源元素的平均值，以减少非同源元素之间的差异。然后，使用sqrt函数扩大元素值之间的差异。因此，非同源元素被调节为平均值以促进元素强度的比较，可以表达为：</p><p><span class="math display">\[b_1 = sqrt(sigmoid(\Phi_1 -F_{mean}(\Phi_1)))\]</span> <span class="math display">\[b_2 =sqrt(sigmoid(\Phi_2 - F_{mean}(\Phi_2)))\]</span></p><p><spanclass="math inline">\(F_{mean}(\Phi)\)</span>表示元素的总和除以元素个数。为了获得<spanclass="math inline">\(b_1\)</span>和<spanclass="math inline">\(b_2\)</span>之间的相关性，将它们归一化为<spanclass="math inline">\(c_1\)</span>和<spanclass="math inline">\(c_2\)</span>：</p><p><span class="math display">\[[c_1, c_2] = [\dfrac{b_1}{b_1+b_2},\dfrac{b_2}{b_1+b_2}]\]</span></p><p>使用sigmoid函数对强度信息权重<spanclass="math inline">\(W_3\)</span>和<spanclass="math inline">\(W_4\)</span>归一化：</p><p><span class="math display">\[W_3 = sigmoid(c_1 -F_{mean}(c_1))\]</span> <span class="math display">\[W_4 = sigmoid(c_2 -F_{mean}(c_2))\]</span></p><p><span class="math inline">\(W_1, W_2, W_3,W_4\)</span>可以作为融合图像各个像素（元素）纹理和强度的约束信息，还可以帮助网络控制不同源图像的信息保存。这是L-PUIF的关键之处。</p><h4 id="loss-function">4.Loss Function</h4><p>损失函数在图像融合中起着重要作用，它决定了不同源图像之间的信息相关性程度。更具体地说，源图像不同区域的信息量有很大的差异。在像素级图像融合中，应根据这些差异确定不同信息的使用。因此，我们使用损失函数来限制各种类型的信息之间的比例关系。</p><p>损失函数主要包括两部分：梯度损失<spanclass="math inline">\(L_{grad}\)</span>和强度损失<spanclass="math inline">\(L_{int}\)</span>，系数<spanclass="math inline">\(\lambda\)</span>权衡二者的比重。</p><p><span class="math display">\[L_{L-PUIF} = \lambda _1 L_{grad} +\lambda _2 L_{int}\]</span></p><p><spanclass="math inline">\(L_{L-PUIF}\)</span>计算融合图像与源图像之间的相似性（差异）。<spanclass="math inline">\(W_1\)</span>和<spanclass="math inline">\(W_2\)</span>用于梯度损失中，用来衡量不同源图像的影响。结构相似度指数测度(SSIM)和均方误差(MSE)用于计算图像的结构相似度和细节相似度。因此，梯度信息可用于保留融合图像中的局部纹理信息：</p><p><span class="math display">\[L_{grad1} = 2 - SSIM_{(W_1 \odot I_1,\W_1 \odot I_f)} - SSIM_{(W_2 \odot I_2,\ W_2 \odot I_f)}\]</span> <spanclass="math display">\[L_{grad2} = MSE_{(W_1 \odot I_1,\ W_1 \odot I_f)}+ MSE_{(W_2 \odot I_2,\ W_2 \odot I_f)}\]</span></p><p>上式中<span class="math inline">\(I_1\)</span>和<spanclass="math inline">\(I_2\)</span>代表源图像，<spanclass="math inline">\(I_f\)</span>代表融合图像，<spanclass="math inline">\(\odot\)</span>代表multiplication，即张量之间对应元素相乘。</p><p><span class="math display">\[L_{grad} = L_{grad1} + \alphaL_{grad2}\]</span></p><p><spanclass="math inline">\(\alpha\)</span>用来平衡两个损失，文中取20来确保可以平衡图像信息的内容。</p><p>与梯度损失的信息约束类似，<spanclass="math inline">\(W_3\)</span>和<spanclass="math inline">\(W_4\)</span>用于梯度损失中，使用MSE计算融合图像和源图像之间的强度相似度：</p><p><span class="math display">\[L_{int} = MSE_{(W_3 \odot I_1, \ W_3\odot I_f)} + MSE_{(W_4 \odot I_2,\ W_4 \odot I_f)}\]</span></p><h4 id="multiple-rgb-inputs">5.Multiple RGB Inputs</h4><p>对于多通道彩色图像输入，将其转换至YCbCr颜色空间中，对Y通道进行L-PUIF融合，对Cb和Cr通道进行加权融合。融合完成后，将融合图像转为RGB图像即可。具体过程见下图所示。</p><!-- ![](https://tvax4.sinaimg.cn/large/008iio8ply1hlxwyqlhjzj30ig0bpju1.jpg) --><p><img src="/img/stage2/stage3/03.png" /></p><p>PS (PostScript, 附言)：</p><div class="code-wrapper"><pre><code class="hljs livecodeserver">对应元素相乘: <span class="hljs-keyword">a</span>*b | torch.mul(<span class="hljs-keyword">a</span>,b) | np.<span class="hljs-built_in">multiply</span>(<span class="hljs-keyword">a</span>,b)矩阵乘法: <span class="hljs-keyword">a</span>@b | torch.mm(<span class="hljs-keyword">a</span>,b) | np.matmul(<span class="hljs-keyword">a</span>,b) | np.dot(<span class="hljs-keyword">a</span>,b)</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理 · 脉络篇（二）</title>
    <link href="/2023/12/20/papers3/"/>
    <url>/2023/12/20/papers3/</url>
    
    <content type="html"><![CDATA[<p>地址：<a href="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">部分基于深度学习的红外与可见光图像融合模型总结</a></p><hr /><h2 id="tc-gan">1.TC-GAN</h2><p>Infrared and Visible Image Fusion via Texture Conditional GenerativeAdversarial Network (Transactions on Circuits and Systems for VideoTechnology, 2021)</p><p>本文模型TC-GAN的一大亮点是使用了引导滤波器，引导滤波器首次于2013年由K.He,J.Sun, andX.Tang提出，可以充分利用相邻像素之间的相关性，让图像包含特定的一致性。具体原理是引导滤波器会用一个引导图像Y来对输入图像X进行滤波，输出图像Z便能在保持X信息的基础上获得引导图像Y的变化趋势。</p><p>TC-GAN会生成一个组合纹理图来捕获梯度变化。生成器是编码器结构，用来提取细节，中间有一个SE-Net模块来提高纹理图中显著纹理信息的权重。判别器的作用是让融合图像的纹理细节更接近可见光图像。为了获得更好的纹理信息，作者们提出了一种使用组合纹理图和自适应引导滤波器的基于多决策图的融合策略。换句话说就是：组合纹理图记录了原图像的纹理，被用作自适应引导滤波器（AGF）的引导图像，然后采用AGF生成的多个决策图来设计融合策略重建融合结果。</p><p><imgsrc="https://pic2.zhimg.com/v2-e9276d6564b7334ca9c578aa9491cbed_r.jpg" /></p><p>首先红外与可见光图像会一起送入TC-GAN中生成组合纹理图，然后组合纹理图会被作为引导图像，用引导滤波器对源图像进行滤波，获得多个决策图。最后用多个决策图融合并重建图像。其中TC-GAN的具体结构如下图。</p><p><imgsrc="https://pic4.zhimg.com/v2-e2b79a5847c2d12984b2192401c3b7b3_r.jpg" /></p><p>生成器由编码器、SE-Net、解码器三部分组成。中间的SE-Net模块学习到的尺度向量对编码器提取的特征进行加权，可以在训练过程中增强有用的纹理特征，因此用在编码器后面来增强特征。要生成组合纹理图，需要在组合纹理图的监督下训练生成器，然而组合纹理图并不存在（还没生成呢）。因此本文是采用GAN的训练模式，定义了一个判别器，以包含丰富纹理细节的可见光图像作为标签来训练生成器。判别器是一个全卷积网络，对图像进行像素级别的分类，可以判断生成图像中的纹理分布是否与可见图像的纹理分布一致。</p><p>损失函数的设计：生成器损失函数包含gradient loss和adversarialloss两个部分，梯度损失是为了让生成的纹理图包含更多的细节，将对抗损失和梯度损失加权求和得到生成器的损失。判别器引导TC-GAN生成器的训练来识别融合图像与可见光图像，促使生成器得到的融合图像包含更多纹理细节，因此判别器利用交叉熵作为损失来计算融合图像与可见光图像之间的差异。</p><div class="code-wrapper"><pre><code class="hljs">对抗损失adversarial loss在所有用GAN模型的文章中都出现了，建议回顾相关资料</code></pre></div><p>附上CSDN关于对抗Loss理解的文章链接：<ahref="https://blog.csdn.net/qq_38784454/article/details/112464701">对抗Loss的理解</a></p><p>在经过TC-GAN获得组合纹理图作为引导图像对源图像进行滤波后，可以获得多个决策图，下面就是对这些决策图进行融合了。多决策图的融合包含两个步骤：1.基于组合纹理图创建多个决策图（组合纹理图作为引导滤波器的引导图像）；2.多决策图被用于获得初步的融合结果。最终的融合图像是通过加权法获得的。</p><h2 id="attentionfgan">2.AttentionFGAN</h2><p>Infrared and Visible Image Fusion Using Attention-Based GenerativeAdversarial Networks (Transactions on Multimedia, 2021)</p><p>AttentionFGAN将多尺度注意力机制加入GAN来进行红外-可见光图像的融合。对于判别器，优势主要是可以限制判别器对注意力区域关注得更多，而非全图都关注；对于生成器，多尺度注意力网络被用来训练学习一个特征的权重，使重要特征被赋予更多注意，冗余特征则被忽略。为了保留注意力区域更多信息，设计了attention损失函数。判别器改用Wassersteindistance计算源图像和融合图像之间的差异。从结果来看AttentionFGAN对红外显著性区域保护的很不错。</p><p>模型的结构如下图。生成器包含2个多尺度注意力网络和1个融合网络，两个多尺度注意力网络分别获得红外与可见光图像的注意力图，融合网络在重建图像时会更关注重点区域。2个判别器会让融合结果更好地保留像素幅度与细节信息，作用分别是区分融合图像与红外/可见光图像。</p><figure><imgsrc="https://pic2.zhimg.com/v2-ae207cc3db61021987f911f3caf658b1_r.jpg"alt="AttentionFGAN模型结构图" /><figcaption aria-hidden="true">AttentionFGAN模型结构图</figcaption></figure><p>生成器部分的两个多尺度注意力模块首先分别获得红外与可见光图像的注意力图，然后将这两个注意力图和源图像在通道维度拼接后送入融合网络。两个判别器分别用来区分融合图像与红外/可见光图像，结构完全相同，但是参数不共享。在训练阶段，将输入图像（融合图像）送入多尺度注意力网络计算注意力图，然后将注意力图和输入图像在通道维度拼接，让判别器能更关注具有区分力的区域。为了提高效果，这里用了WGAN中的Wassersteindistance来计算融合图像与源图像之间的距离。由于WGAN是专门计算Wassersteindistance的，可以看做为一个回归问题，因此在计算损失的时候将log函数去除了，判别器最后一个sigmoid层也被移除了。</p><p>上图中的多尺度融合网络，在生成器中的作用是获得注意力图，在判别器中的作用是让模型更关注具有判别能力的区域，它的结构如下图。</p><figure><imgsrc="https://pic2.zhimg.com/v2-8e772b2c1df5c6e388dbc728867b9d3d_r.jpg"alt="多尺度融合模块" /><figcaption aria-hidden="true">多尺度融合模块</figcaption></figure><p>首先使用卷积层提取特征，这里用最后两个卷积层的输出作为深层特征图。单尺度的特征难以提取到有效的空间信息，因此通过多个尺寸的全局池化来获得多尺度特征。但是在池化操作会产生太多冗余特征图，最好是选择重要特征，去除冗余特征，所以这里让网络基于每个特征图的全局信息来重新计算权重。这个过程由GP+FC+Sg三个模块实现。全局池化GP结束后用全连接层和sigmoid来计算权重。</p><p>获得了最终的权重后，用上采样算子对多尺度特征图进行上采样，获得相同尺寸的特征图。然后，用刚才获得的权重和上采样后的特征图乘累加，再通过一次标准化算子，最后的注意力图需要用最大选择策略，即不同尺度在各自分支获得的Features先在通道维度上拼接，再用最大值法获得最终的注意力图。以上就是多尺度融合网络的机制。</p><p>生成器的损失函数包含adversarial loss、content loss、attentionloss三项，contentloss让生成器生成和红外图像数据分布更接近的图像，因为红外图像对热量敏感，体现在像素幅度上；attentionloss是由于引入了多尺度注意力机制。当判别器无法区分融合图像和源图像时，判别器的两个输入应该具有相同的注意区域。例如，最终的融合结果应该从红外图像中保留足够的典型信息，然后当判别器无法区分融合图像和红外图像时，融合结果和红外图像应该具有相同的注意力图。因此为了从源图像中保留更多注意力区域的信息，设计了融合图像与源图像之间的attentionloss，对融合图像的注意力图与源图像的注意力图之间的差异进行惩罚（约束），具体的公式参考<ahref="https://zhuanlan.zhihu.com/p/467039982">专栏3.10</a>。</p><h2 id="drf">3.DRF</h2><p>Disentangled Representation for Visible and Infrared Image Fusion(Information Fusion, 2021)</p><p>这篇文章的损失函数和之前的方法差异较大。本文将图像分解为场景特征图和属性向量，分别表示两个模态的共同信息和不同信息，个人认为这也是对互补信息的一种探讨。</p><p>大致思想：本文将Disentangledrepresentation应用于可见光和红外图像融合。根据成像原理，将可见光和红外图像中的信息来源进行分解，即分别通过相应的编码器将图像分解为与场景模态和传感器模态（属性）相关的表示。这样，由属性（传感器模态）相关表示定义的唯一信息更接近于每种传感器单独捕获的信息。因此，可以缓解独特信息提取不当的问题。然后应用不同的策略来融合这些不同类型的表示。最后融合的表示被输入到预训练的生成器中以生成融合结果。</p><p>传统的图像分解方法会使用同种方法对图像进行分解，一般来说可能会造成关键信息丢失，也可能会生成冗余信息。一些方法针对不同的模态采取了不同的信息描述，例如对红外图像用像素幅度来描述，对可见光图像用梯度来描述，然而这种人工设计的信息分离方法也不能完整地描述图像信息。为了解决这个问题，应该尽可能地从源图像中的公共信息中分离出唯一信息。为此，可以从源图像的成像过程这个方向探讨信息的保留方式。无论源图像是从可见光传感器还是从红外传感器捕获的，它们都是从同一场景中拍摄的，其中包含大量（共同的）场景信息。不同之处在于这两种类型的传感器使用其特定的成像方式来捕获原始信息，具体方法是将源图像分解为两个部分：场景信息和传感器模态相关的信息。由于与传感器模态相关的信息反映了传感器或源图像的属性，我们将这类信息定义为唯一的属性表示；而来自场景的信息，即场景表示，是两者的共同信息。基于以上思考提出了新的融合模型：disentangledrepresentation for visible and infrared image fusion (DRF) 。</p><p>在DRF中，用disentangledrepresentation来解构源图像中的场景和属性表示。场景信息通过一个场景编码器被作为公共信息来提取，属性表示通过一个属性编码器被作为唯一信息来提取，结构如下图所示。</p><p><imgsrc="https://pic2.zhimg.com/80/v2-2c4280d2522c317019d7b7d34a761099_720w.webp" /></p><p><img src="/img/stage2/3_13.png" /></p><p><imgsrc="https://pic1.zhimg.com/80/v2-60b1c567cdfc12b80029083011a91f44_720w.webp" /></p><p>考虑到场景信息直接和空间、位置相关，所以场景表示就用特征图的方式来呈现（Fig1的形式）。而属性是和传感器模态相关的，不必捕获场景信息，所以属性更适合通过一个向量来表示。</p><p>为了实现Disentangledrepresentation，提出以下三个策略。首先，红外图像域X和可见光图像域Y的场景编码器最后一层参数共享，通过这样的方法就可以使两个域图像的场景特征嵌入到同一空间中了，但是共享高层权重的方式不能保证场景编码器对来自两个不同域的相同信息进行编码。所以第二个策略是对场景特征进行约束，让X和Y的场景编码器对来自两个域的相同场景特征进行编码。第三个策略，为了压缩属性空间中的场景信息，要对属性向量的分布进行约束，因此属性编码器不会对场景相关信息进行编码。</p><p>接下来，为了能让场景和属性两种信息表示源图像，那么通过场景S和属性A必须能够恢复源图像，因此加了一个生成器G来学习这种逆映射。考虑到Ax和Ay对生成器来说是不一致的，并考虑到后续的融合过程，{S,Sx}到X、{S,Ay}到Y的（逆）映射过程共享一个生成器，希望G能同时具有这两种映射的能力。</p><p>一方面，重建得到的x和y应该尽量和源图像x和y相似，这个容易理解。另一方面，S最好跨域X和Y捕获信息，而Ax和Ay应该捕获相应域的特定属性，而不携带域不变的场景相关线索。那么，假设X和Y是对同一场景的描述，那么<spanclass="math inline">\(s_x\)</span>和<spanclass="math inline">\(s_y\)</span>应该是相似的。相反地，给定不同的属性向量，生成器G生成的图像应该和提取属性向量那个模态的图像相同，而和没有提取属性向量那个模态的图像不同。举个例子，以<spanclass="math inline">\(s_x\)</span>和<spanclass="math inline">\(a_y\)</span>为条件，G生成的图像形式应该是这样的：<spanclass="math inline">\(y_x\)</span>是一个由X场景信息和Y的属性信息生成、重建结果看起来像Y的图像，<spanclass="math inline">\(y_x\)</span>和Y应该保持像素连续性。</p><p>网络结构：场景编码器分为红外场景编码器和可见光场景编码器，结构如下图，一共包含七个层，5个残差模块和2个卷积层。</p><p><imgsrc="https://pic1.zhimg.com/80/v2-89bda80f8e9896e943b202242316d188_1440w.webp" /></p><p>其中残差块的结构如下图所示：</p><p><imgsrc="https://pic4.zhimg.com/v2-448aa9b1b001efa7af25ff28d4acc247_r.jpg" /></p><p>属性编码器结构如下图，卷积层都是5×5的卷积核搭配长度为2的步长，然后在通道维度上进行全局平均池化，属性信息就被映射到向量形式了。</p><p><imgsrc="https://pic2.zhimg.com/v2-d850ada815fd015ec69af01efdbf2ad1_r.jpg" /></p><p>生成器结构：</p><p><imgsrc="https://pic4.zhimg.com/v2-2075bda13fb90502ce9b886a0e2651ef_r.jpg" /></p><p>对场景特征，首先通过一个残差块处理，而对属性向量，会被平铺到和场景特征图一样的尺寸，这样就能和场景特征图在通道维度上进行拼接了。再经过几个残差块后用反卷积层对特征图进行上采样。场景特征图的空间分辨率降低到原始图像的四分之一，因此丢失了许多高质量的纹理细节，为了弥补损失，将场景编码器中第一个残差块的输出，和生成器中第二个反卷积层的输出进行拼接，送入后续的卷积层中。再经过多个卷积层，通道数量被削减到和输入图像通道数量一样，最后一个tanh后就是重建得到的图像。</p><p>融合模块，由于场景特征共享同一场景，因此用平均策略融合<spanclass="math inline">\(s_x\)</span>和<spanclass="math inline">\(s_y\)</span>，属性特征<spanclass="math inline">\(a_x\)</span>和<spanclass="math inline">\(a_y\)</span>直接加权求和。以上两种特征图用一个预训练的生成器G来获得融合图像：<spanclass="math inline">\(f = G(s_f,a_f)\)</span></p><p>损失函数由以下几项构成：</p><p>(1)场景特征一致性损失Scene Feature Consistency Loss给定描述同一场景的源图像x和y，它们的场景特征需要类似。因此该项损失用1-范数或者Frobenius-norm进行约束。但是本文发现L1-norm更合适，因为红外和可见光传感器的成像原理不同，这两种源图像中的场景信息不可能完全相同，但是整体而言场景信息不同的区域还是非常小的，大部分区域的场景信息能保持相同就可以了，那些小部分场景信息不同的区域实在不行就算了。换句话说，希望两个模态场景信息中的差异是稀疏的，因此相比较Frobenius-norm，L1-norm更合适。</p><p>(2)属性分布损失Attribute Distribution Loss 基于Disentangledrepresentation的考虑，希望能在属性空间中尽量压缩场景信息，预计属性表示将与先验高斯分布一样接近。KL项会促进disentanglement，因此对x和y属性向量的分布施加约束，通过测量它们的分布和先验高斯分布之间的KL散度并对此进行约束。</p><p>(3)自重建损失Self-Reconstruction Loss原始源图像将根据场景和与之分离的属性表示进行重建。也就是说，生成器G应该能够将场景特征和属性向量解码回原始源图像。因此，我们执行自重建损失以使重建的图像与原始图像达到高保真度：<spanclass="math inline">\(L_{reconstruction} = \Vert x - \hat {x} \Vert_1 +\Vert y - \hat {y} \Vert_1\)</span></p><p>(4)域转换损失Domain-Translation Loss转换后的图像是根据一张源图像的场景特征和另一张源图像的属性向量生成的。<img src="/img/stage2/3_0.png" /></p><h2 id="年论文小结">19-21年论文小结</h2><p>总体来说自监督模型在损失函数方面，像素幅度的信息的计算使用的还是L2-norm/MSE较多，细节纹理信息的计算使用的是SSIM和梯度算子较多。2020年之前的很多模型为修改网络结构和损失函数的形式，早期的融合规则方面有使用attention形式的规则，但仍然是人工设计的融合规则，其中非学习方式产生的显著性图不能反映原图信息的有效性，还是应该用网络来学习融合规则。早期还有用预训练好的VGG进行特征提取，然后融合特征图的方法，但是提取到的特征不一定准确，毕竟不是专门通过多模态图像训练的网络。不过也逐渐有文章开始探讨什么是互补信息，以及如何在实际的融合过程中找出图像对中每个模态的互补信息（或者说一个区域中该模态有，而其他模态没有的信息）。</p><p>基于GAN的方法可以利用无监督对抗学习模型，充分利用源图像的信息。损失函数主要是设计contentloss和adversarialloss。难点是生成器和判别器之间的平衡难以达到。对于图像融合的模型结构，综述Imagefusion meets deep learning: A survey andperspective中认为当前用于图像融合的网络结构最有效的三种是残差连接、dense连接和双分支。提升深度学习模型的两种方式：1.设计高质量的指标用于无监督学习（高质量的评价指标不光可以用于损失函数，也对模型的真实效果评价有利）；2.构建接近真实场景的数据用于有监督学习。</p><p>未来需要根据任务设计更具有针对性的网络，比如做任务驱动的模型，根据下游任务来决定融合的损失函数，例如SeAFusion，不然的话融合效果的评价还是比较主观的。</p><h2 id="补充-4.transmef">补充 4.TransMEF</h2><p>A Transformer-Based Multi-Exposure Image Fusion Framework usingSelf-Supervised Multi-Task Learning (AAAI, 2022)</p><p>目前用CNN做融合的模型很多都把CNN用在编码器上用于特征提取，但是CNN本身存在感受野小、不能提取长程信息的问题。而图像融合中，融合图像的质量不仅和感受野内的每个像素相关，也和图像整体的像素强度与纹理信息有关，所以同时对全局和局部信息建模是有必要的。所以本文针对多曝光图像融合提出了transMEF模型，这是一种基于transformer的自监督多任务学习模型，也采用编码器-解码器结构，在大规模自然图像数据集上进行自监督训练。融合过程中，首先使用编码器提取源图像的特征图，然后用解码器生成融合图像。</p><p>总结本文创新点：1.根据多曝光图像的特点提出三个自监督重建任务，基于多任务学习训练出了一个自编码器，所以模型不仅能在大规模自然图像上训练，也可以学习到多曝光图像的特点；2.为了克服CNN难以捕获长程信息的缺陷，提出了包含transformer结构的编码器结构，这样在特征提取过程中局部和全局信息都能捕获；3.在一个benchmark上和其他12种模型，依据11个指标进行了对比。</p><p>模型总体结构图： <imgsrc="https://pic3.zhimg.com/v2-ee6336a02679c775d68ccf9473b4789e_r.jpg" /></p><p>网络的训练和以前使用CNN的编码器-解码器融合网络类似，即直接将整体作为一个图像重建来训练自编码器。给定一个输入图像，通过三种不同的变换（Gamma-basedtransformation: TG(·), Fourier-based transformation: TF (·) and globalregion shuffling:TS(·)）对源图像的多个区域产生失真，产生三种不同的失真图像，然后将这些失真图像送入编码器中。编码器包含一个特征提取模块（TransBlock）和一个特征增强模块（EnhanceBlock）。其中TransBlock包含CNN和transformer，CNN的输入直接将整图输入即可，而transformer则是将图像先分为多个patch，再送入transformer中训练。TransBlock的输出再送入EnhanceBlock对特征图进行聚合与增强，完成之后送入解码器中重建图像。上述三个任务使用多任务学习同时进行。自编码器训练好后，输入图像就能通过编码器得到特征图了，用上图b的方式对特征图进行融合。</p><p>基于transformer的编码器-解码器结构：前述所说第一阶段的重建训练首先要使用三种方式生成三种失真图像，具体做法是：在每种方式下，对一张输入图像随机产生10个子区域，每个子区域的宽和高是1到25之间的随机整数，然后每张图像在这10个子区域中进行变换，产生失真的子区域，替换源图像的该区域。而三种变换方式就对应TG(·)、TF(·) andTS(·)，和结构图中的三种方式一一对应。然后把这些失真图像送入TransBlock进行特征提取即可。而EnhanceBlock则是将TransBlock输出特征图的全局和局部信息进行聚合，具体是先将CNN和transformer分别输出的特征图在通道维度进行拼接，然后再送入两个卷积块（ConvBlock）中，以此来强化特征。结构图c展示了Transformer和ConVBlock模块的具体组成，每个卷积块包含两个3×3、padding为1的卷积层，解码器中的卷积块也是如此。</p><p>TransBlock：该模块下的CNN部分比较简单，直接级联三个卷积块即可，输入就是完整的三类失真图像。而transformer部分则是先将三类失真图像分成多个P×P大小的patch序列，再用Linearprojection获得embedding，将这些embedding送入L层的transformer，得到输出。Transformer的结构包含多头注意力MSA、LayerNorm、残差连接和MLP，其中MLP是两个使用RELU激活函数的线性层。</p><p>损失函数：损失函数用于多任务学习，针对三类失真图像同时训练三种图像重建任务，在上面的结构图a中最上方有Loss的公式，三项分别对应三类重建任务的损失。每一项任务的损失<span class="math inline">\(Loss_{Task_i}\)</span>都不仅考虑像素级损失，还要考虑结构性损失和梯度信息。三项分别设计为：计算像素级损失的MSE、计算结构损失的SSIM和计算总变分损失的TV范数。</p><p><span class="math display">\[L_{MSE} = \Vert I_{out} -I_{in}\Vert_2\]</span> <span class="math display">\[L_{SSIM} = 1 -SSIM(I_{out},I_{in})\]</span></p><p>第三项的totalvariation项（TV损失）用来减少图像的虚假边缘和噪声等失真，同时能保留源图的边缘信息。计算公式如下：</p><p><span class="math display">\[R(p,q) = I_{out}(p,q) -I_{in}(p,q)\]</span> <span class="math display">\[L_{TV} =\sum\limits_{p,q} (\Vert R(p,q+1) - R(p,q) \Vert_2 + \Vert R(p+1,q) -R(p,q)\Vert_2)\]</span></p><p>从上式能看出R其实计算的是源图和重建图像之间的差分。</p><p>融合规则：直接将两个原始图像经过编码器得到的F1和F2平均即可。</p><p>三类图像重建任务：用于在训练编码器-解码器阶段，对原始输入图像进行变换，产生失真图像，该失真图像作为训练图像重建任务的输入图像。</p><p>1.首先是使用基于Gamma的变换方式来学习场景内容和亮度信息。过曝图像对于实际场景中较暗区域的细节信息保持的较好，欠曝图像则对实际场景中较亮区域的细节保持的较好，因此希望融合图像在能保持每个区域丰富细节的同时也能让图像的亮度较为统一。所以这里用Gamma变换对源图像的多个子区域进行亮度改变，使编码器能应对不同亮度条件下保持细节和结构信息的能力。</p><p>2.使用基于傅里叶变换的方式在频域学习细节和纹理信息。对一张图像进行离散傅里叶变换DFT后得到的频谱图中，振幅确定图像的强度信息，相位主要确定图像的高级语义，并包含有关图像内容和物体对象位置的信息。欠曝和过曝图像的直方图都不是很居中，所以希望网络能学习到合适的曝光度。而相位则较好地保持了物体的形状和内容信息，所以也需要网络能够捕获形状和内容信息。这一步具体做法是先把源图转换到频域，然后对子区域在频域上进行失真处理，针对振幅采用高斯模糊，针对相位对所有相位值执行np次随机交换，np是在1到5之间的随机正整数。</p><p>3.使用全局shuffle学习结构和语义信息。该方法来源于Kang等人的一篇论文内容，具体做法是：对于待失真处理的每个子区域，在图像中随机选一块和该子区域尺寸相同的子区域，然后将这两个区域调换位置，重复10次就完成了一张源图的全局shuffle失真变换处理。</p><p>彩色图像融合：首先将图像转换为YCbCr通道，Y通道的融合方式和上述方式保持一致，而Cb和Cr的融合使用传统的加权求和方式。</p><h2 id="补充-5.vif-net">补充 5.VIF-Net</h2><p>An Unsupervised Framework for Infrared and Visible Image Fusion(IEEE, 2020)</p><p>图像融合是信息融合的一种，本质就是增强技术，运用多传感器获得的不同数据来提高网络性能。相对于单传感器的数据局限于一种的特性，多传感器能同时利用多种数据的特性，在视频监控、卫星成像、军事上都有很好的发展前景。对于本文来说，可见图像提供了丰富的纹理细节和环境信息，而红外图像则受益于夜间可见性和对高动态区域的抑制。</p><p>图像融合最关键的技术是怎么样能融合（利用）多种数据的优势。往往引入的多种数据是双面性的，所以要抑制不同类型数据带来的干扰。</p><figure><img src="/img/stage2/5_0.png" alt="图像融合基本框架" /><figcaption aria-hidden="true">图像融合基本框架</figcaption></figure><p>上图展示了图像融合的基本操作，将可见光和红外图像同时输入网络中，依次进行特征提取、特征融合、特征重建，最终生成融合图像。中间网络的部分就是作者提出的VIF-Net，全称为Visibleand Infrared image Fusion Network，就是可见光和红外图像融合网络。</p><p>本文提出自适应端到端的无监督深度学习模型VIF-Net，比较新颖的地方是损失函数中使用像素强度来衡量每个源图像局部区域红外温度显著性，然后根据红外信息的显著性程度，决定保留相应模态下源图像局部区域的特征信息，也就是指导网络让融合图像的相应位置的局部区域特征更接近红外温度信息更显著的那张源图像。</p><p>模型结构如下：</p><p><imgsrc="https://pic4.zhimg.com/v2-d9d1296109a12552fc53e38a159e082b_r.jpg" /></p><p>红外图像<span class="math inline">\(I_B\)</span>和可见光图像<spanclass="math inline">\(I_A\)</span>各占一个分支进行特征提取，并且两个支路的卷积层参数是共享的（权重相同以提取相同类型的模式或者说深度特征，同时又可以降低计算复杂度）。C11和C12提取底层特征，具体的卷积层设置情况如下表所示。</p><p><imgsrc="https://pic3.zhimg.com/v2-db54cd9a06573ee90f5d22cac371ffae_r.jpg" /></p><p>得到提取的特征图后，融合方法是直接将两个模态下的特征图在通道维度上拼接，然后将拼接好的特征图继续用卷积层C2~C6进行处理（重建），输出最终的融合图像。假设输入都是单通道的图像，经过前面的特征提取层，由于密集连接的作用，每一层的输出都会与后面所有层的输出直接相连（这里是通道叠加）。这样，可见光通道会输出16+16+16+16=64通道的特征图，两个网络通路会输出128通道的特征图，因此C2特征重建的输入是128个通道。</p><p>损失函数使用了Modified Structural Similarity（MSSIM）和TotalVariation（TV-Norm），用这两项对网络进行无监督训练：</p><p><span class="math display">\[Loss = \lambda L_{SSIM} +L_{TV}\]</span></p><p>SSIM是一种衡量图像结构相似性的算法，结合了图像的亮度、对比度和结构三方面对图像质量进行测量。原本的SSIM公式为：</p><p><span class="math display">\[SSIM(X,Y) = \dfrac{(2\mu _X\mu _Y +C_1)(2\sigma _{XY} + C_2)}{(\mu _X^2 + \mu _Y^2 + C_1)(\sigma _X^2 +\sigma _Y^2 + C_2)}\]</span></p><p>本文模型想要设计一种专门处理红外与可见光融合的结构项损失，ImageNet中曾经指出：亮度在较低分辨率下，不能测量全局亮度一致性，因此局部patch中的亮度信息就不是很重要了，所以本文将SSIM中计算亮度的相似度项去掉了，新的SSIM损失项计算如下：</p><p><span class="math display">\[SSIM_M(X,Y|W) = \dfrac{(2\sigma _{XY} +C)}{(\sigma _X^2 + \sigma _Y^2 + C)}\]</span></p><p>这样就能分别计算融合图像与红外图像、可见光图像的结构相似度了。W代表一个滑动窗口，实际计算时本文通过11×11的滑窗遍历整幅图像就能得到计算结果了。</p><p>对于红外图像的温度显著性，温度越高的目标其像素值也会越大，所以一般用像素强度E(I|W)来衡量红外图像的温度信息。所以作者就产生了这样的想法：用每个滑窗内E(I|W)的大小来指导SSIM的计算。具体的做法是：对于两张源图像A和B，如果A一个滑窗内的像素强度值E(I|W)大于B中该位置对应滑窗内的像素强度值E(I|W)，那么就会认为A在该局部区域内的红外温度信息比B的显著，所以就希望网络多保留一些A的特征信息，也让融合图像的该局部区域和A的该区域更相似。最后提出了下面这种计算损失函数的方式（式中的E(I|W)实际上是计算的是该滑窗内所有像素点值的和，而非MSE或者L2范数这种方式）。</p><p><img src="/img/stage2/5_1.png" /></p><p>除了SSIM项，损失函数还有一个总变分项来保留源图像的边缘信息，去除噪声。TV全称是TotalVariation，译为总体变化，是一种衡量图片噪声的指标。</p><p>理解TV的原理可以参考“专业笔记”栏目中距离度量-范数篇附的链接，传统的TV计算公式为：</p><p><span class="math display">\[R_{V^{\beta}(x)} = \sum\limits_{i,j}\Big((x_{i,j+1} - x_{i,j})^2 + (x_{i+1,j} - x_{i,j})^2\Big)^{\frac{\beta}{2}}\]</span></p><p>其中，<spanclass="math inline">\(x_{i,j}\)</span>代表一个像素，将其与水平方向+1的像素做差的平方，并且与垂直方向+1的像素做差的平方，两者之和开<spanclass="math inline">\(\frac{\beta}{2}\)</span>的次方，如此遍历除最后一行和最后一列之外的所有像素点，就可以计算出TV。如果有噪声，噪声点与其他像素之间的变化会很大，TV会明显变大。如果TV很小，说明相近的像素差值较小甚至为0，图像会比较模糊。</p><p>作者运用下列公式计算<span class="math inline">\(L_{TV}\)</span>：</p><p><span class="math display">\[R(i,j) = I_{A}(i,j) -I_{F}(i,j)\]</span> <span class="math display">\[L_{TV} =\sum\limits_{i,j} (\Vert R(i,j+1) - R(i,j) \Vert_2 + \Vert R(i+1,j) -R(i,j)\Vert_2)\]</span></p><p>上式相当于对可见光图像与融合图像做差分，<spanclass="math inline">\(\beta\)</span>取2。</p><p>总的损失函数是：<span class="math inline">\(Loss = \lambda L_{SSIM} +L_{TV}\)</span>，而这两项的实际差值较大，在100至1000之间。如果SSIM项的值较低，会造成融合图像对比度不高，质量低的结果。TV项的结果如果较低，融合结果会丢失更多可见光图像的细节。所以这里引入<spanclass="math inline">\(\lambda\)</span>来平衡两项。</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理 · 脉络篇（一）</title>
    <link href="/2023/12/20/papers2/"/>
    <url>/2023/12/20/papers2/</url>
    
    <content type="html"><![CDATA[<p>声明：本文是根据VIPLab的一位博学多才的博士师兄的知乎专栏内容进行归纳总结的，地址如下： <ahref="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">部分基于深度学习的红外与可见光图像融合模型总结</a></p><hr /><h2 id="引言">引言</h2><p>传统方法用于图像融合的一些问题：1.没有完全考虑不同模态图像的特性，使用同种方法对不同模态图像进行特征提取可能不会提取到最有效的信息；2.融合规则是人工设计的简单规则，例如最大值法、平均值法，对于融合结果有限制。</p><p>深度学习方法做融合的优势：1.不同的网络分支可以对不同模态的图像进行特征提取，甚至对同种模态使用不同分支提取多种信息；2.融合策略可以通过学习得到，可不经过人工设计的融合规则获取融合结果。</p><p>近两年做红外-可见光图像融合的深度学习模型也是以下面的模态特点为基础的：&gt;红外图像独有的信息为像素值幅度信息，能够体现温度显著性，温度越高的目标像素值越大；可见光图像独有的信息为纹理细节信息，多数红外图像中模糊的区域在可见光图像中则具有良好的细节，所以一些自监督模型在设计损失函数时针对红外图像的损失项使用的是MSE/L2-norm，保持温度显著性信息，而针对可见光图像使用的是SSIM或者梯度算子，保持细节纹理信息。自监督方法需要设计出合理的指标作为损失函数。</p><p>根据时间来分类，2019年之前基本都是有监督模型或者直接套用预训练模型，2020年开始有自监督和GAN模型出现，整体还是修改网络结构和损失函数，个别模型思考了不同模态信息的保留方式，亦有加入了self-attention机制的融合模型。到了2021年，涌现了很多的深度学习模型，也相应诞生了很多新的处理思路。很多学者开始思考到底在红外与可见光图像融合中该如何定义互补信息、如何计算每种模态下图像具有的互补信息，以及如何有效地融合这些信息？展现在模型上，就有了基于显著性的模型、基于transformer的模型等，不再是简单地修改损失函数和网络结构，不再简单地将红外图像的特有信息定义为像素幅度、可见光图像的特有信息定义为纹理细节。同时，还出现了多任务融合模型，例如将超分和融合一起做的端对端模型CF-Net。任务驱动的融合方法也是一种趋势，如SeAFusion。</p><p>总体来说，红外与可见光图像融合包含有监督模型、无监督/自监督模型、GAN、显著性模型、transformer/self-attention、多任务驱动、不同分辨率下的融合模型、引导滤波辅助的模型等。下面根据年份对其中一部分模型做简单的介绍。</p><h2 id="densefuse">1.DenseFuse</h2><p>A Fusion Approach to Infrared and Visible Images (Transactions onImage Processing, 2018)</p><p>DenseFuse一般认为是第一个使用深度学习模型对红外与可见光图像进行融合的方法，也是一种有监督学习模型。该模型将红外与可见光图像分解为basepart和detail part两部分，其中base part直接用平均法进行融合，detailpart通过一个预训练的VGG网络先进行特征提取，然后使用multi-layers fusionstrategy基于已有特征图得到融合权重图，将权重图和特征图相乘，再和另一模态的结果相加视为融合特征图的结果。</p><p><imgsrc="https://pic1.zhimg.com/v2-1c849a57e96fa6aa833257b9857a9ae4_r.jpg" /></p><p>实现：经过L1-norm和average获得activity levelmap，然后继续基于该activity level map，使用softmax计算final activitylevel map，该结果和两个模态的detail part分别相乘后再相加就是detailpart的融合结果，这一思想与Infrared and Visible Image Fusion using a DeepLearning Framework(International Conference on Pattern Recognition,2018)中的融合策略一致。Base part和detailpart都完成融合后，将这两个结果相加得到最终的融合结果。</p><p>图像分解与重建使用的是预训练过的自编码器，而此前的论文中对图像分解还多用的是传统的变换域分解方法。可以说DenseFuse不再使用传统图像融合的图像分解方法，取而代之的是利用了卷积层最擅长的特征提取能力，一个优势是提取到的特征对不同场景图像的适应性要好。</p><p>模型的训练是以可见光图像作为输入和输出训练得到一个自编码器，编码器中加入了denseconnection，有利于层之间信息的传播。使用的损失函数为MSE和SSIM。训练完成后直接在编码器和解码器之间加入以上提及的特征图融合方法即可。</p><p><imgsrc="https://pic3.zhimg.com/v2-37f40ceff40885e3435b3fd69f6d2cb2_r.jpg" /></p><p>DenseFuse整体思路和2020年很多模型的思路差不多，其实都是用卷积神经网络作为特征提取与重建模块，不过由于DenseFuse在训练自编码器时使用的只有可见光图像，对红外图像的特征提取能力有一定限制，因此针对融合所需要的互补信息的提取能力可能也存在一定的限制。不过相对于传统方法提升较好，尤其是不同场景下融合结果中的伪影较少。</p><h2 id="fusiongan">2.FusionGAN</h2><p>A generative adversarial network for infrared and visible imagefusion (Information Fusion, 2019)</p><p>这是首个使用GAN来融合红外与可见光图像的模型，通过生成器和判别器之间的对抗学习避免人工设计activitylevel和融合规则。其中生成器同时将红外图像与可见光图像作为输入，输出融合图像；判别器将融合图像与可见光图像作为输入，得到一个分类结果，用于区分融合图像与可见光图像。在生成器和判别器的对抗学习过程中，融合图像中保留的可见光信息将逐渐增多。训练完成后，只保留生成器进行图像融合即可。由于可见光图像的纹理细节不能全部都用梯度表示，所以需要用判别器单独调整融合图像中的可见光信息。</p><p>实际训练中的生成器和判别器的平衡不好把握，FusionGAN的融合结果对比度不是很好，红外目标的显著性保留的不是很好。</p><h2 id="nestfuse">3.NestFuse</h2><p>An Infrared and Visible Image Fusion Architecture Based on NestConnection and Spatial/Channel Attention Models (IEEE Transactions onInstrumentation and Measurement, 2020)</p><p>NestFuse可以说是DenseFuse的升级版本，仍然是有监督模型，在第一阶段训练自编码器时用多张可见光图像作为输入输出来训练网络。编码器的结构与DenseFuse没有太大差异，不同之处就是多个卷积层输出的特征图都会进行融合，因此NestFuse是一种多尺度融合模型，对下采样带来的细节损失有一定保护效果。融合模块使用spitalattention和channel attention生成显著图来进行融合，解码器加入了nestconnection，保护编码器提取到的多尺度特征。</p><p><imgsrc="https://pic4.zhimg.com/v2-f85aa4bd3c89c975845e44469f91fe03_r.jpg" /></p><p>训练完成后的融合模块则不需要训练，仍然是传统方法，采用了两个通道的attention：Spitalattention和channelattention分别在同一图像内对不同像素生成显著性和不同特征图之间的通道维度生成显著性。经过spitalattention和channelattention后可以得到两个特征图，直接进行平均法得到融合特征图。</p><p>NestFuse虽然在解码器阶段使用了nestconnection，不过和之前的有监督模型一样，针对模态之间的互补信息提取的较少，并且融合规则仍然是人工设计的方法，不能针对红外与可见光特有的信息进行融合。</p><h2 id="didfuse">4.DIDFuse</h2><p>Deep Image Decomposition for Infrared and Visible Image Fusion(International Joint Conferences on Artificial Intelligence, 2020)</p><p>最早使用深度学习方法的模型，基本都是先用传统方法将输入图像分解为basepart和detailpart，相当于图像中的低频部分和高频部分。DIDFuse则是将深度图像分解方法引入红外与可见光图像融合模型中，在训练阶段通过编码器将图像分解为背景部分和细节部分，测试阶段将红外与可见光图像的背景部分和细节部分分别进行融合后，再送入解码器进行图像重建，得到融合结果。</p><p><imgsrc="https://pic1.zhimg.com/v2-012dfb0a77a6c984a1e948e132475ba0_r.jpg" /></p><p>为了避免细节信息损失，在网络中加入shortconnection，分别将编码器的前两个卷积层输出特征图与解码器后两个卷积层输出的特征图进行直接拼接。编码器输出的背景特征图和细节特征图需要通过训练得到，因此设计了相应的损失函数。损失函数各项包括编码器损失L1（Figure 1(a) <spanclass="math inline">\(L_{total}\)</span>中的前两项）和解码器损失L2（<spanclass="math inline">\(L_{total}\)</span>的后三项）。L1用于图像分解，希望两种模态背景部分相差较小、细节部分相差较大，因此第一项和第二项分别为正、负。经过tanh函数，可以将二者的差距限定在-1到1之间。L2用于图像重建，第三、四项目的是保持源图像和重建图像的像素幅度信息和细节纹理信息，很多2020年的自监督模型都是通过这种损失函数来训练网络的，用的是非常普遍的L2-norm和结构相似度SSIM，分别计算红外源图像与重建图像之间的损失、可见光源图像与重建图像之间的损失。具体形式如下：</p><p><img src="/img/stage2/2_3.png" /></p><p>损失函数最后一项使用梯度算子来保留可见光图像的细节信息。自监督模型在计算纹理细节损失中，使用SSIM和梯度算子很普遍。</p><h2 id="ddcgan">5.DDcGAN</h2><p>A Dual-Discriminator Conditional Generative Adversarial Network forMulti-Resolution Image Fusion (Transactions on Image Processing,2020)</p><p>是2020年的GAN模型之一，融合效果在细节和对比度方面要优于FusionGAN，并且是针对红外图像分辨率低于可见光图像分辨率的情况。模型首先通过训练过的卷积层对分辨率低的红外图像进行上采样到可见光图像的分辨率（也可以将融合与超分一起做，具体的模型看CF-Net），然后生成器对这两张图像进行融合，两个判别器分别针对红外与可见光图像进行判断。损失函数方面和先前模型类似，都是基于保留红外图像像素幅度、可见光图像细节信息的基础上设计的。</p><p><imgsrc="https://pic3.zhimg.com/v2-d38428c0ec444dd44f06c5dbefd48b96_r.jpg" /></p><p>生成器的结构如下图，首先通过反卷积层将红外图像上采样到和可见光图像一样的尺寸（红外图像训练集手动下采样到原图的1/4大小），两张图像在通道维度拼接后送入5个加入denseconnection的编码层，再经过5个常规卷积层就能得到融合图像了。</p><p><imgsrc="https://pic3.zhimg.com/v2-00228a8f61da0bd5fbb9b21a4e33b726_r.jpg" /></p><p>判别器的结构如下图，红外图像和可见光图像各一个判别器。红外图像对应的判别器是从融合图像或红外原输入图像中随机选出一张作为输入，可见光图像对应的判别器是从融合图像或可见光原输入图像中随机选出一张作为输入，它们的输出都是一个代表概率的标量，表示输入图像是真实图像（原图）的概率。</p><p><imgsrc="https://pic2.zhimg.com/v2-e3e41342d77fe025435deb850101f55d_r.jpg" /></p><p>编码器的损失函数是由adversarial loss和content loss组成，其中Contentloss中红外图像的像素幅度计算通过Frobeniusnorm得到，而可见光图像的细节信息通过TV-norm得到。判别器的损失函数用于区分源（原）图像和融合图像，判别器的adversarialloss可以用来计算不同分布之间的Jensen-Shannon差异，因此可以用来判断像素强度和纹理细节分布的真实性并促使融合图像的分布更贴近真实分布。</p><h2 id="fast-uif">6.Fast-UIF</h2><p>Rethinking the Image Fusion: A Fast Unified Image Fusion Networkbased on Proportional Maintenance of Gradient and Intensity (AAAI,2020)</p><p>这是用于通用融合任务的模型，进行不同融合任务时需要调整损失函数中各项的权重。网络整体分为gradientpath和intensitypath两个分支，输入则是不同模态的混合输入，而非先前模型采用的gradient输入可见光图像（细节信息多的模态），intensity输入红外图像（像素幅度信息多的模态），在进行最终的融合前还有预融合模块。</p><p><imgsrc="https://pic1.zhimg.com/v2-bf617fd5154be3c8d0104ed4bf0ef9dc_r.jpg" /></p><p>Gradient path的输入由两张可见光图像和一张红外图像组成，intensitypath的输入由两张红外图像和一张可见光图像组成，也可以根据场景调整两个模态图像的比例。两个分支都是由卷积层组成，不包含下采样步骤，并且各自包含两次通道维度拼接过程和两个预融合模块（pathwisetransferblock），最后每个卷积块的输出都会全部进行一次通道维度的拼接，经过单层卷积后是最终的融合图像。损失函数的设计以像素幅度项与梯度项为主，不同的融合任务调整四个项的权重即可，比如红外与可见光的融合中红外图像包含的幅度信息多，所以计算像素幅度的项：红外对应的那一项权重应该大于可见光的；同理，计算梯度的项：可见光对应的权重应该大于红外的。</p><h2 id="fusiondn-u2fusion">7.FusionDN &amp; U2Fusion</h2><p>FusionDN和U2Fusion的融合思想是一样的，FusionDN作者先发表在AAAI(2020)上，后来作者将改进后的U2Fusion发表在了TPAMI上。</p><p>两篇文章都是对融合过程中每种模态图像信息量的测量与保留方法进行了研究，FusionDN通过已有的图像质量评价模型NR-IQA和熵Entropy的计算来测量每个模态图像的信息量得到权重，而U2Fusion通过FeatureExtraction、Information Measurement、Information PreservationDegree三个模块根据源图像直接得到其相应的信息量权重，作用就相当于FusionDN的NR-IQA和Entropy，其中FeatureExtraction使用的是预训练的VGG-16，在大量数据上预训练过的深度网络可以提取出图像中各类特征。信息权重作用于损失函数。另外，U2Fusion在损失函数中加了EWC(ElasticWeight Consolidation, 可塑权重巩固, <ahref="https://zhuanlan.zhihu.com/p/86365066">点击查看详细解读</a>)这一项，避免网络串行训练数据时遗忘先前的任务内容。融合网络比较简单，将两个输入在通道维度拼接后送入卷积层就OK了。</p><p>详情可以点击以下链接查看：</p><p><a href="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">红外与可见光图像融合模型总结2.6</a></p><p><a href="https://zhuanlan.zhihu.com/p/397425256"title="这也是博士师兄写的文章哟~">无监督图像融合模型U2Fusion/FusionDN</a></p><h2 id="fusiongan-1">8.FusionGAN++</h2><p>Infrared and visible image fusion via detail preserving adversariallearning (Information Fusion, 2020)</p><p>本文的模型用于改善先前GAN模型带来的细节损失问题，并且加入了针对边缘的保护机制。模型的生成器产生融合图像，然后将融合结果与可见光源图像一起送入判别器，判断融合结果是否来自于可见光图像。当判别器不能区分融合结果与可见光图像时，此时认为融合结果包含了充足的细节信息，通过这种方式就能自动进行细节信息的表示和选择，不再通过人工设计融合规则。边缘保护体现在损失函数中。</p><h2 id="rfn-nest">9.RFN-Nest</h2><p>An end-to-end residual fusion network for infrared and visible images(Information Fusion, 2021)</p><p>RFN-Nest在2020年NestFuse的基础上，将融合模块从人工设计的融合规则变成了用网络进行融合，同时不再是单独用自然图像进行有监督训练，也和很多其他模型类似，加入了自监督学习方法，其中第一阶段训练和NestFuse一样，用大量自然图像训练一个自编码器，而第二阶段训练是RFN-Nest新增加的自监督训练方式，会生成四个用于多尺度融合的RFN(Residualfusionnetwork)模块。相较于其他自监督模型的不同之处是多尺度（很多融合模型为了避免细节损失不会使用任何下采样步骤）和大规模红外-可见光数据集参与了训练。</p><figure><imgsrc="https://pic4.zhimg.com/v2-5ff110ca353056e0f43c189c026d94b3_r.jpg"alt="Framework of proposed RFN-Nest" /><figcaption aria-hidden="true">Framework of proposedRFN-Nest</figcaption></figure><p>首先来说第一阶段训练，编码器和解码器与先前的NestFuse相同，并且第一阶段训练也仍然是用自然图像训练自编码器，采用的是COCO数据集的八万张图像。</p><p>第二阶段是训练四个RFN模块，其实也就是学习融合策略，需要先将编码器和解码器的网络参数都固定（相当于RFN的输入输出固定），然后用自监督方法专门训练该模块的网络。RFN结构如下：</p><figure><imgsrc="https://pic1.zhimg.com/v2-73b9e582ce5b2f0414a8e678879d9dbc_r.jpg"alt="the structure of RFN" /><figcaption aria-hidden="true">the structure of RFN</figcaption></figure><p>两个模态的特征图会分成两个分支，一个分支各自经过单个卷积层后在通道维度拼接，再用后续三个卷积层处理，另一个分支则经过单个卷积层后作为残差连接和另个分支的组合，得到的就是融合特征图。四个RFN模块均为这种结构，在四个尺度上对特征图进行融合。这阶段训练使用的数据集为KAIST数据集，包含八万个红外-可见光图像对。</p><h2 id="cf-net">10.CF-Net</h2><p>Deep Coupled Feedback Network for Exposure Fusion and ImageSuper-Resolution (Transactions on Image Processing, 2021)</p><p>本文提出的CF-Net是将超分和多曝光融合用一个网络做的端对端模型，这种多任务图像融合模型也是综述Imagefusion meets deep learning: A survey andperspective中提出的图像融合趋势。超分辨有利于目标检测的准确度，因此如果把多曝光任务和超分辨率任务一起做，应该也可以提高high-level的任务效果，因此CF-Net也可以认为是任务驱动的融合模型，即未来的发展趋势之一。</p><p>查看详情：<a href="https://zhuanlan.zhihu.com/p/467039982"title="温柔的博士师兄写的文章~">红外与可见光图像融合模型总结3.2</a></p><h2 id="stdfusionnet">11.STDFusionNet</h2><p>An Infrared and Visible Image Fusion Network Based on Salient TargetDetection (Transactions on Instrumentation and Measurement, 2021)</p><p>STDFusionNet是基于显著性目标检测的融合方法，可以保护红外显著性目标的纹理信息和温度信息。首先用mask对红外显著性目标进行标注，然后结合mask设计损失函数来进行提取特征和重建。mask只在训练阶段使用，也就是说本文模型可以隐式完成显著目标检测和关键信息融合。应该说STDFusionNet也考虑了红外与可见光融合的互补信息，只不过人为地将其定义为红外图像的显著目标和可见光图像中的背景纹理的组合。</p><p>网络结构如下图，可以看到网络的输入是完整的图像，只不过在计算损失函数时会用mask对红外图像中的显著性目标单独提出来，将可见光图像的背景区域单独提出来计算相应的损失。两个模态的特征图也是在通道维度拼接后送入图像重建网络中的。</p><p><imgsrc="https://pic2.zhimg.com/v2-f24e7babafa72572dbf86554da7681e5_r.jpg" /></p><p>损失函数：</p><p><img src="/img/stage2/3_4.png" /></p><h2 id="csfsaliency-based">12.CSF(Saliency-based)</h2><p>Classification Saliency-Based Rule for Visible and Infrared ImageFusion (Transactions on Computational Imaging, 2021)</p><p>本文也提出了一种基于显著性分类的融合方法classification saliency-basedfusion method(CSF)，不过和上一个STDFusionNet在图像内选择显著性区域不同，本文的模型是根据多个卷积层输出特征图的可解释性重要性来评估的，这种面向重要性的融合规则有助于保留有价值的特征图，得到的显著性信息就直接作为融合的加权系数。首先使用分类器对两种类型的源图像进行分类，可以测量源图像之的差异信息和独有信息。然后根据每个像素对分类结果的贡献程度计算它的重要性，该重要性将以分类显著图的形式呈现，根据这个分类显著图来融合各个特征图，这种通过预训练过的分类器来自动保留重要特征也是一个新颖的角度。</p><p>整体结构如下图，和其他深度学习模型一样，先提取特征图进行融合，然后对融合特征图进行重建。特征提取和重建都是普通的卷积层，重点就在于融合特征图这部分的设计，也是本文的核心。</p><p><imgsrc="https://pic1.zhimg.com/v2-c96de3a4ab9709d5d838a0bbfff9d628_r.jpg" /></p><p>分类显著性估计（Classification SaliencyEvaluation）：红外与可见光图像融合的一大策略就是将每个模态下图像的重要信息和互补信息提取出来进行融合，因此该融合的关键点就在于特征图每个区域的重要性评估，重要的区域需要被保留，冗余区域需要被压缩。</p><p>一种直观的评价方式是将部分可见光特征图替换为红外特征图，然后观察替换后结果的变化。如果这部分是多余的，在替换之后，重建的图像仍然会看起来像原始的可见光图像。如果不是多余的（这部分包含红外图像中的唯一/关键信息），替换后重建的图像将与红外图像相似。反之亦然。为了量化图像风格，使用一个二分类器测量图像属于任何一种风格的概率。此外，分类器能找到每种风格最明显的特性和不同风格之间最明显的差异，可以比较不同类型的信息并帮助识别重要且值得保留的信息，这种功能对融合很有用处，所以使用分类器来帮助定量设计融合规则。</p><p>上述思想的具体实现方法：以图像对为例进行通道替换。当24张输入的特征图全部都是红外特征图时，该特征图组被判断风格为红外的概率接近1，而将这组中的其中一张特征图替换为可见光图像特征图，再看风格被判断为红外的概率是多少。用这种方法，按照顺序依次将24张特征图的某一张替换为可见光特征图，概率值的变化就可以统计出来了。</p><p><imgsrc="https://pic4.zhimg.com/v2-209f505af521d3b3abeb754e1199295b_r.jpg" /></p><p>上图中有一些位置特征图被替换后，风格被判断为红外的概率大大降低，有的则基本没有下降，可以说那些被替换后概率没有下降太多的特征图，对于分类的贡献就不是很大，也就是说这个位置的特征图对分类没什么作用，即红外和可见光在这个通道位置上对应的特征比较一致，没有较大的区分度。相反地，一些位置的特征图被替换后，风格被判断为红外的概率大大降低，这些位置可以说对分类的贡献较大，红外和可见光在这些位置上的特征差异较大。根据特征图对分类结果的影响，可以评估重要性以反映这些特征图是否包含重要信息，将其称为分类显着性（classificationsaliency），以此作为融合时信息量保留的基础。</p><p>上面所说的是通道维度上的显著性，即channel-wise。在每个特征图上的每个像素也存在显著性差异，即pixel-wise，因此需要综合看待每个特征图上的单个像素对分类器判断为红外或可见光图像中概率较大的那个值的变化，从而计算出每个像素位置上的显著性值，即分类显著性图。</p><p>有了这两个权重图，下一步和先前NestFuse模型一样，每个模态的特征图和权重图相乘再相加即可。</p><p>另外，文中有一段阐述了现有人工设计融合方式的缺陷，写的很好：</p><blockquote><p>The reason why existing fusion rules are rough for fusing features isas follows. Because of the unexplainability and incomprehensibility ofCNNs, the specific characteristics represented in feature maps areunknowable. For instance, some convolution kernels extract brightregions, some extract dark regions while some may extract lines. If thisis the case, the max rule can well preserve the bright regions while theinformation with low brightness will suffer from distortion. Because ofthe unknown and variability, it is difficult to measure the importanceof different regions of feature maps. Thus, it is groundless to design afusion rule by assigning pixel-wise weight maps, which take thepixel-wise importance of feature maps into account. In this case, thelimited choices of fusion rules and their roughness restrict theimprovement of fusion results. Even a well-designed feature extractionway may fail to achieve its optimal performance because of therestriction of the fusion rule.</p></blockquote><h2 id="ganmcc">13.GANMcC</h2><p>A Generative Adversarial Network With Multiclassification Constraintsfor Infrared and Visible Image Fusion (Transactions on Instrumentationand Measurement, 2021)</p><p>以往提起红外与可见光图像，一般都认为红外图像细节差，但是目标显著性好、对比度高，要用像素幅度约束；可见光图像细节好，要用梯度或SSIM约束。GANMcC提出的思想是，红外图的细节不一定就比可见光差，可见光也有可能对比度优于红外图像，而在某些场景下确实如此。</p><p>此外要确保融合图像既有显著的对比度又有丰富的纹理细节，关键是保证源图像的对比度和梯度信息是平衡的，本质上是同时估计两个不同域的分布，GAN可以在无监督情况下更好地估计目标的概率分布，而多分类GAN可以进一步同时拟合多个分布特征，解决这种不平衡的信息融合。</p><p>具体做法是：用一个多分类器作为判别器，可以确定输入是红外图像和可见图像的概率。对于融合图像，在多分类约束下，生成器期望这两个概率都很高，即判别器认为它既是红外图像又是可见光图像；而判别器期望这两个概率同时很小，即判别器判断融合图像既不是红外图像也不是可见光图像。在此过程中，同时约束这两个概率，以确保融合图像在两个类别中的true/false程度相近/相同。经过不断的对抗学习，生成器可以同时拟合红外图像和可见光图像的概率分布，从而产生对比度显著和纹理细节丰富的结果。通过这两种设计的配合，可以生成具有良好视觉效果的融合图像。模型结构如下图所示。</p><p><imgsrc="https://pic2.zhimg.com/v2-33585597ce11890b9fafe430f2d1dba5_r.jpg" /></p><p>输入与上述第6个网络Fast Unified Image FusionNetwork一样，也是两个模态的混合输入，由生成器得到融合图像，判别器输出该融合图像属于红外/可见光的两个概率值，多轮对抗训练后，当两个概率值都比较大的时候，认为互补融合信息达到了平衡。</p><p>生成器的结构如下，其中在拼接特征图时是将两个分支特征图交叉拼接的，目的是为了更好融合信息。</p><p><imgsrc="https://pic3.zhimg.com/v2-d57089ccf6d96ab529bc4849200285d2_r.jpg" /></p><p>因为使用了GAN网络，故损失函数包含content loss和adversarialloss两项，而本文模型的思想是红外图像也有细节信息，可见光图像也有较好的对比度，因此contentloss的设计考虑了辅助信息，也就是红外图像的纹理细节与可见光图像的对比度信息。通过调整每一项损失的权重大小来决定每种信息的保留程度，首先主要信息的权重应该大于辅助信息的权重，并且梯度信息的权重一般要小于像素强度信息的权重。按照这个规则进行权值的设置。</p><p>判别器的结构如下，判别器也是一个多分类器，对输入图像（红外/可见光/融合图像任选一个输入）进行分类得到输入图像的类别，输出是一个包含两个概率值的向量。</p><p><imgsrc="https://pic2.zhimg.com/v2-238205700b307b4261bf2c6f8e5d4489_r.jpg" /></p><p>损失函数必须促使判别器不断提高其判别能力，才能有效地识别出什么是红外图像或可见光图像。损失函数由三项组成，分别为可见光图像、红外图像、融合图像的decisionloss。具体的损失函数设计就不摆在这里了，可以查看作者论文原文或者<ahref="https://zhuanlan.zhihu.com/p/467039982">专栏第3.7中的内容</a>。</p><h2 id="rxdnfuse">14.RXDNFuse</h2><p>A aggregated residual dense network for infrared and visible imagefusion (Information Fusion, 2021)</p><p>RXDNFuse是在融合网络中加入残差连接的模型，对损失函数也有一定的改进，分为像素级损失和特征级损失，像素级损失还是以像素幅度和结构相似度为基础计算的，特征级损失则是以VGG-19提取的深度特征图作为基础。</p><p><imgsrc="https://pic1.zhimg.com/v2-b47d8e450daf3e4754366633b1e034b0_r.jpg" /></p><p>这里知识简单说明文章的中心思想，网络子结构主要是添加了残差连接和密集连接，idea在损失函数方面要更多一点。</p><p>像素级损失的结构损失和像素幅度损失计算如下图，融合图像与红外、可见光源图都会进行像素幅度和SSIM的计算。</p><p><imgsrc="https://pic3.zhimg.com/v2-c401c565fa3a73ce26e5d29951f28faa_r.jpg" /></p><p>特征级损失其实就是perceptualloss，同样也需要分别计算结构损失和像素幅度损失。而使用深度网络特征图来计算损失的原因作者也引用了其他文章中的结论：通过深度神经网络对图像特征进行逐层处理，输入图像会转换为对图像实际内容越来越敏感的抽象表示，如果直接用较浅层的特征图进行重建，其实只是简单地将原始图像的每个像素进行了重现，与之相反的是深层特征可以捕获抽象语义特征。因此这里将每个输入图像复制为三个通道后送入VGG-19模型来提取高级（深层次的）语义特征，如下图所示。</p><p><imgsrc="https://pic3.zhimg.com/v2-2300d01da41d38aebef93d40b4b99dca_r.jpg" /></p><p>感知损失是论文Perceptual Losses for Real-Time Style Transfer andSuper-Resolution中提出的概念，用于实时超分辨任务和风格迁移任务，后来也被应用于更多的领域，如图像去雾。下面是论文的解读和损失函数说明的链接。</p><p><a href="https://zhuanlan.zhihu.com/p/24720434">论文解读</a></p><p><ahref="https://blog.csdn.net/qq_43665602/article/details/127077484">感知损失Perceptualloss</a></p><hr /><p>这篇文章到这里就结束啦！</p><p>本文总结了《部分基于深度学习的红外与可见光图像融合模型总结》3.8及以前的内容，3.9之后的内容和别的文章将在本系列（二）中进行更新，内容涉及：部分传统方法、更新的Paper等，敬请期待~</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像融合</title>
    <link href="/2023/12/16/Review/"/>
    <url>/2023/12/16/Review/</url>
    
    <content type="html"><![CDATA[<h3 id="融合方法分析">融合方法分析</h3><p><a href="https://zhuanlan.zhihu.com/p/475470158?utm_id=0"title="图像融合的方法与分析">图像融合的方法与分析</a></p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>basic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献整理 · 开山篇</title>
    <link href="/2023/11/25/Papers/"/>
    <url>/2023/11/25/Papers/</url>
    
    <content type="html"><![CDATA[<h2 id="tardal">TarDAL</h2><h3 id="title">Title</h3><p>Target-aware Dual Adversarial Learning and a Multi-scenarioMulti-modality Benchmark to Fuse Infrared and Visible for ObjectDetection [CVPR] 2022</p><h3 id="motivation">Motivation</h3><p>Previous approaches:</p><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>.Aiming <span class="hljs-built_in">at</span> generating an image of high visual quality<span class="hljs-number">2</span>.<span class="hljs-keyword">Discover </span>commons underlying the two modalities <span class="hljs-keyword">and </span>fuseupon the common space <span class="hljs-keyword">either </span><span class="hljs-keyword">by </span>iterative optimization <span class="hljs-keyword">or </span>deep networks<span class="hljs-number">3</span>.Neglect that modality <span class="hljs-keyword">differences </span>implying the complementary infomationwhich <span class="hljs-keyword">extremely </span>important for <span class="hljs-keyword">both </span>fusion <span class="hljs-keyword">and </span><span class="hljs-keyword">subsequent </span>detection task</code></pre></div><h3 id="intro">Intro</h3><div class="code-wrapper"><pre><code class="hljs maxima">Visible <span class="hljs-built_in">image</span>, provides rich details with high spatial <span class="hljs-built_in">resolution</span>(under welldefined lighting conditions)</code></pre></div><div class="code-wrapper"><pre><code class="hljs sqf">Infrared <span class="hljs-built_in">image</span>, capturing ambient temperature variations emitted <span class="hljs-keyword">from</span> objects, highlight structures of thermal <span class="hljs-built_in">targets</span> (insensive <span class="hljs-keyword">to</span> lighting changes)</code></pre></div><p>Their evident appearance discrepancy, it's challenging to fusevisually appealing images and/or to support higher-level vision tasks(by making full use of the complementary info from the infrared andvisible images)</p><p>The fusion emphasizes more on "seeking commons" but neglect thedifferences of these two modalities on presenting structure info oftargets and textural details of ambient background.</p><h3 id="method">Method</h3><p>Fusion network:</p><div class="code-wrapper"><pre><code class="hljs mipsasm">Composed of one generator <span class="hljs-keyword">and </span>two target-aware <span class="hljs-keyword">discriminators, </span><span class="hljs-keyword">and </span>a commonly used detection network  [Detection-<span class="hljs-keyword">oriented </span>fusion]<span class="hljs-symbol"></span><span class="hljs-symbol">Discriminator:</span><span class="hljs-keyword">Distinguishes </span>foreground(structure infomation of targets), i.e., thermal targets(infrared image), <span class="hljs-keyword">and </span><span class="hljs-keyword">differentiates</span><span class="hljs-keyword"></span>the <span class="hljs-keyword">background, </span>i.e., textural details(visible image)Derive a cooperative training <span class="hljs-keyword">scheme</span></code></pre></div><h3 id="structure-diagram">Structure diagram</h3><figure><img src="/img/stage1/11.png"alt="Methodology framework: (a) bilevel optimization formulation for fusion and detection, (b) target-aware adversarial dual learning network for fusion, and (c) cooperative training scheme." /><figcaption aria-hidden="true">Methodology framework: (a) bileveloptimization formulation for fusion and detection, (b) target-awareadversarial dual learning network for fusion, and (c) cooperativetraining scheme.</figcaption></figure><figure><img src="/img/stage1/12.png" alt="The details of network" /><figcaption aria-hidden="true">The details of network</figcaption></figure><figure><img src="/img/stage1/13.png"alt="The architectures of generator and discriminator" /><figcaption aria-hidden="true">The architectures of generator anddiscriminator</figcaption></figure><h3 id="loss-function">Loss function</h3><p>Detection network backbone: YOLOv5</p><p>Generator: 1.Structural similarity index (SSIM)</p><blockquote><p>Contributes to generate a fused image that preserves overallstructures and maintains a similar intensity distribution as sourceimages.</p></blockquote><figure><img src="/img/stage1/14.png" alt="Generator loss SSIM" /><figcaption aria-hidden="true">Generator loss SSIM</figcaption></figure><p>2.Pixel loss (based on the saliency degree weight (SDW))</p><blockquote><p>To balance the pixel intensity distribution of source images</p></blockquote><p><img src="/img/stage1/15.png" alt="Pixel Loss" /> w1, w2 arecalculated by saliency value of x and y.</p><p>Target and detail discriminators: Wasserstein divergence (with targetmask m)</p><blockquote><p>The target discriminator <span class="math inline">\(D_T\)</span> isused to distinguish the foreground thermal targets of fused result tothe infrared while the detail discriminator <spanclass="math inline">\(D_D\)</span> contributes to distinguish thebackground details of fused result to the visible.</p></blockquote><figure><img src="/img/stage1/16.png" alt="Discriminator loss" /><figcaption aria-hidden="true">Discriminator loss</figcaption></figure><h2 id="seafusion">SeAFusion</h2><h3 id="title-1">Title</h3><p>Image fusion in the loop of high-level vision tasks: A semantic-awarereal-time infrared and visible image fusion network [Image Fusion]2022</p><h3 id="intro-1">Intro</h3><p>The infrared sensor captures thermal radiation emitted from objects,which could highlight salient targets, but the infrared image neglectstexture and is vulnerable to noise.</p><p>The visible sensor captures reflective light infomation, the visibleimage usually contains abundant texture and structure info, but issensitive to the environment, such as illumination and occlusion.</p><p>Complementary info: to fuse Ir and Vis image, to generate a desiredimage.</p><p>Fused image has been broadly used as a preprocessing module forhigh-level vision tasks, e.g., object detection, tracking, semanticsegmentation.</p><h3 id="motivation-1">Motivation</h3><p>Pressing challenges:</p><div class="code-wrapper"><pre><code class="hljs livecodeserver"><span class="hljs-number">1.</span>The existing fusion algorithms are inclined <span class="hljs-built_in">to</span> pursue better visual quality<span class="hljs-keyword">and</span> higher evaluation metrics but seldom systematically considerwhether fused image can facilitate high-level vision tasksSome studies: cannot effectively enhance/boost <span class="hljs-keyword">the</span> semantic info <span class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> fused image<span class="hljs-number">2.</span>Neither visual comparison nor quantitative evaluation (evaluation manners)reflects <span class="hljs-keyword">the</span> facilitation <span class="hljs-keyword">of</span> fused images <span class="hljs-keyword">for</span> high-level vision tasks<span class="hljs-number">3.</span>Not <span class="hljs-keyword">effective</span> <span class="hljs-keyword">in</span> extracting fine-grained detail features<span class="hljs-number">4.</span>Ignore <span class="hljs-keyword">the</span> demand <span class="hljs-keyword">for</span> real-<span class="hljs-built_in">time</span> image fusion</code></pre></div><p>Ours network:</p><p>SeAFusion, can be used for achieving real-time Ir &amp; Vis imagefusion</p><h3 id="method-1">Method</h3><div class="code-wrapper"><pre><code class="hljs pgsql"><span class="hljs-number">1.</span>Simultaneously obtaining superior performance <span class="hljs-keyword">in</span><span class="hljs-keyword">both</span> image fusion <span class="hljs-keyword">and</span> high-<span class="hljs-keyword">level</span> vision tasks.<span class="hljs-number">2.</span>A segmentation network <span class="hljs-keyword">to</span> predict the segmentation results<span class="hljs-keyword">on</span> fused images, which <span class="hljs-keyword">is</span> utilized <span class="hljs-keyword">to</span> construct semantic loss(<span class="hljs-keyword">is</span> leveraged <span class="hljs-keyword">to</span> guide the training <span class="hljs-keyword">of</span> the fusion network via back-propagation, so the loss can flow back <span class="hljs-keyword">to</span> the image fusion module<span class="hljs-keyword">to</span> forcing fused images <span class="hljs-keyword">to</span> contain more semantic infomation).<span class="hljs-number">3.</span><span class="hljs-type">Real</span>-<span class="hljs-type">time</span>: light-weight network based <span class="hljs-keyword">on</span> GRDB (gradient residual dense block)<span class="hljs-keyword">to</span> boost the description ability <span class="hljs-keyword">for</span> fine-grained details <span class="hljs-keyword">and</span> achieve feature reuse.<span class="hljs-number">4.</span>The authors proposed a joint low-<span class="hljs-keyword">level</span> <span class="hljs-keyword">and</span> high-<span class="hljs-keyword">level</span> adaptive training strategy<span class="hljs-keyword">to</span> achieve simultaneously impressive performance <span class="hljs-keyword">in</span> <span class="hljs-keyword">both</span>image fusion <span class="hljs-keyword">and</span> various high-<span class="hljs-keyword">level</span> vision tasks.</code></pre></div><h3 id="structure-diagram-1">Structure diagram</h3><figure><img src="/img/stage1/21.png"alt="The overall framework of the proposed semantic-aware infrared and visible image fusion algorithm." /><figcaption aria-hidden="true">The overall framework of the proposedsemantic-aware infrared and visible image fusion algorithm.</figcaption></figure><figure><img src="/img/stage1/22.png"alt="The architecture of the real-time infrared and visible image fusion network based on gradient residual dense block." /><figcaption aria-hidden="true">The architecture of the real-timeinfrared and visible image fusion network based on gradient residualdense block.</figcaption></figure><figure><img src="/img/stage1/23.png"alt="The specific devise of the gradient residual dense block. The Sobel operator is selected as the Gradient Operator to extract fine-grained detail information of feature maps." /><figcaption aria-hidden="true">The specific devise of the gradientresidual dense block. The Sobel operator is selected as the GradientOperator to extract fine-grained detail information of featuremaps.</figcaption></figure><h3 id="分析">分析</h3><figure><img src="/img/stage1/24.png" alt="Analysis1: fusion network" /><figcaption aria-hidden="true">Analysis1: fusion network</figcaption></figure><figure><img src="/img/stage1/25.png" alt="Analysis2: segmentation module" /><figcaption aria-hidden="true">Analysis2: segmentationmodule</figcaption></figure><figure><img src="/img/stage1/26.png" alt="Algorithm" /><figcaption aria-hidden="true">Algorithm</figcaption></figure><p>注：分割网络的backbone采用YOLOv5网络</p><h2 id="segmif">SegMiF</h2><h3 id="title-2">Title</h3><p>Multi-interactive Feature Learning and a Full-time Multi-modalityBenchmark for Image Fusion and Segmentation [ICCV] 2023</p><h3 id="abstract">Abstract</h3><p>Early efforts boost performance for only one task.</p><p>This paper, SegMiF can dual-task correlation to promote theperformance of both tasks (fusion and segmentation).</p><h3 id="intro-2">Intro</h3><p>Contributions:</p><div class="code-wrapper"><pre><code class="hljs sql"><span class="hljs-number">1.</span>SegMiF <span class="hljs-keyword">contains</span> two modules, i.e., <span class="hljs-keyword">fusion</span> network<span class="hljs-keyword">and</span> common segmentation network.<span class="hljs-number">2.</span>Hierarchical interactive attention (HIA) block. Fine<span class="hljs-operator">-</span>grained mapping <span class="hljs-keyword">of</span> <span class="hljs-keyword">all</span> the vital infomation <span class="hljs-keyword">between</span> <span class="hljs-keyword">fusion</span> <span class="hljs-keyword">and</span> segmentation. Modality<span class="hljs-operator">-</span><span class="hljs-operator">/</span>Semantic<span class="hljs-operator">-</span> oriented features can be fully mutual<span class="hljs-operator">-</span>interactive(bridge the feature gap <span class="hljs-keyword">between</span> <span class="hljs-keyword">fusion</span> <span class="hljs-keyword">and</span> segmentation).<span class="hljs-number">3.</span><span class="hljs-keyword">Dynamic</span> weight factor, automatically adjust the <span class="hljs-keyword">corresponding</span> weights<span class="hljs-keyword">of</span> <span class="hljs-keyword">each</span> task (optimal parameters).<span class="hljs-number">4.</span>Interactive feature training scheme.<span class="hljs-number">5.</span>Construct an imaging <span class="hljs-keyword">system</span> (benchmark).</code></pre></div><h3 id="structure-diagram-2">Structure diagram</h3><figure><img src="/img/stage1/31.png"alt="Workflow of the proposed SegMiF. The left part depicts the latent interactive relationship between image fusion and segmentation. The middle part plots the concrete architecture of the SegMiF. The right part details the components of proposed hierarchical interactive attention." /><figcaption aria-hidden="true">Workflow of the proposed SegMiF. The leftpart depicts the latent interactive relationship between image fusionand segmentation. The middle part plots the concrete architecture of theSegMiF. The right part details the components of proposed hierarchicalinteractive attention.</figcaption></figure><figure><img src="/img/stage1/32.png" alt="Partial zoom" /><figcaption aria-hidden="true">Partial zoom</figcaption></figure><h3 id="分析-1">分析</h3><figure><img src="/img/stage1/33.png" alt="Fusion network and DRDB module" /><figcaption aria-hidden="true">Fusion network and DRDBmodule</figcaption></figure><p><img src="/img/stage1/34.png"alt="Detailed architectures of SoAM and MoAM" />注：分割网络是论文[SegFormer: Simple and Efficient Design for SemanticSegmentation with Transformers]的内容</p><p>分割网络： <img src="/img/stage1/36.png"alt="Segmentation network" /></p><h4 id="hia">HIA</h4><p>1.SoAM (Semantic-oriented attention module)</p><blockquote><p>SoAM utilizes the token <spanclass="math inline">\(F_{seg}^{s}\)</span> to generate the query <spanclass="math inline">\(Q_s\)</span>, which represents the inhere semanticinformation that needs to be enhanced.</p></blockquote><blockquote><p>The global context representation of each can be calculated by as<span class="math inline">\(K_{ir}^{T}\cdot V_{ir} (G_{ir})\)</span> and<span class="math inline">\(K_{vis}^{T}\cdot V_{vis} (G_{vis})\)</span>, 其中K, V ∈ {<span class="math inline">\(F_{ir}^{s}\)</span>, <spanclass="math inline">\(F_{vis}^{s}\)</span>}</p></blockquote><p><span class="math inline">\(S_{ir} = Q_s\cdot G_{ir}\)</span>, <spanclass="math inline">\(S_{vis} = Q_s\cdot G_{vis}\)</span></p><blockquote><p>SoAM to provide more semantic attention for the modality feature.</p></blockquote><p>2.MoAM (Modality-oriented attention module)</p><blockquote><p>MoAM introduce two modality queries <spanclass="math inline">\(Q_{ir}\)</span> and <spanclass="math inline">\(Q_{vis}\)</span> ∈ {<spanclass="math inline">\(F_{ir}^{m}\)</span>, <spanclass="math inline">\(F_{vis}^{m}\)</span>}</p></blockquote><blockquote><p>The global context of segmentation <spanclass="math inline">\(G_s\)</span> by <spanclass="math inline">\(K_{s}^{T}\cdot V_s\)</span></p></blockquote><p><span class="math inline">\(M_{ir} = Q_{ir}\cdot G_s\)</span>, <spanclass="math inline">\(M_{vis} = Q_{vis}\cdot G_s\)</span></p><blockquote><p>MoAM to investigate the significant feature from semanticcontexts.</p></blockquote><h4 id="目标函数">目标函数</h4><figure><img src="/img/stage1/35.png" alt="Joint formulation" /><figcaption aria-hidden="true">Joint formulation</figcaption></figure><p>损失函数： <img src="/img/stage1/37.png" alt="Loss" /></p><div class="code-wrapper"><pre><code class="hljs armasm">损失函数中涉及了两个常用的概念：结构相似度、显著性图<span class="hljs-number">1</span>.结构相似度SSIM主要用来衡量两幅图亮度、对比度和结构的相似性  详见<span class="hljs-string">&quot;专业笔记&quot;</span>中所述<span class="hljs-number">2</span>.显著性图主要用来计算MSE损失中的显著性参数m  源自论文: Infrared <span class="hljs-keyword">and</span> visible image fusion based on visual saliency <span class="hljs-meta">map</span>            <span class="hljs-keyword">and</span> weighted least square optimization</code></pre></div><h3 id="相关内容">相关内容</h3><div class="code-wrapper"><pre><code class="hljs mathematica">当<span class="hljs-variable">CNN</span>层数变深时，输出到输入的路径就会变得很长。梯度反向传播，到达输入层可能就会消失。<span class="hljs-variable">DenseNet</span>是一种深度卷积神经网络，引入密集连接（<span class="hljs-variable">Dense</span> <span class="hljs-variable">Connection</span>）将前面所有层与后面的层建立密集连接。与<span class="hljs-variable">ResNet</span>的关键区别是，<span class="hljs-variable">ResNet</span>是简单相加，<span class="hljs-variable">DenseNet</span>是进行连接。<span class="hljs-variable">DenseNet</span>通过基本构建单元<span class="hljs-variable">Dense</span> <span class="hljs-built_in">Block</span>实现稠密连接对特征进行重用，实现信息共享，并能增强梯度流动，避免梯度消失。过渡层（<span class="hljs-variable">Transition</span> <span class="hljs-variable">Layer</span>）控制通道数（稠密块会带来通道数的增加），防止模型过于复杂。<span class="hljs-variable">DenseNet</span>在模型精度和泛化能力上通常表现优异，训练更稳定，但结构仍比较复杂，需要消耗较多的计算资源和时间，内存占用大。<span class="hljs-variable">ResNet</span>（<span class="hljs-variable">Residual</span> <span class="hljs-variable">Neural</span> <span class="hljs-variable">Network</span>）是基于残差学习框架的神经网络，其在前向网络中增加了一些快捷连接<span class="hljs-variable">Shortcut</span><span class="hljs-punctuation">(</span><span class="hljs-built_in">Short</span><span class="hljs-punctuation">)</span> <span class="hljs-variable">Connection</span><span class="hljs-operator">/</span><span class="hljs-built_in">Skip</span> <span class="hljs-variable">Connection</span>，这些连接会跳过某些层，将数据直接传到之后的层。<span class="hljs-variable">ResNet</span>对网络深度增加带来的梯度消失或爆炸、网络退化（由于训练和测试误差的积累导致正确率趋于饱和甚至下降，与过拟合（训练误差小，测试误差大，泛化能力差）不同）等问题具有一定的作用，增加了非线性，一定程度上抑制了语义间隙的影响，但模型表现可能略逊一筹。残差块（<span class="hljs-variable">Residual</span> <span class="hljs-built_in">Block</span>）是<span class="hljs-variable">ResNet</span>的基本组成成分。</code></pre></div><div class="code-wrapper"><pre><code class="hljs arcade">空洞卷积（Dilated/Atrous Convolution）可以增加卷积核的尺寸，如原本<span class="hljs-number">3</span>*<span class="hljs-number">3</span>的卷积核可以扩充至<span class="hljs-number">5</span>*<span class="hljs-number">5</span>，但有效的参数个数不变，仍为<span class="hljs-number">9</span>个，剩余的位置不予考虑（空洞/零），利用超参数“扩张率”来定义卷积核处理数据时各值的间距（可以理解为空洞数），在扩大了感受野的同时，避免了Pooling操作，从而保持分辨率不变。PS：正常扩大感受野会带来计算量的增加，后面进行池化的降采样处理可以降低计算量，但是空间分辨率也随之降低了。感受野（Receptive Field）越大，捕获的图像区域越大，对图像全局的特征提取能力也就越强。而且对于目标检测任务而言，最后一层特征图（<span class="hljs-built_in">Feature</span> <span class="hljs-built_in">Map</span>）的感受野大小要大于等于输入图像大小，否则分类性能会不理想。一般而言，感受野越大、网络越深，对复杂问题求解的模型性能越好。</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>summarize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>专业笔记</title>
    <link href="/2023/11/25/Professional/"/>
    <url>/2023/11/25/Professional/</url>
    
    <content type="html"><![CDATA[<h3 id="terminology-index">Terminology Index</h3><div class="code-wrapper"><pre><code class="hljs mipsasm">IQA：图像质量评价，image quality assessment上游任务，预训练模型，对应 low-level task；下游任务，具体的 task，对应 high-level taskAblation study：消融实验，移除 Model 的部分<span class="hljs-string">&quot;feature&quot;</span>（控制变量法），对比研究模型的性能FPS：每秒传输帧数，frames per secondFine-tune：微调光谱（Spectrum）：复色光经过分光成为单色光，经成像系统得到按波长或者频率依次排列的光学图像多光谱（<span class="hljs-keyword">Multispectrum）：同时获取多个光学频谱波段（大于3个），在可见光基础上向红外、紫外两个方向扩展</span><span class="hljs-keyword"></span>高光谱（Hyperspectrum）：包含成百上千的波段，可以捕获和分析一片空间区域内“逐点”的光谱Embedding：在深度学习中利用线性或非线性转换，对复杂的数据进行自动特征抽取，并将其features表示为向量形式，以便于输入到网络中进行处理。这个过程被称之为<span class="hljs-string">&quot;Embedding&quot;</span><span class="hljs-keyword">BN(Batch </span><span class="hljs-keyword">Normalization)：像深度学习模型输入数据时，要对data进行归一化操作，如均值为0、方差为1的规范化处理，</span><span class="hljs-keyword"></span>否则过大的特征会“淹没”小特征。这种情况不仅会在输入模型时出现，也会在层与层中存在，因为随着数据的逐级传播，经过损失函数（如Softmax）后会使得特征之间的差异变大，逐渐累积会对最终结果造成较大的影响。所以需要对层间数据也进行规范化，<span class="hljs-keyword">BN就是在两个隐藏层之间对数据在Batch方向进行Norm处理，</span><span class="hljs-keyword"></span>并且最后加入了<span class="hljs-keyword">Scale </span><span class="hljs-keyword">and </span><span class="hljs-keyword">Shift操作，以提高网络对Norm的学习能力。</span><span class="hljs-keyword"></span>LN(Layer <span class="hljs-keyword">Normalization)：与BN不同之处在于，LN是在数据的Channel方向进行Norm归一化操作。</span><span class="hljs-keyword"></span><span class="hljs-keyword">BN是对一个batch的数据的对应位置上所有的feature进行归一化，而LN是对每个sample进行归一化。</span><span class="hljs-keyword"></span>在三维视觉任务的情形下，可以理解为：<span class="hljs-keyword">BN对每一张图片对应位置的像素做Norm处理，LN则是对单幅图像的整体做Norm。</span><span class="hljs-keyword"></span>但通常在数据传入Model前会进行数据的归一化处理，因此通常不会再使用LN。</code></pre></div><p><img src="/img/stage1/bn.png" alt="Batch Norm" /> <imgsrc="/img/stage1/ln.png" alt="Layer Norm" /></p><h3 id="距离度量">距离度量</h3><div class="code-wrapper"><pre><code class="hljs arcade">欧氏距离（Euclidean <span class="hljs-built_in">Distance</span>）：两点连线长度城区距离（曼哈顿距离，Manhattan <span class="hljs-built_in">Distance</span>）：直角移动距离，不涉及对角线棋盘距离（切比雪夫距离，Chebyshev <span class="hljs-built_in">Distance</span>）：沿某个轴的距离最大值汉明距离（Hamming <span class="hljs-built_in">Distance</span>）：比较两等长二进制串不同值的个数余弦相似度（Cosine Similarity）：两向量夹角余弦表示距离，与内积有关</code></pre></div><p>范数：</p><p>1.Norm of vector:</p><figure><img src="/img/stage2/norm_vector.png" alt="向量的范数" /><figcaption aria-hidden="true">向量的范数</figcaption></figure><p>2.Norm of matrix:</p><p>若 <span class="math inline">\(A = (a_{ij})_{m \times n}\)</span>，则矩阵的诱导范数（算子范数）为：</p><p>p-范数：<span class="math inline">\(\Vert A \Vert _p =\mathop{max}\limits_{x \neq 0} \frac{\Vert Ax \Vert _p}{\Vert x \Vert_p}\)</span></p><p>L1范数（列和的最大值）：<span class="math inline">\(\Vert A \Vert_1 =\mathop{max}\limits_{1\le j \le n} \sum\limits_{i=1}^{m} \vert a_{ij}\vert\)</span></p><p>无穷范数（行和的最大值）：<span class="math inline">\(\Vert A\Vert_\infty = \mathop{max}\limits_{1\le i \le m} \sum\limits_{j=1}^{n}\vert a_{ij} \vert\)</span></p><p>L2范数（谱范数，当A为方阵时）：<span class="math inline">\(\Vert A\Vert_2 = \sqrt{\lambda_{max} (A^TA)}\)</span></p><p>矩阵<span class="math inline">\(A_{m \timesn}\)</span>的元范数（元素p-范数）：<span class="math inline">\(\Vert A\Vert _p = (\sum\limits_{i=1}^{m} \sum\limits_{j=1}^{n} \vert a_{ij}\vert ^p)^{\frac{1}{p}}\)</span></p><p>注：不要把矩阵元 p-范数与诱导 p-范数混淆。</p><p>对p = 2的元范数，称为弗罗贝尼乌斯范数（Frobeniusnorm）或希尔伯特-施密特范数（Hilbert–Schmidtnorm），不过后面这个术语通常只用于希尔伯特空间。Frobenius-norm可用不同的方式定义：<span class="math display">\[ \Vert A \Vert _F =\sqrt{\sum\limits_{i=1}^{m} \sum\limits_{j=1}^{n} \vert a_{ij} \vert ^2}= \sqrt{trace(A^{*} \cdot A)} = \sqrt{\sum\limits_{i=1}^{min\{m,n\}}\sigma _i^2} \]</span> 其中<spanclass="math inline">\(A^{*}\)</span>是<spanclass="math inline">\(A\)</span>的共轭转置，<spanclass="math inline">\(\sigma _i\)</span>是<spanclass="math inline">\(A\)</span>的奇异值，<spanclass="math inline">\(trace()\)</span>是迹函数。</p><p>Schaten范数出现于当元p-范数应用于一个矩阵的奇异值向量时。如果奇异值记做<spanclass="math inline">\(\sigma _i\)</span>，则Schatten p-范数定义为：<span class="math display">\[ \Vert A \Vert _F =\bigg(\sum\limits_{i=1}^{min\{m,n\}} \sigma _i^p \bigg)^{\frac{1}{p}}\]</span></p><p>TV范数：Total Variation，一种常用的正则项（距离度量），参考文章：<ahref="https://www.zhihu.com/question/47162419">如何理解全变分Distance</a></p><p>范数的等价需要一致范数的知识点，详情可以参考：<ahref="https://blog.csdn.net/qiuchangyong/article/details/101041583">各种矩阵范数总结</a></p><h3 id="常见损失函数">常见损失函数</h3><p>SSIM是一种衡量图像结构相似性的算法，结合了图像的亮度，对比度和结构三方面对图像质量进行测量。SSIM的公式如下:</p><p><span class="math inline">\(SSIM(x,y) = [L(x,y)]^{\alpha} \cdot[C(x,y)]^{\beta} \cdot [S(x,y)]^{\gamma}\)</span></p><p>L(x,y)是亮度部分：</p><p><span class="math inline">\(L(x,y) = \dfrac{2\mu _x\mu _y + C_1}{\mu_x^2 + \mu _y^2 + C_1}\)</span></p><p>C(x,y)是对比度部分：</p><p><span class="math inline">\(C(x,y) = \dfrac{2\sigma _x\sigma _y +C_2}{\sigma _x^2 + \sigma _y^2 + C_2}\)</span></p><p>S(x,y)是结构部分：</p><p><span class="math inline">\(L(x,y) = \dfrac{\sigma _{xy} +C_3}{\sigma _x\sigma _y + C_3}\)</span></p><p>式子中，<span class="math inline">\(\mu _x\)</span>与<spanclass="math inline">\(\mu _y\)</span>是图像的像素平均值，<spanclass="math inline">\(\sigma _x\)</span>和<spanclass="math inline">\(\sigma _y\)</span>是图像的像素标准差，<spanclass="math inline">\(\sigma _{xy}\)</span>是图像x和y的协方差，<spanclass="math inline">\(C_1\)</span>、<spanclass="math inline">\(C_2\)</span>、<spanclass="math inline">\(C_3\)</span>是常数，防止分母等于0。一般情况下，<spanclass="math inline">\(\alpha = \beta = \gamma = 1\)</span>，<spanclass="math inline">\(C_2 = 2\times C_3\)</span>，则有：</p><p><span class="math inline">\(SSIM(x,y) = \dfrac{(2\mu _x\mu _y +C_1)}{(\mu _x^2 + \mu _y^2 + C_1)}\cdot \dfrac{(2\sigma _{xy} +C_2)}{(\sigma _x^2 + \sigma _y^2 + C_2)}\)</span></p><h3 id="图片格式">图片格式</h3><div class="code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-keyword">JPEG，静态图像压缩标准，压缩比越高，质量越差</span><span class="hljs-keyword"></span><span class="hljs-keyword">JPG，与JPEG类似，相当于简化版，去除了相机的拍照参数等</span><span class="hljs-keyword"></span>PNG，无损压缩，位图片格式GIF，静态、动态图像，如动图、表情包TIFF，无失真压缩，占用空间较大TGA，兼顾 <span class="hljs-keyword">BMP </span>图像的质量与 <span class="hljs-keyword">JPEG </span>的体积<span class="hljs-keyword">BMP，位图 </span><span class="hljs-keyword">Bitmap，不采用压缩，占用空间大</span><span class="hljs-keyword"></span>SVG，二维矢量图形格式RAW，经 CMOS 或 CCD 图像感应器将捕捉到的光信号转换为数字信号的原始数据HDR，高动态范围图像，High-Dynamic Range，记录了照明信息</code></pre></div><h3 id="拓扑结构">拓扑结构</h3><p>什么是拓扑结构？</p><p>所谓“拓扑”，就是把实体抽象成与其大小、形状无关的“点”，把连接实体的线路抽象成“线”，进而以图的形式来表达这些点与线的关系。这个图被称为拓扑结构图，表示的关系是拓扑结构。</p><div class="code-wrapper"><pre><code class="hljs">几何结构：强调点与线构成的形状及大小，考察点、线（甚至面）的位置关系。拓扑结构：注重点、线之间的连接关系。</code></pre></div><p>例子：梯形、矩形、平行四边形、圆具有不同的形状，因此属于不同的几何结构，但是组成它们的点、线连接关系是一样的，因此具有相同的拓扑结构（环形结构）。</p><p>在计算机网络中，我们把计算机、终端、通信处理机等设备抽象成点，把连接这些设备的通信线路抽象成线，其构成的拓扑叫网络拓扑结构，反应网络的结构关系。</p><div class="code-wrapper"><pre><code class="hljs">几种常见的网络拓扑结构：总线型、星型、环型、树型、网状型等。</code></pre></div><p><a href="https://zhuanlan.zhihu.com/p/451069548?utm_id=0"title="什么是拓扑结构？">详情点击此处查看</a></p><h3 id="线性代数基础知识">线性代数基础知识</h3><h4 id="矩阵的正定和负定">矩阵的正定和负定</h4><p>矩阵Q正定的充要条件：<br /> (1) Q所有的特征值大于0 <br /> (2)Q各阶顺序主子式都大于0 <br /> (3) Q各阶主子式都大于0 <br /> (4)存在非奇异矩阵G，使得 <span class="math inline">\(Q = GG^T\)</span></p><p>矩阵Q半正定（正半定）的充要条件：<br /> (1) Q所有的特征值大于等于0<br /> (2) Q各阶主子式都大于等于0 <br /> (3) 存在矩阵G，使得 <spanclass="math inline">\(Q = GG^T\)</span></p><p>矩阵Q负定的充要条件：<br /> (1) Q所有的特征值小于0 <br /> (2)Q奇数阶顺序主子式都小于0，偶数阶顺序主子式都大于0 <br /> (3)Q奇数阶主子式都小于0，偶数阶主子式都大于0 <br /> (4)存在非奇异矩阵G，使得 <span class="math inline">\(-Q = GG^T\)</span></p><p>矩阵Q半负定（负半定）的充要条件：<br /> (1) Q所有的特征值小于等于0<br /> (2) Q奇数阶主子式都小于等于0，偶数阶主子式都大于等于0 <br /> (3)存在矩阵G，使得 <span class="math inline">\(-Q = GG^T\)</span></p><p>关于更基础的一些概念：主子式、顺序主子式、余子式、代数余子式、伴随矩阵、方向导数与梯度等内容，自行回顾学习！</p><h4 id="奇异非奇异矩阵">奇异、非奇异矩阵</h4><p>讨论非奇异矩阵和奇异矩阵的前提：矩阵A是n阶方阵。</p><p>矩阵A可逆（非奇异）的判别方法（满足下列充要条件之一）：<br /> (1)<span class="math inline">\(det(A) = \vert A \vert \neq 0\)</span><br /> (2) <span class="math inline">\(Rank(A) = n\)</span> 即满秩<br /> (3) <span class="math inline">\(A\)</span> 的所有特征值都不等于0<br /> (4) <span class="math inline">\(A\)</span>可以表示成若干个初等矩阵的乘积（初等矩阵是可逆的）</p><p>矩阵A奇异的判别方法（满足下列条件之一）：<br /> (1) <spanclass="math inline">\(\vert A \vert = 0\)</span> <br /> (2) <spanclass="math inline">\(Rank(A) &lt; n\)</span> 非满秩</p><p>非奇异矩阵就是可逆矩阵（满秩矩阵）。在数学上，“奇异”(singular)一词被用来形容破坏了某种优良性质的数学对象。对于矩阵来说，“可逆”是一个好的性质，表示该矩阵包含了其所能容纳的所有可用信息（不存在某几个行/列向量是线性相关的），即满秩。奇异矩阵行列式值为0，不可逆，等价于某个线性变换退化了，即变换之后有部分东西丢失了。</p><p>PS(PostScript): <div class="code-wrapper"><pre><code class="hljs css"><span class="hljs-selector-tag">A</span>的充分条件是<span class="hljs-selector-tag">B</span>（<span class="hljs-selector-tag">B</span>是<span class="hljs-selector-tag">A</span>的充分条件，条件是<span class="hljs-selector-tag">B</span>，结论是<span class="hljs-selector-tag">A</span>），说明有了<span class="hljs-selector-tag">B</span>就一定有<span class="hljs-selector-tag">A</span>（充分性），即由条件<span class="hljs-selector-tag">B</span>可以推导出结论<span class="hljs-selector-tag">A</span>。<span class="hljs-selector-tag">A</span>的必要条件是<span class="hljs-selector-tag">B</span>（<span class="hljs-selector-tag">B</span>是<span class="hljs-selector-tag">A</span>的必要条件，即<span class="hljs-selector-tag">B</span>是必要的），说明没有<span class="hljs-selector-tag">B</span>就一定没有<span class="hljs-selector-tag">A</span>，因为<span class="hljs-selector-tag">B</span>是必要的（要作为结论），所以由<span class="hljs-selector-tag">A</span>可以推导出<span class="hljs-selector-tag">B</span>。</code></pre></div></p><p>Reference: <ahref="https://zhuanlan.zhihu.com/p/334822347">矩阵可逆的几个充要条件</a></p><h4 id="特征值与特征向量">特征值与特征向量</h4><p>定义：<span class="math inline">\(Ax = \lambda x\)</span>，其中 <spanclass="math inline">\(A\)</span> 是矩阵，<spanclass="math inline">\(\lambda\)</span> 是特征值（也叫本征值），非零向量<span class="math inline">\(x\)</span> 是特征向量。<spanclass="math inline">\(A \cdot x\)</span> 表示用矩阵对向量 <spanclass="math inline">\(x\)</span>进行一次线性变换（可以是拉伸+旋转，也可以只是拉伸或者只是旋转），而这种转换的效果就是一个常数<span class="math inline">\(\lambda\)</span> 与向量 <spanclass="math inline">\(x\)</span>的乘积（说明只进行了拉伸）。换句话说，就是对一个特定的矩阵 <spanclass="math inline">\(A\)</span> 来说，总存在一些特定方向的向量 <spanclass="math inline">\(x\)</span>，使得 <spanclass="math inline">\(Ax\)</span> 和 <spanclass="math inline">\(x\)</span>的方向一样（未发生变化），只有长度改变了。这个长度的变化就是特征值的几何含义。</p><p>我们通常求特征值和特征向量就是为了求出在笛卡尔坐标系中，该矩阵能使哪些向量（当然是特征向量）只发生拉伸，使其发生拉伸的程度如何（特征值大小）。如果把矩阵看作运动，那么特征值就是运动的速度，特征向量是运动的方向。特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定（缩放因子）。旋转矩阵是没有实数特征向量和特征值的。这样做的意义在于，看清一个矩阵在哪些方面能产生最大的效果，并根据所产生的每个特征向量（一般研究特征值最大的那几个）进行分类讨论，可以作为主成分进行研究、减少计算量、降维等。</p><p>附：笛卡尔坐标系：正交坐标系，比如直角坐标系（坐标轴互相垂直）。笛卡尔空间中，任何一个点的位置由数轴上对应的坐标设定。</p><h4 id="极大线性无关组">极大线性无关组</h4><h5 id="线性相关与线性无关">线性相关与线性无关</h5><p>在一个线性空间中，如果一组向量 <span class="math inline">\(a_1, a_2,\cdots, a_s\)</span> 是线性相关的，那么存在一组不全为0的数 <spanclass="math inline">\(k_1, k_2, \cdots, k_s\)</span> ，使得 <spanclass="math inline">\(k_1a_1 + k_2a_2 + \cdots + k_sa_s = 0\)</span>成立。</p><p>如果 <span class="math inline">\(a_{j \ (j=1,2,\ldots,s)}\)</span>线性无关，则 <span class="math inline">\(k_1a_1 + k_2a_2 + \cdots +k_sa_s = 0\)</span> 成立：当且仅当所有的 <spanclass="math inline">\(k_{j \ (j=1,2,\ldots,s)} = 0\)</span>。</p><h5 id="极大线性无关组-1">极大线性无关组</h5><p>设A是一个n维向量组，如果从A中取出s个向量 <spanclass="math inline">\(a_1, a_2,\cdots, a_s\)</span>组成向量组，该向量组线性无关，且对于A中的其他任意向量b，都有 <spanclass="math inline">\(a_1, a_2, \cdots, a_s, b\)</span>线性相关，那么向量组 <span class="math inline">\(a_{j \(j=1,2,\ldots,s)}\)</span>是矩阵（向量组）A的一个极大线性无关组，简称极大无关组。</p><h4 id="矩阵的秩">矩阵的秩</h4><p>小时候老师总是告诉我们：要有n个方程才能解出n个未知数。上了大学以后，我们知道这句话其实是不严谨的。如果你想确定的解出这n个未知数，只有n个方程是不够的，这n个方程必须都是“有用的”，也就是线性无关的才行，否则就会有某几个方程式表达重复（无用）的信息，相当于只是换了衣服而已，实质上还是同一个方程式，自然就无法求解其对应的未知元。这些真正有用的方程式个数就是（系数）矩阵的秩。从这个角度，我们可以更好地理解什么是矩阵的秩（Rank）。</p><p>矩阵的秩，是矩阵中极大线性无关组的向量个数，也就是以此矩阵的元素作为系数的方程组中，线性无关的方程个数。从另一个角度说，秩是矩阵变换以后的空间维度，代表这个矩阵实际包含的信息量的多少。我们可以回顾一下初等变换的过程：对矩阵的行列式作初等行变换时，线性相关的向量（行）会被消去（成为0），即不含有用信息。</p><p>线性代数，主要就是研究n元1次线性方程组（某个问题）解的存在性（有没有答案？）和解的唯一性（答案是否只有一个？）。如果存在解，如何找到这个解或者所有解？一般的含有n个未知数和m个方程的非齐次线性方程组可以表示如下：<span class="math display">\[ \begin{cases}a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\\qquad \qquad \qquad \quad \vdots \\a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m \\\end{cases} \]</span></p><p>若 <span class="math inline">\(n\le m\)</span> ，则有以下结论：</p><p>1）当方程组的系数矩阵A的秩：Rank(A) 与方程组增广矩阵 (A, b)的秩：Rank(A, b) 相等，并且都等于未知元个数 n 时，方程组具有唯一解；</p><p>2）当方程组 Rank(A) = Rank(A, b) &lt; n 时，方程组有无穷多个解；</p><p>3）当方程组 Rank(A) &lt; Rank(A, b) 时，无解。</p><p>结论(1)很好理解，方程组里所有方程式互不冲突，m =n，信息一一对应。结论(2)，方程组里所有方程互不冲突，但是 m &lt;n，存在自由变量可以随意取值（约束不够），因此具有无穷多解。结论(3)，存在多个方程式有冲突，信息有互斥的成分，存在"0 = c" 的现象，自然无法求解。</p><p>另外，根据克拉默法则：</p><p>1）当系数矩阵A的行列式值：det(A) = 0时，线性方程组（齐次或非齐次）要么有无穷多解，要么无解。</p><p>2a）对于齐次线性方程组（<span class="math inline">\(b_i =0\)</span>）来说，det(A)不等于0时，只有零解；det(A)等于0时，有0解和非0解。</p><p>2b）对非齐次方程组（上式）来说，det(A)不等于0时，有唯一解；det(A)等于0时，有无穷多解或没有解。</p><p>参考：<ahref="https://www.zhihu.com/question/21605094">如何理解矩阵的秩？</a></p><h4 id="矩阵的行列式和迹">矩阵的行列式和迹</h4><p>矩阵（方阵）A的迹：trace(A)，是矩阵A所有特征值之和。对 <spanclass="math inline">\(n \times n\)</span>的方阵A而言，迹等于A所有主对角线上的元素之和。</p><p>矩阵（n阶方阵）A的行列式（determinant）的值：det(A)，是方阵A全部特征值的乘积。行列式描述的是一个线性变换对欧几里得空间（线性的，可以用加法和数乘表示空间中所有元素；内积空间，可以用内积表示空间向量的角度信息（一定是赋范空间）；维度有限）中“体积”造成的影响。因为特征值是在特征向量方向上放缩的程度，可以认为特征值构成了特征空间的“骨”，类似于多面体的“边”、“高”等信息，当这些特征值相乘时，可以粗略的类比为体积的概念。</p>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>basic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列5：数码</title>
    <link href="/2023/11/25/NumericalCode/"/>
    <url>/2023/11/25/NumericalCode/</url>
    
    <content type="html"><![CDATA[<p>正在施工中……</p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>functional</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列4：ICT</title>
    <link href="/2023/11/25/ICT/"/>
    <url>/2023/11/25/ICT/</url>
    
    <content type="html"><![CDATA[<h3 id="显示器接口">显示器接口</h3><ol type="1"><li>VGA接口</li></ol><blockquote><p>采用模拟协议，常用于老式显示器。目前不常用了，像一个四圆角梯形。</p></blockquote><figure><imgsrc="https://pica.zhimg.com/80/v2-9260a9e7bb2d44713e0976a8ac2ee667_1440w.webp?source=2c26e567"title="VGA接口" alt="VGA接口" /><figcaption aria-hidden="true">VGA接口</figcaption></figure><ol start="2" type="1"><li>DVI接口</li></ol><blockquote><p>有很多种类，算是VGA到HDMI的过渡产物。</p></blockquote><figure><imgsrc="https://pica.zhimg.com/80/v2-5dd3c4951a6a5c178e773deb1ba6b422_1440w.webp?source=2c26e567"title="DVI-D接口" alt="DVI-D接口" /><figcaption aria-hidden="true">DVI-D接口</figcaption></figure><ol start="3" type="1"><li>HDMI接口</li></ol><blockquote><p>音视频同时传输，支持高动态范围HDR成像，很常用，像带两个弯角的矩形。</p></blockquote><figure><imgsrc="https://picx.zhimg.com/80/v2-50888e26e96dafd6b0b5df25451923b9_1440w.webp?source=2c26e567"title="HDMI接口" alt="HDMI接口" /><figcaption aria-hidden="true">HDMI接口</figcaption></figure><ol start="4" type="1"><li>DP接口</li></ol><blockquote><p>DP(Digital Port)，也很常用，是HDMI的竞争对手，有很大的发展前景。</p></blockquote><figure><imgsrc="https://picx.zhimg.com/80/v2-3c1c4109ffc3f017bfdea221a49b77ca_1440w.webp?source=2c26e567"title="DP接口" alt="DP接口" /><figcaption aria-hidden="true">DP接口</figcaption></figure><h3 id="dns">DNS</h3><p>DNS(Domain NameSystem，域名系统)，用于域名解析，是因特网上作为域名和IP地址互相映射的一个分布式数据库，能让用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。</p><p><a href="https://zhuanlan.zhihu.com/p/186028919"title="什么是DNS？">点击查看详情</a></p><h3 id="路由器相关">路由器相关</h3><h4 id="直连">直连</h4><p>用一根网线连接两台主机的网口，可以相互发送和接收数据。<div class="code-wrapper"><pre><code class="hljs">多台主机通信怎么办？</code></pre></div></p><h4 id="集线器">集线器</h4><p>集线器(HUB)，将网线集结起来，实现初级的网络互通，工作在物理层，半双工通信。同时可以将信号放大后再传输以扩大传输距离。<div class="code-wrapper"><pre><code class="hljs 1c">集线器以广播形式以<span class="hljs-string">&quot;泛红转发&quot;</span>来发送数据，无法分辨具体的主机。</code></pre></div></p><h4 id="交换机">交换机</h4><p>交换机(Switch)，在集线器原有功能上，增加了自动寻址能力和交换作用，通过学习MAC地址（MAC地址表），查找对应的端口号，建立临时交换路径进行收发数据，工作在数据链路层，全双工通信。<div class="code-wrapper"><pre><code class="hljs">要求交换机端口上所有主机在同一个子网中。不同网段的主机怎么通信？</code></pre></div></p><h4 id="路由器">路由器</h4><p>路由器(Router)，连接不同类型的网络并能选择数据传输的IP路径，充当网关的角色，能在多网络互联环境中建立灵活的连接，工作在网络层。<div class="code-wrapper"><pre><code class="hljs arduino">家里拉了一条宽带（一条网线，现在多为光纤），只能一个人享受上网。如果要多设备连接网络，或者享受无线上网(<span class="hljs-built_in">WiFi</span>)，那么就需要安装路由器实现IP地址分配、网络共享和无线网使用（无线路由器），甚至进行网络加速（增强）。</code></pre></div></p><h4 id="猫">猫</h4><p>猫(Modem)，即光猫，调制解调器，进行光电转换。为了取代之前慢速的电话线上网，目前运营商多采用光纤传输，利用高速的光信号（会被限速，按你交的钱让你使用相应的带宽）进行数据传输。光信号无法直接被设备利用，经过光猫，将其转换为数字信号，即可被设备使用。</p><h4 id="ipmac地址">IP/MAC地址</h4><p>IP(InternetProtocol)，即互联网协议地址，为互联网上每一个网络和每一台主机配置唯一的逻辑地址，与物理地址区分。</p><p>IP地址分为IPv4和IPv6，IPv4使用32位（4字节）地址，IPv6地址长度为128位。最初设计互联网络时，为了便于寻址和层次化构造网络，每个IP地址包括两个标识码（ID），也就是网络ID和主机ID。同一个物理网络上的所有主机都使用同一个网络ID，网络上的一个主机（包括网络上工作站，服务器和路由器等）有一个主机ID与其对应。以IPv4地址为例，IP地址分为：1、公有地址(Publicaddress)，我们通过公有IP地址是可以实现直接访问因特网的。2、私有地址(Privateaddress)，分为五类：A类、B类、C类、D类、E类。其中，A、B、C类私有地址是由InternetNIC公司在全球范围内统一分配的，D、E类为特殊地址。<div class="code-wrapper"><pre><code class="hljs dns"><span class="hljs-keyword">A</span>类IP地址(适用于大型网络)的网络的标识(网络ID)长度为<span class="hljs-number">8</span>位，主机标识(主机ID)长度为<span class="hljs-number">24</span>位，它的范围：<span class="hljs-number">1.0.0.1</span>到<span class="hljs-number">127.255.255.254</span>；B类IP地址(适用于中型网络)的网络ID为<span class="hljs-number">16</span>位，主机ID长度为<span class="hljs-number">16</span>位，它的范围：<span class="hljs-number">128.0.0.1</span>-<span class="hljs-number">191.255.255.254</span>；C类IP地址(适用于小型网络)网络ID为<span class="hljs-number">24</span>位，主机ID长度为<span class="hljs-number">8</span>位，它的范围：<span class="hljs-number">192.0.0.1</span>-<span class="hljs-number">223.255.255.254</span>；D类地址被叫做多播地址(multicast address)，即组播地址，它的范围：<span class="hljs-number">224.0.0.0</span>到<span class="hljs-number">239.255.255.255</span>；E类地址主要用于Internet试验和开发，它的范围：<span class="hljs-number">240.0.0.0</span>~<span class="hljs-number">255.255.255.255</span></code></pre></div></p><p>MAC地址（Media Access ControlAddress）的全称叫做媒体访问控制地址，也称作局域网地址、以太网地址或者物理地址。MAC地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的MAC地址。MAC地址共48位（6个字节），前24位由IEEE（电气和电子工程师协会）决定如何分配，后24位由实际生产该网络设备的厂商自行制定。</p><p>OSI模型（Open System Interconnection ReferenceModel），是一种概念模型，是一个标准，一个试图使各种计算机在世界范围内互连为网络的标准框架。<div class="code-wrapper"><pre><code class="hljs armasm">第一层：物理层（Physical Layer），它是提供物理链路、传递电信号或光信号用的在局域网上传输比特流，它负责管理计算机通信设备和网络媒体之间的互通包括针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等第二层：数据链路层（<span class="hljs-meta">Data</span> Link Layer），负责网络寻址、错误侦测和改错当表头和表尾被加至数据包时，会形成帧（数据帧：<span class="hljs-meta">data</span> frame）而且此层还负责MAC地址第三层：网络层（Network Layer），决定数据的路径选择（数据选路）和转寄将网络表头（NH）加至数据包，以形成分组网络表头包含了网络数据，例如：<span class="hljs-built_in">IP</span>地址第四层：传输层（Transport Layer），它会建立一个安全通道，以防数据丢失端到端之间的连接建立，把传输表头（TH）加至数据以形成数据包传输表头包含了所使用的协议等发送信息，例如：传输控制协议（TCP）第五层：会话层（Session Layer），负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接第六层：表达层（Presentation Layer），将信息数据进行加密，及数据的转化信息数据经过加密、转换、压缩，转换为能与接收者的系统格式兼容并适合传输的格式第七层：应用层（Application Layer），起调用的作用提供为应用软件而设的接口，设置与另一应用软件之间的通信例如: HTTP，HTTPS，FTP，TELNET，SSH，SMTP，POP3.HTML.等</code></pre></div></p><p>IP地址与MAC地址的区别：1、IP地址应用于OSI模型的网络层，而MAC地址应用在OSI模型的数据链路层。2、地址长度、设计理念不同、分配依据不同。3、IP地址未必都不一样，与地理区域等有关。MAC地址只和硬件设备有关。</p><p>IP地址的提出，主要是为了减少广播的数量，可以智能学习目标地址在哪个网卡。</p><p><a href="https://www.zhihu.com/question/21546408"title="有了 IP 地址，为什么还要用 MAC 地址？">点击查看详情</a></p><h3 id="存储">存储</h3><p><a href="https://zhuanlan.zhihu.com/p/166633984"title="存储技术入门详解">点击跳转：存储技术入门详解</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列3：生活常识</title>
    <link href="/2023/11/25/Informal%20Essay%2003/"/>
    <url>/2023/11/25/Informal%20Essay%2003/</url>
    
    <content type="html"><![CDATA[<h3 id="常见亲属称谓">常见亲属称谓</h3><p>父亲的哥哥 &gt; 伯父、伯伯、大爷（~~~伯母、大娘）</p><p>父亲的弟弟 &gt; 叔叔（~~~婶婶）</p><p>父亲的姐妹 &gt; 姑姑（~~~姑父/夫）</p><p>母亲的兄弟 &gt; 舅舅（~~~舅妈）</p><p>母亲的姐妹 &gt; 姨（~~~姨夫/父）</p><p>父亲之父 &gt; 爷爷（祖父）</p><p>父亲之母 &gt; 奶奶（祖母）</p><p>母亲之父 &gt; 姥爷、外公、外爷（外祖父）</p><p>母亲之母 &gt; 姥姥、外婆（外祖母）</p><p>爷爷的姐妹 &gt; 姑奶（~~~姑爷）</p><p>爷爷的兄弟 &gt; 爷爷</p><p>奶奶的姐妹 &gt; 姨奶（~~~姨爷）</p><p>奶奶的兄弟 &gt; 舅爷（~~~舅奶）</p><h3 id="名酒记">名酒记</h3><p>中国新老八大名酒： <div class="code-wrapper"><pre><code class="hljs">茅台  汾酒  泸州老窖（特曲）古井贡酒  五粮液  董酒剑南春  洋河大曲  西凤酒</code></pre></div> 安徽酒： <div class="code-wrapper"><pre><code class="hljs">古井贡酒  口子窖  高炉家酒  文王贡酒迎驾贡酒  金种子  宣酒  皖酒  金坛子酒店小二酒  沙河王  明光酒  ···</code></pre></div></p><h3 id="处方药和非处方药">处方药和非处方药</h3><p>无论处方药还是非处方药，都必须经过国家食品药品监督管理局（CFDA）的批准才能上市，包装盒上必须注明国药准字。</p><div class="code-wrapper"><pre><code class="hljs gcode">国际上通用 Rx <span class="hljs-comment">(拉丁文Recipe，请求)</span>表示处方药，用 OTC <span class="hljs-comment">(Over The Counter，可以在柜台上买到的)</span>表示非处方药。包装盒上只有国药准字，没有Rx或OTC标识的，为处方药。</code></pre></div><p>非处方药又分为甲、乙两类，在包装盒正面分别用红色OTC、绿色OTC标识。</p><p>乙类非处方药一般不需要医生或药师的指导，消费者可以自行购买，看说明书使用即可，风险较低。</p><p>使用区别：</p><blockquote><p>处方药（无标识或Rx），医生处方，需要仔细阅读说明书并在医师指导下使用（服用）。</p></blockquote><blockquote><p>非处方药（红色或绿色OTC），医生处方、药师指导或自我诊断用，仔细阅读说明书并按照说明书或药师指导下使用。</p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/27355994?utm_id=0"title="你会识别处方药和非处方药吗？">点击此处查看详情</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列2：省会</title>
    <link href="/2023/11/25/Informal%20Essay%2002/"/>
    <url>/2023/11/25/Informal%20Essay%2002/</url>
    
    <content type="html"><![CDATA[<h3 id="省份直辖市-简称-首都省会">省份/直辖市-简称-首都（省会）</h3><div class="code-wrapper"><pre><code class="hljs">北京市-京-北京  天津市-津-天津  上海市-沪-上海  重庆市-渝-重庆黑龙江省-黑-哈尔滨  吉林省-吉-长春  辽宁省-辽-沈阳  内蒙古自治区-蒙-呼和浩特新疆维吾尔族自治区-新-乌鲁木齐  西藏自治区-藏-拉萨  甘肃省-甘-兰州青海省-青-西宁  陕西省-陕-西安  宁夏回族自治区-宁-银川  河南省-豫-郑州河北省-冀-石家庄  安徽省-皖-合肥  山西省-晋-太原  湖南省-湘-长沙湖北省-鄂-武汉  江苏省-苏-南京  四川省-蜀-成都  贵州省-黔-贵阳云南省-云-昆明  广西壮族自治区-桂-南宁  广东省-粤-广州  山东省-鲁-济南浙江省-浙-杭州  江西省-赣-南昌  福建省-闽-福州  海南省-琼-海口香港特别行政区-港-香港  澳门特别行政区-澳-澳门  台湾省-台-台北</code></pre></div><h3 id="行政区划分">行政区划分</h3><p>省级行政区：23个省，5个自治区，4个直辖市，2个特别行政区，共计34个省级行政区。</p><p>地级行政区：地级市（含省会）、地区、自治州、盟。</p><p>县级行政区：县（包括县级市、自治县、旗、自治旗）、市辖区、特区、林区。</p><p>乡级行政区：街道办事处、镇、乡（包括民族乡、苏木）、县辖区。</p><p>村级：居民委员会、村民委员会。</p><p>PS：根据中华人民共和国宪法，人民代表大会是中国最高权利机关，各级人民代表大会选举各级人民政府作为行政机关。所以严格来说，只有拥有政府和人大的级别才算行政区划，没有的就不算。因此，地区、盟、街道办事处、村民委员会、居民委员会均不是行政区划，其中村民委员会、居民委员会是基层群众自治组织，地区、盟、街道办事处是上级政府的派出机构。相应的，地区和盟下辖的县级行政区，实际上直接属于上级行政区（省、自治区），街道办事处下辖的村委会和居委会也同样直接隶属于县、县级市、市辖区。</p><h3 id="中国地理区域">中国地理区域</h3><p>行政大区是在中国建国初期设置的，是当年位于省级之上的行政区划。当时为了便于管理，政府将全国划分为六大行政区：华北（北京）、东北（沈阳）、华东（上海）、中南（武汉）、西南（重庆）、西北（西安）。</p><p>中国七大地理区域：华东地区、华南地区、华北地区、华中地区、东北地区、西南地区、西北地区。</p><p><a href="https://zhuanlan.zhihu.com/p/107971624?utm_source=weibo"title="点击此处查看你不知道的中国各种地理区域划分！">⪧中国地理划分⪦</a></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔系列1：朝代</title>
    <link href="/2023/11/25/Informal%20Essay%2001/"/>
    <url>/2023/11/25/Informal%20Essay%2001/</url>
    
    <content type="html"><![CDATA[<h4 id="夏朝">夏朝</h4><p>大禹 ---------- 桀</p><h4 id="商朝">商朝</h4><p>成汤 ---------- 商纣王</p><h4 id="周朝">周朝</h4><p>分封诸侯，是中国历史上最长的朝代，从西周【周武王】到东周【周平王】，终于【郝王】。<div class="code-wrapper"><pre><code class="hljs">东周时期分为春秋、战国春秋五霸：齐 晋 楚 秦 宋战国七雄：秦 韩 赵 魏 楚 燕 齐诸子百家也在这一时期</code></pre></div></p><h4 id="秦国">秦国</h4><p>始皇帝：嬴政 ---------- 胡亥 <div class="code-wrapper"><pre><code class="hljs">奴隶社会的终结，封建社会的开始，提出并实行郡县制</code></pre></div></p><h5 id="楚汉之争">楚汉之争</h5><p>项羽 VS 刘邦</p><h4 id="西汉">西汉</h4><p>始于汉高祖：刘邦</p><h5 id="新朝">新朝</h5><p>王莽篡汉，成立新朝</p><h4 id="东汉">东汉</h4><p>光武帝刘秀 ---------- 汉献帝刘协</p><h4 id="三国">三国</h4><p>曹魏（曹操）、蜀汉（刘备）、孙吴（孙权）</p><h4 id="西晋">西晋</h4><p>司马炎（司马昭之子）建立</p><h5 id="东晋-五胡十六国-南北朝">东晋 五胡十六国 南北朝</h5><p>乱世时期</p><h4 id="隋朝">隋朝</h4><p>隋文帝杨坚 ---------- 隋炀帝杨广</p><h4 id="唐朝">唐朝</h4><p>唐高祖李渊 ---------- 唐景宗李柷 &gt; 开元盛世：玄宗李隆基 &gt;贞观之治：太宗李世民</p><h5 id="五代十国">五代十国</h5><p>乱世时期</p><h4 id="北宋">北宋</h4><p>宋太祖赵匡胤 ---------- 宋钦宗赵恒（宋徽宗赵佶之子）</p><h5 id="辽">辽</h5><p>实为北宋前由耶律阿保机建立</p><h5 id="金">金</h5><p>完颜阿骨打建立，灭辽、北宋，亡于南宋和蒙古</p><h4 id="南宋">南宋</h4><p>宋高宗赵构（亦为徽宗之子）建立</p><h4 id="元朝">元朝</h4><p>忽必烈建立 <div class="code-wrapper"><pre><code class="hljs">成吉思汗建立蒙古政权，算元朝的前身元朝是首次由少数民族建立的朝代</code></pre></div></p><h4 id="明朝">明朝</h4><p>明太祖朱元璋 ---------- 末代皇帝崇祯（朱由检）</p><h4 id="清朝">清朝</h4><p>由清太宗皇太极改国号为清 ———— 顺治皇帝爱新觉罗•福临一统 ————末代皇帝宣统帝（溥仪） <div class="code-wrapper"><pre><code class="hljs">努尔哈赤建立的是后金（称汗），不算清帝</code></pre></div></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>common</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markdown基本语法</title>
    <link href="/2023/11/24/Markdown/"/>
    <url>/2023/11/24/Markdown/</url>
    
    <content type="html"><![CDATA[<h3 id="一标题">一、标题</h3><p>在文字前加井号 # 表示标题，共支持六级标题。</p><p><strong>注：几乎所有语法符号后要跟个空格再接内容</strong></p><p>示例： <div class="code-wrapper"><pre><code class="hljs clean"># 一级标题## 二级标题### 三级标题···以此类推···</code></pre></div></p><h3 id="二文字与排版">二、文字与排版</h3><ul><li><p><strong>加粗</strong> 在两个星号 * ··· *之间写的内容会被加粗</p></li><li><p><em>斜体</em> 将要倾斜的文字前后各用两个 * 包起来</p></li><li><p><strong><em>斜体加粗</em></strong> 前后各三个 *</p></li><li><p><del>删除线</del> 前后各两个 ~</p></li></ul><p>注：以上不加空格</p><ul><li>换行 &lt;br/&gt; (&lt;br&gt;···&lt;/br&gt;)</li></ul><p>可以内嵌Html语法，例如：段落&lt;p&gt;&lt;/p&gt;(可能不加结束标签也可，但不要依赖这种做法！)、链接&lt;ahref="URL名"&gt;显示内容&lt;/a&gt;、图片&lt;img src="URL名" width="宽度"height="高度" /&gt;等</p><ul><li>引用 在引用的文字前加一个或多个 &gt; 即可，适应屏幕</li></ul><p>效果：</p><blockquote><p>引用1</p></blockquote><blockquote><blockquote><p>引用2</p></blockquote></blockquote><ul><li>分割线 用三个及以上的 * 或 - 单独成行</li></ul><hr /><ul><li><p>列表</p><ul><li><p>无序列表 用 * 或 - 或 + 都行，后接空格</p></li><li><p>有序列表 数字加点 <div class="code-wrapper"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> list1<span class="hljs-bullet">2.</span> list2</code></pre></div></p></li><li><p>嵌套列表 上下两级之间敲两个空格即可</p></li></ul></li><li><p>代码块 (``````)用括号中的反引号将需要标注的块区域包起来，三个就行，但是自动补全机制写六个更省事（不加括号，自成一行）</p><p>单行代码：前后各用一个 ` 就行</p><p>代码块： <div class="code-wrapper"><pre><code class="hljs">这里是代码块</code></pre></div> 单行代码： <code>这里是单行代码</code></p><p>支持语法高亮，在首行 ` 后紧跟着写相应语言即可，如python</p></li><li><p>脚注用方括号[]和^可以构成一个脚注，可在后方写注释，注释显示在文末</p><p>脚注写法：[^1] <br/> 注释写法：[^1]: Content</p><p>效果：脚注<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="这是一个脚注^_^">[1]</span></a></sup></p></li></ul><h3 id="三插入超链接和图片">三、插入超链接和图片</h3><ol type="1"><li><p>[超链接名] (超链接地址 "超链接title") 示例：[百度](http://baidu.com)</p></li><li><p>![图片名] (图片地址 "图片title") 图片名为下标</p></li><li><p><URL名> 可点击的链接，也可填入Email地址</p></li></ol><p>注：名和地址之间中间不加空格，title可不加<br>title与地址间空一格，内容在鼠标悬停于超链接或图片上时显示</br></p><h3 id="四表格">四、表格</h3><p>用管道符 | 来分隔各列，用 ------ (多个) 来创建（隔开）表头。</p><p>示例： <div class="code-wrapper"><pre><code class="hljs gherkin">|<span class="hljs-string"> theHead1 </span>|<span class="hljs-string"> theHead2 </span>|<span class="hljs-string"> theHead3 </span>|<span class="hljs-string"> theHead4 </span>||<span class="hljs-string"> -------- </span>|<span class="hljs-string"> :------- </span>|<span class="hljs-string"> -------: </span>|<span class="hljs-string"> :------: </span>||<span class="hljs-string"> 内容1 </span>|<span class="hljs-string"> 内容2 </span>|<span class="hljs-string"> 内容3 </span>|<span class="hljs-string"> 内容4 </span>||<span class="hljs-string"> 内容1 </span>|<span class="hljs-string"> 内容2 </span>|<span class="hljs-string"> 内容3 </span>|<span class="hljs-string"> 内容4 </span>|</code></pre></div></p><table><thead><tr class="header"><th>theLongHead1</th><th style="text-align: left;">theLongHead2</th><th style="text-align: right;">theLongHead3</th><th style="text-align: center;">theLongHead4</th></tr></thead><tbody><tr class="odd"><td>内容1</td><td style="text-align: left;">内容2</td><td style="text-align: right;">内容3</td><td style="text-align: center;">内容4</td></tr><tr class="even"><td>内容1</td><td style="text-align: left;">内容2</td><td style="text-align: right;">内容3</td><td style="text-align: center;">内容4</td></tr></tbody></table><div class="code-wrapper"><pre><code class="hljs gherkin">|<span class="hljs-string"> 1 </span>|<span class="hljs-string"> 2 </span>|<span class="hljs-string"> 3 </span>|<span class="hljs-string"> 4 </span>||<span class="hljs-string"> ---- </span>|<span class="hljs-string"> :---- </span>|<span class="hljs-string"> ----: </span>|<span class="hljs-string"> :---: </span>||<span class="hljs-string"> 超长的内容1 </span>|<span class="hljs-string"> 超长的内容2 </span>|<span class="hljs-string"> 超长的内容3 </span>|<span class="hljs-string"> 超长的内容4 </span>||<span class="hljs-string"> 超长的内容1 </span>|<span class="hljs-string"> 超长的内容2 </span>|<span class="hljs-string"> 超长的内容3 </span>|<span class="hljs-string"> 超长的内容4 </span>|</code></pre></div><table><thead><tr class="header"><th>1</th><th style="text-align: left;">2</th><th style="text-align: right;">3</th><th style="text-align: center;">4</th></tr></thead><tbody><tr class="odd"><td>超长的内容1</td><td style="text-align: left;">超长的内容2</td><td style="text-align: right;">超长的内容3</td><td style="text-align: center;">超长的内容4</td></tr><tr class="even"><td>超长的内容1</td><td style="text-align: left;">超长的内容2</td><td style="text-align: right;">超长的内容3</td><td style="text-align: center;">超长的内容4</td></tr></tbody></table><p>在分隔行的连字符 - 左侧、右侧或两侧可以加冒号 :使得该列内容左对齐、右对齐或居中显示，默认（不加冒号时）左对齐。</p><h3 id="扩展语法">扩展语法</h3><h4 id="任务列表语法">任务列表语法</h4><p>短横杠 - 加方括号 [ ] （都有空格） <br/> 选择（复选框）则用 x替换方括号中的空格：[x]</p><p>效果：</p><ul class="task-list"><li><label><input type="checkbox"checked="" />这是一个复选框</label></li><li><label><input type="checkbox" />这是一个普通任务项</label></li></ul><h4 id="注意事项">注意事项</h4><p>使用hexo-renderer-pandoc渲染器，卸载了hexo自带的marked渲染器后，有些Markdown语法格式与LaTex语法格式冲突，因此需要注意以下几点：</p><ol type="1"><li>不要在代码块(`划定的区域)中插入公式，会无法解析/渲染</li><li>引用&gt;时，或者用其他符号如任务列表语法-[]、标题#时，最好单独成行，前、后各空一行，LaTex对“吃”空格/行</li><li>使用公式要在Front-matter中指定："math: true"</li><li>有脚注时，要在文末空个两行，防止脚注紧贴正文，正文格式会影响脚注的显示格式</li><li>空一行及以上，渲染结果就是内容之间间隔一行；换行渲染后仅相当于空格；可用&lt;br/&gt;来换行</li><li>不要在$后面随意加空格(插入公式时)</li></ol><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>这是一个脚注^_^<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>auxiliary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/11/24/HelloWorld/"/>
    <url>/2023/11/24/HelloWorld/</url>
    
    <content type="html"><![CDATA[<p>This is my first article.</p><p>In order to create my own blog, I studied for several days, andfinally achieved initial results.</p><hr /><p>I've been studying for days to create a blog of my own, and I'mfinally getting there.</p><p>This shows that it is very important to learn English well.</p>]]></content>
    
    
    <categories>
      
      <category>Default</category>
      
    </categories>
    
    
    <tags>
      
      <tag>foreword</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
